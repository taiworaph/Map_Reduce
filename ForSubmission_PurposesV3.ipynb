{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click Through Rate (CTR) Prediction for Advertisement Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0: Goal of the Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial goal of our analysis is to build a model to predict ClickThroughRate (CTR), i.e. given a set of features of both an advertisement(advertisment length, type, duration, etc) and a set of features of a user (webpage visited, web-page history), what is the probability that he or she will click on that given advertisement? The larger goal is to build a model that will perform at scale using concepts taught in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: What Questions Do We Seek To Answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions that we seek to answer are:\n",
    "\n",
    "(1) Which variables are more important in determining the click-through rate prediction?\n",
    "\n",
    "(2) We will be exploring the use of Logistic Regression for Click Through Rate prediction? But we will also discuss improvements and expected accuracy increase with using other Machine Learning techniques.\n",
    "\n",
    "(3) More importantly we will be discussing how the eventual predictive modelling approach is expected to scale to the entire 10GB dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Why Do People Perform This Kind Of Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click-through rate (CTR) prediction is critical to many web applications including web search, recommender systems, sponsored search, and display advertising. Search advertising, known as sponsored search, refers to advertisers identifying relevant keywords based on their product or service for advertising. When the user retrieves the keyword purchased by the advertiser, the corresponding advertisement is triggered and displayed. In the cost-per-click model, the advertiser pays the web publisher only when a user clicks their advertisements and visits the advertiser's site. The CTR prediction is defined to estimate the ratio of clicks to impressions of advertisements that will be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: What Level Of Performance Should The Model Have To Achieve Practical Use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a literature search, the model would have to have an \"areaUnderROC\" value of close to 75% and a logloss of 0.025 for it to be considered a high-performing model (Wang et al., 2018). Entropy or mutual information is another metric that is useful for CTR prediction algorithms. Juan et al., 2017 used a normalized log loss to measure performance of their model as well as a \"Utility\" metric which allows to model offline the potential change in profit due to a prediction model change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw5_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ade4fe4f76456bb4b29f7801ee7d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainRDD = sc.textFile('s3://breastcancerfile/e-AKVLH55FZCUE4SN1JHTY7DWU4/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile('data/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Distribution of the Labels in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used for finding the count of \"1\"s and \"0\"s in the entire dataset. The count shows that we have 34,095,179 \"0\"s (no-clicks) and 11,745,438 \"1\"s (clicks). The dataset is unbalanced, but not by a lot, so it is not necessary that we do subsampling of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 34095179), ('1', 11745438)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CountNumberLabel(toyRDDLine):\n",
    "    \n",
    "    \"\"\" Takes the Count of Number of 1 and 0's in the Label for the entire dataset \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T1 = Values[0]\n",
    "    return (T1, 1)\n",
    "\n",
    "trainRDD.map(CountNumberLabel).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Creating a Training and Testing Dataset from the Original 10GB DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset was created from the original dataset using the `randomSplit()` function. `randomSplit()` enables us to generate a random subset of any dataset. We decided to go with an 1% split of the dataset that represents a count of 458,906 rows called `TrainRDD` - enough for one to train a Logistic Regression function. The testing dataset was created from the 99% remainder of the split (called `TrainRDD2`). The test dataset was used for validating the weights from the Logistic Regression Model used as a model for click prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainRDD, TrainRDD2 = trainRDD.randomSplit([0.01,0.99], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to doing exploratory data analysis, we needed to know what the distribution of the number of clicks (1) and no-clicks (0) were in the entire dataset. This enabled us to understand if we have an unbalanced dataset and if subsampling of the dataset would be necessary to ensure that we have an even distribution between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestRDD2, TestRDD = TrainRDD2.randomSplit([0.001,0.999], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of the number of rows of data in the Train and Test dataset is shown below. 458,455 rows are present in the TrainRDD while 45,355 are present in the TestRDD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the training dataset:  458455\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows in the training dataset: \", TrainRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the test dataset:  45355\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows in the test dataset: \", TestRDD2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: Dataset Contents and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 14 numeric columns or features and 25 categorical columns or features. The first numerical column in the dataset represents the binary \"Yes\"/\"No\" click event represented as either a \"1\" or a \"0\" respectively. \n",
    "\n",
    "To better understand the dataset, we divided the dataset exploratory section into a numerical exploratory section and a categorical exploratory section. \n",
    "\n",
    "In the numerical exploratory section, our main goals were to:\n",
    "\n",
    "(1) Find the minimum and maximum values of each numerical feature in the column(s) from 1-14.\n",
    "\n",
    "(2) Find the mean and standard deviation of each of the numerical features contained in the entire dataset.\n",
    "\n",
    "The distribution within each dataset is discussed in the Exploratory Data Analysis section.\n",
    "\n",
    "The questions we intended to investigate in the numerical data introduction section were:\n",
    "\n",
    "(1) What were the types of Labels and what was the the distribution of the Labels (\"1\" and \"0\")? \n",
    "- The ratio of \"1\"s to \"0\"s, i.e., clicks to no-clicks, was shown to be roughly 1:3 (see Section 1.0 above), indicating an unbalanced dataset but not severely unbalanced.\n",
    "\n",
    "(2) What were the minimum and maximum values of all the numerical features within each numerical column? \n",
    "- This can be done by going through the entire dataset. This is a computationally expensive process but is required. The values need to be saved into an array since the resultant matrix can easily be saved to disk/cached so that we do not have to compute that every time.\n",
    "\n",
    "(3) What were the means and standard deviations of the numerical features within each numerical column? \n",
    "- This is also a computationally expensive process, but the resultant matrix can also be saved to disk/cached so that computation is only done once.\n",
    "\n",
    "#### 1.6.1: Some Core Course Concepts Utilized in this Section:\n",
    "*Caching and Broadcasting the Matrix containing Mean and Standard Deviations* -- Finding the mean and standard deviation for the trainRDD (10GB dataset) involves going through the entire dataset. However, mean and standard deviation are supposed to be utilized for mean-normalizing only the TrainRDD and the TestRDD2. Thus an important concept here is to *first* do the mean and standard deviation calculation and then *cache* these values. Because the matrix of the mean and standard deviation is small, one can broadcast these values without incurring much performance penalty for use in mean-normalizing the TrainRDD and the TestRDD2.\n",
    "\n",
    "*Mean Normalization for SGD* -- Mean normalization of the numerical variables is very important for Gradient Descent because non-normalized features would take a longer time to converge. To ensure that we converge faster we use mean normalization of the features using the array of mean and standard deviation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractNumerals(toyRDDline):\n",
    "    \n",
    "    YY = toyRDDline.split('\\t')\n",
    "    \n",
    "    if YY[ii] == '':   ## There are empty values/NaNs - They are replaced with 0's for this initial Mean, StdDev calculation\n",
    "        ReturnValue = 0\n",
    "    else:\n",
    "        ReturnValue = float(YY[ii])\n",
    "    return ReturnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node = []\n",
    "NodeMeanMax = []\n",
    "    \n",
    "for ii in range(1,14):\n",
    "\n",
    "    featureRDD = trainRDD.map(ExtractNumerals)\\\n",
    "                         .cache()\n",
    "    featureMeans = featureRDD.mean()\n",
    "    featureStdev = np.sqrt(featureRDD.variance())\n",
    "    \n",
    "    featureMax = featureRDD.max()\n",
    "    featureMin = featureRDD.min()\n",
    "    \n",
    "    Node.append((featureMeans, featureStdev))\n",
    "    NodeMeanMax.append((featureMax, featureMin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.9136993727636809, 7.184627898705608),\n",
       " (105.84841979766556, 391.45781841729996),\n",
       " (21.13629851448115, 352.8574390110648),\n",
       " (5.735263227368864, 8.346464252535688),\n",
       " (18060.51214960742, 68556.28645274039),\n",
       " (90.10443053155232, 340.53335300283976),\n",
       " (15.62662976373116, 64.6908374702787),\n",
       " (12.510823839914709, 16.68706965443228),\n",
       " (101.51997332409383, 216.54476824937402),\n",
       " (0.33741472109766746, 0.59176564119444),\n",
       " (2.6146237734976343, 5.115681630473718),\n",
       " (0.2328159762771082, 2.7454850748632738),\n",
       " (6.436072751813085, 14.741644511057435)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Needed = sc.broadcast(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5775.0, 0),\n",
       " (257675.0, -3.0),\n",
       " (65535.0, 0),\n",
       " (969.0, 0.0),\n",
       " (23159456.0, 0.0),\n",
       " (431037.0, 0),\n",
       " (56311.0, 0.0),\n",
       " (6047.0, 0.0),\n",
       " (29019.0, 0.0),\n",
       " (11.0, 0),\n",
       " (231.0, 0.0),\n",
       " (4008.0, 0),\n",
       " (7393.0, 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NodeMeanMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NodeMeanMax = [(5775.0, 0),\n",
    " (257675.0, -3.0),\n",
    " (65535.0, 0),\n",
    " (969.0, 0.0),\n",
    " (23159456.0, 0.0),\n",
    " (431037.0, 0),\n",
    " (56311.0, 0.0),\n",
    " (6047.0, 0.0),\n",
    " (29019.0, 0.0),\n",
    " (11.0, 0),\n",
    " (231.0, 0.0),\n",
    " (4008.0, 0),\n",
    " (7393.0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NodeMeans = [(1.9136993727636809, 7.184627898705608),\n",
    " (105.84841979766556, 391.45781841729996),\n",
    " (21.13629851448115, 352.8574390110648),\n",
    " (5.735263227368864, 8.346464252535688),\n",
    " (18060.51214960742, 68556.28645274039),\n",
    " (90.10443053155232, 340.53335300283976),\n",
    " (15.62662976373116, 64.6908374702787),\n",
    " (12.510823839914709, 16.68706965443228),\n",
    " (101.51997332409383, 216.54476824937402),\n",
    " (0.33741472109766746, 0.59176564119444),\n",
    " (2.6146237734976343, 5.115681630473718),\n",
    " (0.2328159762771082, 2.7454850748632738),\n",
    " (6.436072751813085, 14.741644511057435)]\n",
    "\n",
    "Needed = sc.broadcast(NodeMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7: Mean-Normalization of the Numerical Variable -- TrainRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, mean-normalization of the numerical variable is key to enabling fast gradient descent optimization.\n",
    "We can mean normalize the dataset by splitting the data into Numerical and Categorical variables and then working only on the numerical variable in this section. \n",
    "However, that would result to creating additional data and that is O(space) inefficient. A better idea to cater to scalability would be to work with the dataset as is and make modifications to the dataset. This would lead to an O(1) space complexity and thus enable faster computation of the mean normalization. \n",
    "Recall, we are broadcasting the Mean and Standard deviation array to all the partitions for computing the mean-normalization. This computation process has  O(n) time complexity and O(1) space complexity, while is ideal for scaling and also ideal for use in a map-reduce paradigmn or environment since it now represents an embarassingly parallel computation.\n",
    "\n",
    "During the computation of the mean normalization, we are also replacing all NaN or empty values with the calculated mean for that numerical column and replacing all the NaN or empty values with 0 for the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericValuesOnly(LineRDD):\n",
    "    \"\"\"Function takes the RDD Mean normalizes the Numerical Column in the RDD and replaces 'NAN' and '' with mean for \n",
    "       Numerical values and '0' for Categorical values\"\"\"\n",
    "    \n",
    "    Node = Needed.value\n",
    "    Values = LineRDD.split('\\t')\n",
    "    ZZ = []\n",
    "    for ii in range(0,40):\n",
    "        \n",
    "        if ii < 14:\n",
    "            if Values[ii] == '':\n",
    "                ZZ.append(Node[ii-1][0])\n",
    "            else:\n",
    "                if (ii == 0):\n",
    "                    ZZ.append(float(Values[ii]))\n",
    "                else:\n",
    "                    ZZ.append((float(Values[ii])- float(Node[ii-1][0]))/float(Node[ii-1][1]))\n",
    "        else:\n",
    "            if Values[ii] == '':\n",
    "                ZZ.append(str(0))\n",
    "            else:\n",
    "                ZZ.append(str(Values[ii]))\n",
    "    \n",
    "    return (((str(Values[14])+ str(Values[15]) + str(Values[16])), ZZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68fd1e64287130e071e126ad',\n",
       "  [0.0,\n",
       "   -0.26636026245819294,\n",
       "   -0.2729500211023082,\n",
       "   0.00811574638625955,\n",
       "   -0.6871488397768515,\n",
       "   -0.052139815829603434,\n",
       "   0.9217762862301186,\n",
       "   -0.1488097872925819,\n",
       "   -0.6298783463831777,\n",
       "   -0.24253632977921483,\n",
       "   -0.5701830211308282,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   '68fd1e64',\n",
       "   '287130e0',\n",
       "   '71e126ad',\n",
       "   '7846ae91',\n",
       "   '4cf72387',\n",
       "   '6f6d9be8',\n",
       "   'f6ce794a',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '70962768',\n",
       "   'ab066900',\n",
       "   'e95d3160',\n",
       "   '5d4198ed',\n",
       "   'f7c1b33f',\n",
       "   '42793602',\n",
       "   'fc877894',\n",
       "   'e5ba7672',\n",
       "   '891589e7',\n",
       "   '55dd3565',\n",
       "   'a458ea53',\n",
       "   'c8d017f7',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '7d290d33',\n",
       "   'c243e98b',\n",
       "   '71848e87']),\n",
       " ('68fd1e64083aa75b',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.2652863601435636,\n",
       "   -0.03722834511098185,\n",
       "   0.9902081315594536,\n",
       "   -0.09341976470709805,\n",
       "   -0.12951574388422543,\n",
       "   -0.08697722867347693,\n",
       "   0.3888745174837627,\n",
       "   -0.2563902779685573,\n",
       "   0.33741472109766746,\n",
       "   -0.3156224116605392,\n",
       "   0.2328159762771082,\n",
       "   0.852274469022974,\n",
       "   '68fd1e64',\n",
       "   '083aa75b',\n",
       "   '0',\n",
       "   '0',\n",
       "   '25c83c98',\n",
       "   '0',\n",
       "   'd385ea68',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '3b08e48b',\n",
       "   '7940fc2a',\n",
       "   '0',\n",
       "   '00e20e7b',\n",
       "   '1adce6ef',\n",
       "   '84203dfc',\n",
       "   '0',\n",
       "   'e5ba7672',\n",
       "   '06747363',\n",
       "   '21ddcdc9',\n",
       "   'b1252a9d',\n",
       "   '0',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '0',\n",
       "   'e8b83407',\n",
       "   'ed36f3c2']),\n",
       " ('be589b5180e26c9b74e1a23a',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.17843153594445796,\n",
       "   0.025119780697725077,\n",
       "   -0.44752641530023646,\n",
       "   18060.51214960742,\n",
       "   90.10443053155232,\n",
       "   -0.24155862522123936,\n",
       "   -0.6298783463831777,\n",
       "   -0.4595815180789135,\n",
       "   0.33741472109766746,\n",
       "   -0.5110997834428405,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   'be589b51',\n",
       "   '80e26c9b',\n",
       "   '74e1a23a',\n",
       "   '9a6888fb',\n",
       "   '25c83c98',\n",
       "   '3bf701e7',\n",
       "   '0dab78da',\n",
       "   '0b153874',\n",
       "   '7cc72ec2',\n",
       "   '3b08e48b',\n",
       "   '7bc78da9',\n",
       "   'fb8fab62',\n",
       "   '6b5d07b4',\n",
       "   'b28479f6',\n",
       "   '4c1df281',\n",
       "   'c6b1e1b2',\n",
       "   '2005abd1',\n",
       "   'f54016b9',\n",
       "   '21ddcdc9',\n",
       "   'b1252a9d',\n",
       "   '99c09e97',\n",
       "   '0',\n",
       "   'be7c41b4',\n",
       "   '335a6a1e',\n",
       "   'e8b83407',\n",
       "   'd15c0cc8']),\n",
       " ('05db91640468d67296166464',\n",
       "  [0.0,\n",
       "   0.2903839498232316,\n",
       "   1.2546730633407848,\n",
       "   -0.03722834511098185,\n",
       "   0.6307744948445312,\n",
       "   -0.25391562248062216,\n",
       "   -0.03848207647208949,\n",
       "   -0.1488097872925819,\n",
       "   0.5087277955857558,\n",
       "   0.011452720358730456,\n",
       "   1.119675142958533,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   3.9048511314330825,\n",
       "   '05db9164',\n",
       "   '0468d672',\n",
       "   '96166464',\n",
       "   '867d05be',\n",
       "   '25c83c98',\n",
       "   '7e0ccccf',\n",
       "   '81bb0302',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '012bac1e',\n",
       "   'b7094596',\n",
       "   'dc2f19a6',\n",
       "   '1f9d2c38',\n",
       "   '07d13a8f',\n",
       "   'a888f201',\n",
       "   '668f77c8',\n",
       "   'e5ba7672',\n",
       "   '9880032b',\n",
       "   '21ddcdc9',\n",
       "   '5840adea',\n",
       "   '1c6ba3ba',\n",
       "   '0',\n",
       "   '55dd3565',\n",
       "   'a9a2ac1a',\n",
       "   'ea9a246c',\n",
       "   '409c7293']),\n",
       " ('05db9164287130e0287f8cdf',\n",
       "  [1.0,\n",
       "   1.9136993727636809,\n",
       "   -0.07880394348077799,\n",
       "   21.13629851448115,\n",
       "   -0.44752641530023646,\n",
       "   -0.15463953341340367,\n",
       "   -0.21761284137984085,\n",
       "   -0.16426792694735814,\n",
       "   0.20909460033077323,\n",
       "   -0.2610082606983381,\n",
       "   0.33741472109766746,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   '05db9164',\n",
       "   '287130e0',\n",
       "   '287f8cdf',\n",
       "   '8e068c13',\n",
       "   '30903e74',\n",
       "   'fe6b92e5',\n",
       "   'fed3cb1d',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '1cf80d48',\n",
       "   'b7094596',\n",
       "   'dd28c867',\n",
       "   '1f9d2c38',\n",
       "   'b28479f6',\n",
       "   '9efd8b77',\n",
       "   'f8584e89',\n",
       "   'e5ba7672',\n",
       "   '891589e7',\n",
       "   'a35e3db3',\n",
       "   'b1252a9d',\n",
       "   '204382b1',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '3fdb382b',\n",
       "   '9721386e',\n",
       "   '49d68486'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonNumericRDD = TrainRDD.map(NumericValuesOnly).cache()\n",
    "NonNumericRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8: Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the categorical exploratory section our main goals were to:\n",
    "\n",
    "(1) Find the minimum and maximum occurence of each categorical feature in the full dataset as well as the distict set of categorical features present in the dataset.\n",
    "\n",
    "(2) We were also interested in finding the minimum and maximum count of the categorical feature in each categorical column.\n",
    "\n",
    "Note: These two methods are computationally expensive but they give an underlying overview of the numerical distribution within the categorical dataset.\n",
    "\n",
    "More information on the underlying distrbution of the categorical features is in the EDA section.\n",
    "\n",
    "(3) More interesting and useful here is that we will need to implement an hashing algorithm in this section.\n",
    "\n",
    "- The need for hashing is borne out of the curse of dimensionality that we would have had if we were to one-hot encode all the categorical features in the dataset. We would probably have had tens of thousands of columns resulting in a very sparse matrix. That would not only be computationally expensive but also a challenge for shuffling through a computing network.\n",
    "\n",
    "To ensure we have a resulting dense matrix, we used a home-grown hashing function to reduce the dimensionality of each categorical variable from >100 to just 10. This eventually reduced the cardinality of the resultant architecture to 300 rather than in the order of thousands.\n",
    "\n",
    "#### 1.8.1: Some Core Course Concepts Utilized in this Section:\n",
    "*Hashing* -- Although we did not touch on this concept in depth in class, the hashing function as used here is the function that enables us to reduce the cardinality of the categorical variables from several thousands to only 10 or as many as we deem fit. \n",
    "\n",
    "Hashing here was implemented using an home grown algorithm.\n",
    "\n",
    "Home-Grown Hashing Function Algorithm:\n",
    "\n",
    "The hashing function uses the co-occurence count of each feature within a column with the click outcome \"1\" divided by the total count of coccurence of the feature. This results in what we call a \"co-occurence count of importance\". Because this is normalized by the overall count, the values are always between 0 and 1. With 1 showing that the feature is very important (meaning everytime this feature occured we also had the occurence of a count) and 0 meaning the feature is not important at all (meaning everytime this feature occured we did not see the occurence of a count).\n",
    "\n",
    "The hashing function enables us to reduce cardinality and also go from a sparse distribution to a denser representation of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68fd1e64287130e071e126ad',\n",
       "  [0.0,\n",
       "   -0.26636026245819294,\n",
       "   -0.2729500211023082,\n",
       "   0.00811574638625955,\n",
       "   -0.6871488397768515,\n",
       "   -0.052139815829603434,\n",
       "   0.9217762862301186,\n",
       "   -0.1488097872925819,\n",
       "   -0.6298783463831777,\n",
       "   -0.24253632977921483,\n",
       "   -0.5701830211308282,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   7,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   7,\n",
       "   8,\n",
       "   7,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   7,\n",
       "   0]),\n",
       " ('68fd1e64083aa75b',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.2652863601435636,\n",
       "   -0.03722834511098185,\n",
       "   0.9902081315594536,\n",
       "   -0.09341976470709805,\n",
       "   -0.12951574388422543,\n",
       "   -0.08697722867347693,\n",
       "   0.3888745174837627,\n",
       "   -0.2563902779685573,\n",
       "   0.33741472109766746,\n",
       "   -0.3156224116605392,\n",
       "   0.2328159762771082,\n",
       "   0.852274469022974,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   7,\n",
       "   9,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   0])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" This for loop loops through the rows in the RDD containing categorical features and converts the categorical features\n",
    "    based on feature importance into a more condensed representation between 1 and 10 \"\"\"\n",
    "\n",
    "\n",
    "StartFromA = 14\n",
    "for ii in range(14, 40):\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "\n",
    "\n",
    "    def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        T0 = Values[0]\n",
    "        return (((T14, T0), 1))\n",
    "\n",
    "    def ConvertForMerge(toyRDDLine):\n",
    "\n",
    "        YY = toyRDDLine[0][0]\n",
    "        YYY = toyRDDLine[0][1]\n",
    "        YYZ = toyRDDLine[1]\n",
    "\n",
    "        return ((YY,(YYY,YYZ)))\n",
    "\n",
    "\n",
    "    def MovingOn(trainRDD):\n",
    "\n",
    "        Key = trainRDD[0]\n",
    "        Value = trainRDD[1]\n",
    "        \n",
    "        ValueKey = Value[0][0]\n",
    "        ValueValue = Value[0][1]\n",
    "        ValueDivide = Value[1]\n",
    "        \n",
    "        return ((Key, (ValueKey, float(ValueValue/ValueDivide))))\n",
    "\n",
    "\n",
    "    def HasherFunction(trainRDD):\n",
    "    \n",
    "        \"\"\" The Hashing Function converts all categorical variables based on feature importance to values between \n",
    "            1 and 10\"\"\"\n",
    "        Key = trainRDD[0]\n",
    "        Value = trainRDD[1][1]\n",
    "        \n",
    "        if (Value >0.9):\n",
    "            FinalValue = 1\n",
    "        elif (Value <=0.9) & (Value >0.8):\n",
    "            FinalValue = 2\n",
    "        elif (Value <=0.8) & (Value >0.7):\n",
    "            FinalValue = 3\n",
    "        elif (Value <=0.7) & (Value>0.6):\n",
    "            FinalValue = 4\n",
    "        elif (Value <=0.6) & (Value >0.5):\n",
    "            FinalValue = 5\n",
    "        elif (Value <=0.5) & (Value >0.4):\n",
    "            FinalValue = 6\n",
    "        elif (Value <=0.4) & (Value >0.3):\n",
    "            FinalValue = 7\n",
    "        elif (Value <=0.3) & (Value >0.2):\n",
    "            FinalValue =8\n",
    "        elif (Value <=0.2) & (Value >0.1):\n",
    "            FinalValue = 9\n",
    "        elif (Value <= 0.1):\n",
    "            FinalValue = 10\n",
    "            \n",
    "        return ((Key, FinalValue))\n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "    YY = TrainRDD.map(ValuesNonNumericFeatures)\\\n",
    "                .reduceByKey(lambda x,y: x+y)\\\n",
    "                .cache()\n",
    "\n",
    "    ZZ = TrainRDD.map(NonNumericFeatures)\\\n",
    "                 .reduceByKey(lambda x,y: x+y)\\\n",
    "                 .map(ConvertForMerge)\\\n",
    "                 .filter(lambda x: x[1][0] == '1')\\\n",
    "                 .leftOuterJoin(YY)\\\n",
    "                 .map(MovingOn).map(HasherFunction).cache()\n",
    "    TT = ZZ.collect()\n",
    "\n",
    "    ZZ.unpersist()\n",
    "    YY.unpersist()\n",
    "\n",
    "    #NamingDict = \"HashDictionary\" + str(Node)\n",
    "    from collections import defaultdict\n",
    "\n",
    "    NamingDict = defaultdict(list)\n",
    "\n",
    "    for ii in TT:\n",
    "        NamingDict[ii[0]] = ii[1]\n",
    "        \n",
    "\n",
    "    \"\"\" The dictionary containing feature importance is broadcasted to all partitions for use in conversion \n",
    "        of the categorical variables to features with lower cardinality - denser representation\"\"\"  \n",
    "    \n",
    "    YYY = sc.broadcast(NamingDict)\n",
    "    \n",
    "    \n",
    "    def MappingChangesWithDictionary(trainRDD):\n",
    "\n",
    "\n",
    "        \"\"\" Mapping Changes with What the Dictionary has for feature importance \"\"\"\n",
    "        Dictionary = YYY.value\n",
    "\n",
    "        \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "        FinalKey = trainRDD[0]\n",
    "        Value = trainRDD[1]\n",
    "        Value[Node] = Dictionary.get(Value[Node], 0)\n",
    "\n",
    "        return (FinalKey,Value)\n",
    "\n",
    "\n",
    "\n",
    "    NonNumericRDD = NonNumericRDD.map(MappingChangesWithDictionary)\\\n",
    "                    .cache()\n",
    "    \n",
    "    \n",
    "NonNumericRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2: Minimum and Maximum Count In Each Column Of Categorical Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the minumum value for : 14 Minimum: = 7 This is the maximum value for : 14 Maximum: = 22950860\n",
      "This is the minumum value for : 15 Minimum: = 1 This is the maximum value for : 15 Maximum: = 5245774\n",
      "This is the minumum value for : 16 Minimum: = 1 This is the maximum value for : 16 Maximum: = 1559473\n",
      "This is the minumum value for : 17 Minimum: = 1 This is the maximum value for : 17 Maximum: = 1636423\n",
      "This is the minumum value for : 18 Minimum: = 10 This is the maximum value for : 18 Maximum: = 30772137\n",
      "This is the minumum value for : 19 Minimum: = 2 This is the maximum value for : 19 Maximum: = 18166950\n",
      "This is the minumum value for : 20 Minimum: = 1 This is the maximum value for : 20 Maximum: = 955418\n",
      "This is the minumum value for : 21 Minimum: = 10 This is the maximum value for : 21 Maximum: = 27232196\n",
      "This is the minumum value for : 22 Minimum: = 7646 This is the maximum value for : 22 Maximum: = 41200152\n",
      "This is the minumum value for : 23 Minimum: = 1 This is the maximum value for : 23 Maximum: = 10157763\n",
      "This is the minumum value for : 24 Minimum: = 1 This is the maximum value for : 24 Maximum: = 1458428\n",
      "This is the minumum value for : 25 Minimum: = 1 This is the maximum value for : 25 Maximum: = 1559473\n",
      "This is the minumum value for : 26 Minimum: = 2 This is the maximum value for : 26 Maximum: = 1458428\n",
      "This is the minumum value for : 27 Minimum: = 4 This is the maximum value for : 27 Maximum: = 16029055\n",
      "This is the minumum value for : 28 Minimum: = 1 This is the maximum value for : 28 Maximum: = 684365\n",
      "This is the minumum value for : 29 Minimum: = 1 This is the maximum value for : 29 Maximum: = 1559473\n",
      "This is the minumum value for : 30 Minimum: = 1292 This is the maximum value for : 30 Maximum: = 21153559\n",
      "This is the minumum value for : 31 Minimum: = 1 This is the maximum value for : 31 Maximum: = 1450322\n",
      "This is the minumum value for : 32 Minimum: = 1 This is the maximum value for : 32 Maximum: = 20172858\n",
      "This is the minumum value for : 33 Minimum: = 8431986 This is the maximum value for : 33 Maximum: = 20172858\n",
      "This is the minumum value for : 34 Minimum: = 1 This is the maximum value for : 34 Maximum: = 1559473\n",
      "This is the minumum value for : 35 Minimum: = 5 This is the maximum value for : 35 Maximum: = 34955073\n",
      "This is the minumum value for : 36 Minimum: = 62 This is the maximum value for : 36 Maximum: = 20170568\n",
      "This is the minumum value for : 37 Minimum: = 1 This is the maximum value for : 37 Maximum: = 2400439\n",
      "This is the minumum value for : 38 Minimum: = 1 This is the maximum value for : 38 Maximum: = 20172858\n",
      "This is the minumum value for : 39 Minimum: = 1 This is the maximum value for : 39 Maximum: = 20172858\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This function caalculates the minimum and maximum counts of distinct features in each categorical column \"\"\"\n",
    "for ii in range(14,40):\n",
    "    \"\"\" Nodes and their meanings \"\"\"\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "    \n",
    "    Feature = trainRDD.map(ValuesNonNumericFeatures)\\\n",
    "                    .reduceByKey(lambda x,y: x+y)\\\n",
    "                    .values()\\\n",
    "                    .collect()\n",
    "    print(\"This is the minumum value for :\", Node, \"Minimum: =\", min(Feature),\n",
    "          \"This is the maximum value for :\", Node, \"Maximum: =\", max(Feature))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.3: The Features With The Minimum And Maximum Occurence For Each Of The 40 Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('05db9164', 22950860), ('68fd1e64', 7648314), ('5a9ed9b0', 3824933), ('8cf07265', 2269393), ('be589b51', 1512743)]\n",
      "[('38a947a1', 5245774), ('207b2d81', 1978351), ('38d50e09', 1754706), ('1cfdf714', 1665114), ('287130e0', 1637732)]\n",
      "[('', 1559473), ('d032c263', 1141289), ('02cf9876', 495012), ('aa8c1539', 453649), ('9143c832', 449682)]\n",
      "[('c18be181', 1636423), ('', 1559473), ('29998ed1', 996589), ('d16679b9', 973669), ('85dd697c', 888439)]\n",
      "[('25c83c98', 30772137), ('4cf72387', 7174714), ('43b19349', 2899544), ('384874ce', 1502511), ('30903e74', 884958)]\n",
      "[('7e0ccccf', 18166950), ('fbad5c96', 9976696), ('fe6b92e5', 8531935), ('', 5540625), ('13718bbd', 1457093)]\n",
      "[('1c86e0eb', 955418), ('dc7659bd', 577839), ('7195046d', 406801), ('5e64ce5f', 352344), ('468a0854', 351515)]\n",
      "[('0b153874', 27232196), ('5b392875', 7626648), ('1f89b562', 3430948), ('37e4aa92', 1905810), ('062b5529', 1188847)]\n",
      "[('a73ee510', 41200152), ('7cc72ec2', 4632819), ('a18233ea', 7646)]\n",
      "[('3b08e48b', 10157763), ('efea433b', 675066), ('fbbf2c95', 351884), ('fa7d0797', 263446), ('03e48276', 246622)]\n",
      "[('755e4a50', 1458428), ('e51ddf94', 962691), ('7f8ffe57', 689062), ('4d8549da', 595244), ('8b94178b', 502420)]\n",
      "[('', 1559473), ('dfbb09fb', 1141289), ('6aaba33c', 996574), ('8fe001f4', 495012), ('d8c29807', 453649)]\n",
      "[('5978055e', 1458428), ('3516f6e6', 1076241), ('46f42a63', 755851), ('025225f2', 650688), ('1aa94af3', 608829)]\n",
      "[('b28479f6', 16029055), ('07d13a8f', 15720748), ('1adce6ef', 7075126), ('64c94865', 2036349), ('cfef1c29', 1395040)]\n",
      "[('2d0bb053', 684365), ('d345b1a0', 474394), ('3628a186', 420885), ('10040656', 405943), ('10935a85', 379091)]\n",
      "[('', 1559473), ('84898b2a', 1141316), ('b041b04a', 996589), ('36103458', 495012), ('c64d548f', 453649)]\n",
      "[('e5ba7672', 21153559), ('07c540c4', 5989891), ('d4bb7bd8', 5260423), ('3486227d', 3830530), ('776ce399', 2409566)]\n",
      "[('e88ffc9d', 1450322), ('891589e7', 1302630), ('2804effd', 1197719), ('c21c3e4c', 1092433), ('5aed7436', 994785)]\n",
      "[('', 20172858), ('21ddcdc9', 15800355), ('55dd3565', 892076), ('5b885066', 369827), ('9437f62f', 336752)]\n",
      "[('', 20172858), ('b1252a9d', 8702869), ('5840adea', 8532904), ('a458ea53', 8431986)]\n",
      "[('', 1559473), ('0014c32a', 1141289), ('723b4dfd', 996574), ('e587c466', 495012), ('5f957280', 453649)]\n",
      "[('', 34955073), ('ad3062eb', 6253065), ('c9d4222a', 3845718), ('78e2e389', 334398), ('8ec974f4', 242748)]\n",
      "[('32c7478e', 20170568), ('3a171ecb', 9191234), ('423fab69', 5530255), ('bcdee96c', 3184415), ('be7c41b4', 2586145)]\n",
      "[('3fdb382b', 2400439), ('b34f3128', 2173528), ('3b183c5c', 2099568), ('1793a828', 1980629), ('', 1559473)]\n",
      "[('', 20172858), ('001f3601', 6557407), ('e8b83407', 4976914), ('ea9a246c', 3674247), ('cb079c2d', 1814255)]\n",
      "[('', 20172858), ('49d68486', 1888619), ('c84c4aec', 857129), ('2fede552', 730377), ('c27f155b', 683020)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This for loop calculates count of the features within each column picks the top 5-features\"\"\"\n",
    "for ii in range(14,40):\n",
    "    \"\"\" Nodes and their meanings \"\"\"\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "    \n",
    "    print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(5,lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 2 - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0: Logistic Regression\n",
    "For the CTR prediction problem, the dependent variable is binary - either the customer clicks on an advertisement or they don't. It is therefore a classification problem, as the dependent variable (click/no-click) is in categorical form. Linear Regression is typically not used for classification problems since it does not perform well when there are outliers. This because the Linear Regression best fit line may not give accurate decision boundaries. \n",
    "\n",
    "Instead, for classification problems we use Logistic Regression. Logistic Regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike Linear Regression which outputs continuous number values, Logistic Regression transforms its output using the logistic sigmoid function which maps any real value into another value between 0 and 1 to return a probability value, which can then be mapped to two or more discrete classes. \n",
    "\n",
    "### 2.1: The Sigmoid Function\n",
    "The sigmoid function can be written as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s(z) = \\frac{1}{1-e^{-z}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(1)\n",
    "\n",
    "where:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s(z)$ is the output, i.e. the probability estimate, between 0 and 1 \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $z$ is the input to the function, i.e. the algorithm’s prediction: $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...$\n",
    "(where $x_i = i$th feature or predictor and $\\beta_i$ is the $i$th coefficient of the model. Note that $x_i$ can be a categorical or continuous variable. If categorical, we need to convert it to a numerical )\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $e$ is the base of natural log\n",
    "\n",
    "The following is a plot of the sigmoid function:\n",
    "\n",
    "<img src=\"sigmoid.png\" width=200>\n",
    "\n",
    "### 2.2: The Loss Function\n",
    "All predictive algorithms use a loss function to be minimized in order to arrive at a prediction. For Linear Regression, we typically use Mean Squared Error (MSE) as the loss or cost function. However, we cannot use MSE as the loss function for Logistic Regression. This is because the prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.\n",
    "\n",
    "Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.\n",
    "\n",
    "Combining the two cost functions, the loss function (which is to be minimized in Logistic Regression) can be written compactly as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1 - y^{(i)})(1 - log(h_{\\theta}(x^{(i)}))]$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(2)\n",
    "\n",
    "where $h_{\\theta}(x)$ is the model expression.\n",
    "\n",
    "Vectorizing this equation we have:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $h = g(X\\theta)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $J(\\theta) = \\frac{1}{m}\\cdot(-y^{T}log(h)-(1 - y)^{T}log(1-h)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(3)\n",
    "\n",
    "where $X$ is the input feature vector, $\\theta$ is the parameter vector and $y$ is the observation vector.\n",
    "\n",
    "### 2.3: Gradient Descent\n",
    "To minimize the cost we use Gradient Descent. We know from calculus that to find the minimum of a function, we take the derivative and solve for the derivative equal to zero. \n",
    "\n",
    "Taking the derivative of the cost function, we have:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $C' = x(s(z)-y)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(4)\n",
    "\n",
    "where\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$C′$ is the derivative of cost with respect to weights\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y$ is the actual class label (0 or 1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s(z)$ is the model’s prediction\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x$ is the feature vector.\n",
    "\n",
    "Steps to be implented include:\n",
    "\n",
    "  1. Calculate gradient average\n",
    "  2. Multiply by learning rate\n",
    "  3. Subtract from weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0: Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the we have an understanding of the impact of the numerical variables and the categorical variables on the Click-Through prediction accuracy, we decided to develop two home-grown logistic regression algorithms. \n",
    "\n",
    "The first algorithm takes all the numerical variables columns[1:14] and does logistic regression on these variables while the second algorithm takes both the numerical and categorical variables and does logistic regression. The goal is to evaluate the increase in accuracy as we include the categorical variables in the logistic regression.\n",
    "\n",
    "To implement logistic regression with gradient descent in a scalable manner/approach, the goal was to ensure that we maintain the same dense representation as we have in the dataset; i.e., be as linear as we can in the space complexity of the problem. This was achieved using Accumulators as counters for the Weights matrix (the array of weight coefficients were stored as `numpy` arrays and initilized to 0's in the Accumulator). While iterating through every row in the RDD, we created a new `DefCost` variable that kept the values of the additional error $y-\\hat{y}$ multiplied by the $x_i$ term. This was then added to the already initialized accumulator (since accumulators can act on mathematical equations that are either commutative and/or associative in nature). This allowed us to keep a shared variable that was updated by every RDD in every partition. \n",
    "\n",
    "This we believe saves large on space complexity because rather than having a sparse matrix and doing sparse matrix multiplication we are instead maintaining the initial dense format of the dataset and only expanding to a denser matrix on the accumulator with every row.\n",
    "\n",
    "#### 4.0.1: Some Core Course Concepts Utilized in this Section:\n",
    "(1) *Accumulators/ Commutative and Associative mathematical equations/operations*:\n",
    "\n",
    "Accumulators are shared variables that allows update from multiple Spark workers. They are used for mathematical equations that demonstrate Commutativity and Associativity. \n",
    "\n",
    "- Example of Commutativity is 3+4 = 4+3. This is the case we are dealing with in the addition of individual weights associated with each \n",
    "column in a sparse matrix dataset.\n",
    "- Example of Associativity is (2+3)+4 = 2+(3+4). Accumulators were used to save and increment numpy arrays of \n",
    "beta coefficients during the gradient descent calculation.\n",
    "\n",
    "(2) *One Hot Encoding (Not)*:\n",
    "\n",
    "To ensure scalability and prevent shuffling of large amount of data over the network our goal was to maintain the compactness of the dataset until computation was about to be done. Thus, after hashing, we optimized the algorithm to run without the need for one hot encoding of the categorical features. This definitely saved on space since one hot encoding of our features would have created an additional 250 columns that mostly consist of 0's.\n",
    "\n",
    "(3) *Gradient-Descent / Batch vs Stochastic*:\n",
    "\n",
    "Gradient descent was used because the minimization of the log loss of the sigmoid function results to a convex optimization problem that has a global minimum. By using the gradient descent with log loss we ensure that we will eventually converge to a local minimum the more iterations we make over the entire dataset.\n",
    "\n",
    "Because we are going through the entire dataset at each epoch before we make a descent along the direction of maximum gradient, the algorithm for now is quite slow. However, to implement this on a larger scale we would need to use either a batch gradient descent methodology or a stochastic gradient descent methodology.\n",
    "\n",
    "The batch gradient descent methodology makes changes to the beta coeffients/weights after receiving a specified number of batches, while the stochastic gradient descent takes a randomized sample from the dataset at every iteration rather than just taking the next row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Logistic Regression On Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression function below takes as input the `NonNumericRDD` from section 1.4, a learning rate and initial weights. It returns the final weights for that epoch as well as the loss.\n",
    "\n",
    "Accumulators are used to keep track of weights and changes in the weight during the epoch.\n",
    "\n",
    "This is a scalable approach and parallelizable. we ensured that there are no nested loops within any of the functions.\n",
    "To ensure faster training and for scaling we can use batch gradient descent/stochastic gradient descent so that we make more updates per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionEvaluator(NonNumericRDD, initialWeights, lr):\n",
    "    \n",
    "    \n",
    "    from pyspark.accumulators import AccumulatorParam\n",
    "    class FloatAccumulatorParam(AccumulatorParam):\n",
    "        \"\"\"\n",
    "        Custom accumulator for use in page rank to keep track of various masses.\n",
    "        IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "        We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        def zero(self, value):\n",
    "            return value\n",
    "        def addInPlace(self, val1, val2):\n",
    "            return (val1 + val2)\n",
    "    mmAccum = sc.accumulator(initialWeights, FloatAccumulatorParam())\n",
    "    \"\"\" This function does the logistic regression and returns updated weights \"\"\"\n",
    "    \n",
    "    def sigmoidFunction(z):\n",
    "        \"\"\" This is the Sigmoid Function \"\"\"\n",
    "    \n",
    "        yhat = 1.0/(1.0 + np.exp(-1 * z))\n",
    "        return (yhat)\n",
    "\n",
    "    \n",
    "    def LogisticLoss(y, yhat):\n",
    "        \"\"\" This computes the Logistic Loss Function \"\"\"\n",
    "    \n",
    "        TotalLoss = ((-y*np.log(yhat)) + ((1-y)*np.log(1- yhat)))\n",
    "        return TotalLoss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def PerRowCalculation(RDDRow):\n",
    "        \"\"\" The calculation made on a per row basis \"\"\"\n",
    "        Key = RDDRow[0]\n",
    "        Value = RDDRow[1]\n",
    "        realY = Value[0]\n",
    "        RestY = Value[1:15]\n",
    "\n",
    "        Y = initialWeights[0]\n",
    "\n",
    "        for i in range(len(RestY) -1):\n",
    "            Y += initialWeights[i+1] * RestY[i]\n",
    "        y_hat = sigmoidFunction(Y)\n",
    "        Error = (realY - y_hat)\n",
    "\n",
    "        DefCost = [Error]\n",
    "        for i in range(len(RestY) - 1):\n",
    "            DefCost.append(Error*RestY[i]) \n",
    "\n",
    "\n",
    "        Loss = realY*np.log(y_hat) + (1-realY)* np.log(1-y_hat)\n",
    "\n",
    "        return((realY, (DefCost, Loss)))\n",
    "    \n",
    "    \n",
    "    def LossSummation(rowRDD):\n",
    "        \"\"\" This calculates the loss and sums them \"\"\"\n",
    "        Key = rowRDD[0]\n",
    "        DefCostLoss = rowRDD[1]\n",
    "        Loss = rowRDD[1][1]\n",
    "\n",
    "        return Loss\n",
    "    \n",
    "    \n",
    "    def AccumulatingArrays(rowRDD):\n",
    "        \"\"\" This function is to accumulate the Accumulator Function \"\"\"\n",
    "\n",
    "        Key = rowRDD[0]\n",
    "        DefCostLoss = rowRDD[1]\n",
    "        Loss = rowRDD[1][1]\n",
    "        DefCost = np.array(rowRDD[1][0])\n",
    "\n",
    "        mmAccum.add(DefCost)\n",
    "    \n",
    "    ################################################################\n",
    "    ########### Logistic Regression Function Begins Here ###########\n",
    "    \n",
    "    Update = NonNumericRDD.map(PerRowCalculation).cache()\n",
    "    FinalLoss = -1* Update.map(LossSummation).mean()\n",
    "    Update.foreach(AccumulatingArrays)\n",
    "    m = Update.count()\n",
    "    FinalWeight = initialWeights - ((1/m)* lr * (mmAccum.value))\n",
    "    \n",
    "    return ((FinalWeight,FinalLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.70179298e-01,  1.84573500e-01, -1.33481739e-02,  6.05789582e-01,\n",
       "         2.26955631e-01,  1.30266695e+02,  5.27975144e+00,  1.45676582e-01,\n",
       "         1.06650906e-02,  1.09239015e+00,  3.21860628e-02, -1.42630337e-02,\n",
       "         2.73556437e-02,  2.47757845e-01]), 0.6931471805599454)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegressionEvaluator(NonNumericRDD,initialWeights,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Evaluating Model Performance - Predicting The Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was used to evaluate the performance of the new model weights obtained after one iteration on the training dataset.\n",
    "The function computes $\\hat{y}$ and compares with $y$. If the values are similar, it outputs a 1 for that row or a 0 is they are different. \n",
    "The code also calculates the True-positive, False-positive, True-negative, False-negative as well as the recall, precision and ROC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccuracyPrediction(Weights, NonNumericRDD):\n",
    "    \n",
    "    \"\"\" This function computes the Accuracy given the weights and RDD \"\"\"\n",
    "\n",
    "    def PredictTest(RDDRow):\n",
    "        Key = RDDRow[0]\n",
    "        Value = RDDRow[1]\n",
    "        realY = Value[0]\n",
    "        RestY = Value[1:15]\n",
    "\n",
    "        Y = Weights[0]\n",
    "\n",
    "        for i in range(len(RestY) -1):\n",
    "            Y += initialWeights[i+1] * RestY[i]\n",
    "        y_hat = round(sigmoidFunction(Y))\n",
    "\n",
    "        if y_hat == realY:\n",
    "            Z = 1\n",
    "        else:\n",
    "            Z = 0\n",
    "\n",
    "\n",
    "        return (((realY, y_hat),Z))\n",
    "    \n",
    "    ######### Accuracy Computation and Return ###########\n",
    "    Yeah = NonNumericRDD.map(PredictTest).values().cache()\n",
    "    Accuracy = Yeah.sum()/ Yeah.count()\n",
    "    \n",
    "    ################ TP, TN, FP, FN, Precision, Recall, ROC ####################\n",
    "    Yeah2 = NonNumericRDD.map(PredictTest).keys().cache()\n",
    "    TP = Yeah2.filter(lambda x: (x[0][0] == x[0][1]) & ( x[0][0]== 1)).count()\n",
    "    TN = Yeah2.filter(lambda x: (x[0][0] == x[0][1]) & ( x[0][0]== 0)).count()\n",
    "    FN = Yeah2.filter(lambda x: (x[0][0] != x[0][1]) & ( x[0][0]== 1)).count()\n",
    "    FP = Yeah2.filter(lambda x: (x[0][0] != x[0][1]) & ( x[0][0]== 0)).count()\n",
    "    \n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    \n",
    "    ROC = 2* Precision*Recall/(Precision + Recall)\n",
    "    \n",
    "    return (Accuracy, ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model after running the logistic regression algorithm is around 26%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2568867173441232"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY = [ 1.70179298e-01,  1.84573500e-01, -1.33481739e-02,  6.05789582e-01,\n",
    "         2.26955631e-01,  1.30266695e+02,  5.27975144e+00,  1.45676582e-01,\n",
    "         1.06650906e-02,  1.09239015e+00,  3.21860628e-02, -1.42630337e-02,\n",
    "         2.73556437e-02,  2.47757845e-01]\n",
    "\n",
    "AccuracyPrediction(YY,NonNumericRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Logistic Regression On Categorical And Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression function below takes as input the `NonNumericRDD` from section 1.4, a learning rate and initial weights. It returns the Final weights for that Epoch as well as the loss. \n",
    "\n",
    "Accumulators are used to keep track of weights and changes in the weight during the epoch.\n",
    "\n",
    "This is a scalable approach and parallelizable. we ensured that there are no nested loops within any of the functions. To ensure faster training and for scaling we can use batch gradient descent/stochastic gradient descnet so that we make more updates per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialWeights = (np.ones(289))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionEvaluator2(NonNumericRDD, initialWeights, lr):\n",
    "    \n",
    "    \n",
    "    from pyspark.accumulators import AccumulatorParam\n",
    "    class FloatAccumulatorParam(AccumulatorParam):\n",
    "        \"\"\"\n",
    "        Custom accumulator for use in page rank to keep track of various masses.\n",
    "        IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "        We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        def zero(self, value):\n",
    "            return value\n",
    "        def addInPlace(self, val1, val2):\n",
    "            return (val1 + val2)\n",
    "    mmAccum = sc.accumulator(initialWeights, FloatAccumulatorParam())\n",
    "    \"\"\" This function does the logistic regression and returns updated weights \"\"\"\n",
    "    \n",
    "    def sigmoidFunction(z):\n",
    "        \"\"\" This is the Sigmoid Function \"\"\"\n",
    "    \n",
    "        yhat = 1.0/(1.0 + np.exp(-1 * z))\n",
    "        return (yhat)\n",
    "\n",
    "    \n",
    "    def LogisticLoss(y, yhat):\n",
    "        \"\"\" This computes the Logistic Loss Function \"\"\"\n",
    "    \n",
    "        TotalLoss = ((-y*np.log(yhat)) + ((1-y)*np.log(1- yhat)))\n",
    "        return TotalLoss\n",
    "    \n",
    "\n",
    "#################### Needs to Change for including Categorical Variables #################\n",
    "    \n",
    "    def PerRowCalculation(RDDRow):\n",
    "        \"\"\" The calculation made on a per row basis \"\"\"\n",
    "        Key = RDDRow[0]\n",
    "        Value = RDDRow[1]\n",
    "        realY = Value[0]\n",
    "        RestY = Value[1:15]\n",
    "        CatY = Value[15:]\n",
    "\n",
    "        Y = initialWeights[0]\n",
    "\n",
    "        for i in range(len(RestY) -1):\n",
    "            Y += initialWeights[i+1] * RestY[i]\n",
    "        \n",
    "        for index,value in enumerate(CatY):\n",
    "            Y += initialWeights[14+(value) + (index*11)]\n",
    "            \n",
    "        y_hat = sigmoidFunction(Y)\n",
    "        Error = (realY - y_hat)\n",
    "\n",
    "        DefCost = [Error]\n",
    "        for i in range(len(RestY) - 1):\n",
    "            DefCost.append(Error*RestY[i]) \n",
    "            \n",
    "        for index,value in enumerate(CatY):\n",
    "            for ii in range(0,11):\n",
    "                if ii != value:\n",
    "                    DefCost.append(0)\n",
    "                elif ii == value:\n",
    "                    DefCost.append(Error*1)\n",
    "                    \n",
    "\n",
    "        if y_hat == 0:   ##Prevent us from having a -inf value\n",
    "            Loss =  (1-realY)* np.log(1-y_hat)\n",
    "        elif y_hat ==1:  ## Prevent us from having a -inf value\n",
    "            Loss = realY*np.log(y_hat) \n",
    "        else:\n",
    "            Loss = realY*np.log(y_hat) + (1-realY)* np.log(1-y_hat)\n",
    "\n",
    "        return((realY, (DefCost, Loss)))\n",
    "#########################################################################################    \n",
    "    \n",
    "    def LossSummation(rowRDD):\n",
    "        \"\"\" This calculates the loss and sums them \"\"\"\n",
    "        Key = rowRDD[0]\n",
    "        DefCostLoss = rowRDD[1]\n",
    "        Loss = rowRDD[1][1]\n",
    "\n",
    "        return Loss\n",
    "    \n",
    "    \n",
    "    def AccumulatingArrays(rowRDD):\n",
    "        \"\"\" This function is to accumulate the Accumulator Function \"\"\"\n",
    "\n",
    "        Key = rowRDD[0]\n",
    "        DefCostLoss = rowRDD[1]\n",
    "        Loss = rowRDD[1][1]\n",
    "        DefCost = np.array(rowRDD[1][0])\n",
    "\n",
    "        mmAccum.add(DefCost)\n",
    "    \n",
    "    ################################################################\n",
    "    ########### Logistic Regression Function Begins Here ###########\n",
    "    \n",
    "    Update = NonNumericRDD.map(PerRowCalculation).cache()\n",
    "    FinalLoss = -1* Update.map(LossSummation).mean()\n",
    "    Update.foreach(AccumulatingArrays)\n",
    "    m = Update.count()\n",
    "    FinalWeight = initialWeights - ((1/m)* lr * (mmAccum.value))\n",
    "    \n",
    "    return ((FinalWeight,FinalLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.51966932,   1.53039054,   0.98597392,   3.19532361,\n",
       "          1.71228969, 293.25008743,  13.34180519,   1.38499477,\n",
       "          1.01294389,   3.63184942,   1.17587723,   1.03227896,\n",
       "          1.11209385,   1.76742003,   0.99976181,   0.99949003,\n",
       "          0.99949613,   0.99950224,   0.99950071,   1.00162459,\n",
       "          1.01630847,   1.13166483,   1.22814606,   1.12380604,\n",
       "          1.01526867,   1.2347238 ,   0.9995282 ,   0.99966714,\n",
       "          1.00037866,   1.00149786,   1.00555016,   1.02217164,\n",
       "          1.0428042 ,   1.05920886,   1.12304872,   1.02599034,\n",
       "          1.12252653,   0.99953278,   0.99965035,   1.00060769,\n",
       "          1.00195286,   1.00688617,   1.03560655,   1.05882867,\n",
       "          1.10683644,   1.15235388,   1.02978766,   0.99967325,\n",
       "          0.99949003,   0.99949003,   0.99949155,   0.99949766,\n",
       "          0.99950377,   0.99954194,   1.0001023 ,   1.51822796,\n",
       "          1.00004581,   0.99950529,   1.06274051,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   1.0127356 ,   1.44315189,   0.9995114 ,\n",
       "          0.99949003,   1.01499231,   0.99949919,   0.99952667,\n",
       "          0.99982441,   1.0054387 ,   1.00750761,   1.0232374 ,\n",
       "          1.09789358,   1.17500191,   1.13040822,   1.0612396 ,\n",
       "          0.99994198,   0.99949003,   0.99949003,   0.99949766,\n",
       "          0.99950377,   0.99951751,   0.99966256,   1.00128257,\n",
       "          1.51630716,   1.00031148,   0.99956484,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   1.45790601,\n",
       "          1.06125334,   0.99949003,   1.03116947,   0.99949155,\n",
       "          0.99954499,   0.9999771 ,   1.00203989,   1.00617465,\n",
       "          1.02407106,   1.0697076 ,   1.28112072,   1.08492742,\n",
       "          1.01634512,   1.0048417 ,   0.99949308,   0.99950988,\n",
       "          0.99998779,   1.00489819,   1.00648919,   1.01263635,\n",
       "          1.09522461,   1.21768549,   1.12874851,   1.0450548 ,\n",
       "          1.21384389,   0.9995282 ,   0.99968241,   1.00038172,\n",
       "          1.00151923,   1.00564788,   1.02241899,   1.04324547,\n",
       "          1.07626397,   1.12573295,   1.02630487,   1.00196355,\n",
       "          0.99949308,   0.99949003,   0.99960149,   1.00464473,\n",
       "          1.00633497,   1.01325932,   1.09086083,   1.23363209,\n",
       "          1.12351747,   1.04177204,   0.99949461,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949461,   0.99960149,\n",
       "          1.00276668,   1.03422015,   1.46448528,   1.00759311,\n",
       "          1.00844358,   1.01044225,   0.99949003,   0.99966409,\n",
       "          1.00016796,   1.00191316,   1.00664035,   1.03516986,\n",
       "          1.07337514,   1.19144016,   1.15051706,   1.04574953,\n",
       "          1.17508436,   0.99952667,   0.99966409,   1.00051608,\n",
       "          1.0014887 ,   1.00608457,   1.02440698,   1.05286168,\n",
       "          1.08807735,   1.13837236,   1.02848677,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   0.99949003,   1.24157944,   1.13739364,\n",
       "          1.12349609,   1.01567024,   1.00340797,   0.99949155,\n",
       "          0.99962592,   0.99985953,   1.0011482 ,   1.00519135,\n",
       "          1.03174968,   1.06293747,   1.23738818,   1.14976737,\n",
       "          1.02400236,   1.22602982,   0.99949003,   0.99949308,\n",
       "          0.99950682,   0.99957553,   0.99968241,   1.00393168,\n",
       "          1.02520247,   1.23963115,   1.01990882,   1.00211777,\n",
       "          1.22447089,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          1.29468846,   0.99949003,   0.99949003,   1.1985462 ,\n",
       "          0.99952972,   0.99976028,   1.00044737,   1.00164749,\n",
       "          1.00583263,   1.02313968,   1.04851316,   1.07850236,\n",
       "          1.13238246,   1.02626823,   1.40026371,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   0.99964882,   1.11873684,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          1.062797  ,   1.41432921,   1.00465847,   1.03635471,\n",
       "          1.05952035,   0.9995282 ,   0.99960912,   1.00050692,\n",
       "          1.00149633,   1.00422942,   1.02657818,   1.06898387,\n",
       "          1.21152916,   1.1137959 ,   1.02879214,   1.22451975,\n",
       "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
       "          0.99949461,   0.99949613,   1.02290148,   1.27032424,\n",
       "          1.0003695 ,   0.99950377,   1.25682826,   0.99949155,\n",
       "          0.99963355,   1.00024583,   1.0009604 ,   1.00284761,\n",
       "          1.01885681,   1.03902062,   1.08595653,   1.08335933,\n",
       "          1.0273691 ]), 10.91438469365193)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegressionEvaluator2(NonNumericRDD,initialWeights,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccuracyPrediction2(initialWeights, NonNumericRDD):\n",
    "    \n",
    "    \"\"\" This function computes the Accuracy given the weights and RDD for Categorical and Numerical Variables \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    def PredictTest(RDDRow):\n",
    "        Key = RDDRow[0]\n",
    "        Value = RDDRow[1]\n",
    "        realY = Value[0]\n",
    "        RestY = Value[1:15]\n",
    "        CatY = Value[15:]\n",
    "\n",
    "        Y = Weights[0]\n",
    "        \n",
    "        \"\"\" This calculates the (Beta*X) values for the Numerical features \"\"\"\n",
    "        for i in range(len(RestY) -1):\n",
    "            Y += initialWeights[i+1] * RestY[i]\n",
    "            \n",
    "        \"\"\" This calculates the (Beta*1) values for the Categorical features \"\"\"\n",
    "        for index,value in enumerate(CatY):\n",
    "            Y += (initialWeights[14+(value) + (index*11)] *1)\n",
    "        \n",
    "        y_hat = round(sigmoidFunction(Y))\n",
    "\n",
    "        if y_hat == realY:\n",
    "            Z = 1\n",
    "        else:\n",
    "            Z = 0\n",
    "\n",
    "\n",
    "        return (((realY, y_hat),Z))\n",
    "    \n",
    "    ######### Accuracy Computation and Return ###########\n",
    "    Yeah = NonNumericRDD.map(PredictTest).values().cache()\n",
    "    Accuracy = Yeah.sum()/ Yeah.count()\n",
    "    \n",
    "    ################ TP, TN, FP, FN, Precision, Recall, ROC ####################\n",
    "    Yeah2 = NonNumericRDD.map(PredictTest).keys().cache()\n",
    "    TP = Yeah2.filter(lambda x: (x[0][0] == x[0][1]) & ( x[0][0]== 1)).count()\n",
    "    TN = Yeah2.filter(lambda x: (x[0][0] == x[0][1]) & ( x[0][0]== 0)).count()\n",
    "    FN = Yeah2.filter(lambda x: (x[0][0] != x[0][1]) & ( x[0][0]== 1)).count()\n",
    "    FP = Yeah2.filter(lambda x: (x[0][0] != x[0][1]) & ( x[0][0]== 0)).count()\n",
    "    \n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    \n",
    "    ROC = 2* Precision*Recall/(Precision + Recall)\n",
    "    \n",
    "    return (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.287969375402166"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY = [  1.51966932,   1.53039054,   0.98597392,   3.19532361,\n",
    "          1.71228969, 293.25008743,  13.34180519,   1.38499477,\n",
    "          1.01294389,   3.63184942,   1.17587723,   1.03227896,\n",
    "          1.11209385,   1.76742003,   0.99976181,   0.99949003,\n",
    "          0.99949613,   0.99950224,   0.99950071,   1.00162459,\n",
    "          1.01630847,   1.13166483,   1.22814606,   1.12380604,\n",
    "          1.01526867,   1.2347238 ,   0.9995282 ,   0.99966714,\n",
    "          1.00037866,   1.00149786,   1.00555016,   1.02217164,\n",
    "          1.0428042 ,   1.05920886,   1.12304872,   1.02599034,\n",
    "          1.12252653,   0.99953278,   0.99965035,   1.00060769,\n",
    "          1.00195286,   1.00688617,   1.03560655,   1.05882867,\n",
    "          1.10683644,   1.15235388,   1.02978766,   0.99967325,\n",
    "          0.99949003,   0.99949003,   0.99949155,   0.99949766,\n",
    "          0.99950377,   0.99954194,   1.0001023 ,   1.51822796,\n",
    "          1.00004581,   0.99950529,   1.06274051,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   1.0127356 ,   1.44315189,   0.9995114 ,\n",
    "          0.99949003,   1.01499231,   0.99949919,   0.99952667,\n",
    "          0.99982441,   1.0054387 ,   1.00750761,   1.0232374 ,\n",
    "          1.09789358,   1.17500191,   1.13040822,   1.0612396 ,\n",
    "          0.99994198,   0.99949003,   0.99949003,   0.99949766,\n",
    "          0.99950377,   0.99951751,   0.99966256,   1.00128257,\n",
    "          1.51630716,   1.00031148,   0.99956484,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   1.45790601,\n",
    "          1.06125334,   0.99949003,   1.03116947,   0.99949155,\n",
    "          0.99954499,   0.9999771 ,   1.00203989,   1.00617465,\n",
    "          1.02407106,   1.0697076 ,   1.28112072,   1.08492742,\n",
    "          1.01634512,   1.0048417 ,   0.99949308,   0.99950988,\n",
    "          0.99998779,   1.00489819,   1.00648919,   1.01263635,\n",
    "          1.09522461,   1.21768549,   1.12874851,   1.0450548 ,\n",
    "          1.21384389,   0.9995282 ,   0.99968241,   1.00038172,\n",
    "          1.00151923,   1.00564788,   1.02241899,   1.04324547,\n",
    "          1.07626397,   1.12573295,   1.02630487,   1.00196355,\n",
    "          0.99949308,   0.99949003,   0.99960149,   1.00464473,\n",
    "          1.00633497,   1.01325932,   1.09086083,   1.23363209,\n",
    "          1.12351747,   1.04177204,   0.99949461,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949461,   0.99960149,\n",
    "          1.00276668,   1.03422015,   1.46448528,   1.00759311,\n",
    "          1.00844358,   1.01044225,   0.99949003,   0.99966409,\n",
    "          1.00016796,   1.00191316,   1.00664035,   1.03516986,\n",
    "          1.07337514,   1.19144016,   1.15051706,   1.04574953,\n",
    "          1.17508436,   0.99952667,   0.99966409,   1.00051608,\n",
    "          1.0014887 ,   1.00608457,   1.02440698,   1.05286168,\n",
    "          1.08807735,   1.13837236,   1.02848677,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   0.99949003,   1.24157944,   1.13739364,\n",
    "          1.12349609,   1.01567024,   1.00340797,   0.99949155,\n",
    "          0.99962592,   0.99985953,   1.0011482 ,   1.00519135,\n",
    "          1.03174968,   1.06293747,   1.23738818,   1.14976737,\n",
    "          1.02400236,   1.22602982,   0.99949003,   0.99949308,\n",
    "          0.99950682,   0.99957553,   0.99968241,   1.00393168,\n",
    "          1.02520247,   1.23963115,   1.01990882,   1.00211777,\n",
    "          1.22447089,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          1.29468846,   0.99949003,   0.99949003,   1.1985462 ,\n",
    "          0.99952972,   0.99976028,   1.00044737,   1.00164749,\n",
    "          1.00583263,   1.02313968,   1.04851316,   1.07850236,\n",
    "          1.13238246,   1.02626823,   1.40026371,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   0.99964882,   1.11873684,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          1.062797  ,   1.41432921,   1.00465847,   1.03635471,\n",
    "          1.05952035,   0.9995282 ,   0.99960912,   1.00050692,\n",
    "          1.00149633,   1.00422942,   1.02657818,   1.06898387,\n",
    "          1.21152916,   1.1137959 ,   1.02879214,   1.22451975,\n",
    "          0.99949003,   0.99949003,   0.99949003,   0.99949003,\n",
    "          0.99949461,   0.99949613,   1.02290148,   1.27032424,\n",
    "          1.0003695 ,   0.99950377,   1.25682826,   0.99949155,\n",
    "          0.99963355,   1.00024583,   1.0009604 ,   1.00284761,\n",
    "          1.01885681,   1.03902062,   1.08595653,   1.08335933,\n",
    "          1.0273691 ]\n",
    "AccuracyPrediction(YY,NonNumericRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Core Course Concepts Used For Algorithm And Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0: Summary\n",
    "(1) *Hashing* -- \n",
    "\n",
    "Although we did not touch on this concept in depth in class, the hashing function as used here is the function that enables us to reduce the cardinality of the categorical variables from several thousands to only 10 or as many as we deem fit. \n",
    "\n",
    "Hashing here was implemented using an home grown algorithm.\n",
    "\n",
    "Home-Grown Hashing Function Algorithm:\n",
    "\n",
    "The hashing function uses the co-occurence count of each feature within a column with the click outcome \"1\" divided by the total count of co-occurence of the feature. This results in what we call a \"co-occurence count of importance\". Basically because this is normalized by the overall count, the values are always between 0 and 1, with 1 showing that the feature is very important (meaning everytime this feature occured we also had the occurence of a count), and 0 meaning the feature is not important at all (meaning everytime this feature occured we did not see the occurence of a count).\n",
    "The hashing function enables us to reduce cardinality and also go from a sparse distribution to a denser representation of the categorical variables.\n",
    "\n",
    "By choosing an hashing from 1-10 we went from a very high dimensional (cardinal) problem to a lower dimensionality. One can argue that even 10 categories for every categorical column is still a lot, will it be more practical to have the categories be 5 instead of 10. Definitely, if computation resource is a bottleneck and given that we know there is some similarity amongst some of the features within a column we can certainly divide the categorical varaibles into 5 classes. Also note that hashing, as discussed previously, enables us to make the data less sparse.\n",
    "\n",
    "(2) *Caching and Broadcasting* -- (small array containing Mean and Standard Deviations), (dictionary containing feature importance of the categorical variables), `NonNumericRDD`\n",
    "\n",
    "Finding the mean and standard deviation for the `trainRDD` (10GB dataset) involves going through the entire dataset.\n",
    "However, mean and standard deviation are supposed to be utilized for mean-normalizing only the `TrainRDD` and the `TestRDD2`. \n",
    "Thus an important concept here is to do the mean and standard deviation calculation and then cache these values.\n",
    "Because the array generated containing both the mean and standard deviation is small, one can broadcast these values for use in mean-normalizing the `TrainRDD` and the `TestRDD2`.\n",
    "\n",
    "In addition, generating the dictionary containing, as keys the categorical feature, and as value the co-occurring feature importance is computationally expensive. The resulting dictionary (a default dict from Python) is not a large data for each column. This data is thus broadcasted for use in the transformation of each column to a hashed value between 1 and 10.\n",
    "\n",
    "Caching was also used multiple times in the initial analysis, this is because some RDDs calculated at an initial time $t_1$  still had multiple uses downstream at times t+1 and t+2. If the RDD were not cached, pyspark would need to reevaluate the RDD everytime a call was made to that RDD. Caching is especially useful when RDD computation is expensive (NonNumericRDDs - These are computationally expensive calculations). Without caching, every call to these RDDs would mean reevaluating them again.\n",
    "\n",
    "While Caching is good, it also takes away computational resource as some part of memory is allocated to the RDD. When all memory is used for Caching the unused RDDs are spilled-to and stored on disk. Although having RDDs in memry ensures faster computation/evaluation than having RDDs on disk, it is still faster re-evaluating RDDs from memory then recomputing/re-evaluating the RDD. Because of the spill-to-disk problem it is definitely worth it storing in memory only RDDs that are computationally expensive to calculate. Once we are done with the RDD one should ensure to unpersist() the RDD to free memory space for other RDDs.\n",
    "\n",
    "To ensure scalability one needed to ensure that RDDs were unpersisted() from memory once they were no longer in use.If the RDDs were not unpersisted() then it is totally concievable that one would run out of memory space for computation and eventually RDDs would spill to disk.\n",
    "\n",
    "\n",
    "(3) *Normalization* --\n",
    "\n",
    "Normalization of features (numerical features) is extremely important especially for gradient descent. Numerical features can have widely varying ranges. The range of values (-inf to inf). These range of values can result to Stochastic Gradient Descent taking a very long time to converge. To prevent this from occuring Normalization of the Numerical variables is done.\n",
    "\n",
    "The method utilized for normalizing here was deployed on the 10GB dataset and it appears to be scalable. The process involves taking a pass through the dataset to calculate the mean and standard deviation of each numerical column and storing each of these values in a list as a set (mean, standard deviation).\n",
    "Because this set is small one can then broadcast this set to the RDD partitions and do another loop through the dataset this time only doing the mena normalization computation ( x_i - mean)/sd.\n",
    "\n",
    "Mean Normalization for SGD: Mean normalization of the numerical variables is very important for Gradient Descent because non normalized features would take a longer time to converge. To ensure that we converge faster we use mean normalization of the features using the array of mean and standard deviation values.\n",
    "\n",
    "\n",
    "(4) *Accumulators/Commutative and Associative Mathematical Equations/Operations* --\n",
    "\n",
    "Accumulators are shared variables that allows update from multiple Spark workers. They are used for mathematical equations that demonstrate commutativity and associativity. \n",
    "\n",
    "- Example of Commutativity: 3+4 = 4+3 - This is the case we are dealing with in the addition of individual weights associated with each column in a sparse matrix dataset) \n",
    "- Example of Associativity: (2+3)+4 = 2+(3+4). Accumulators were used to save and increment numpy arrays of beta coeeficients during the gradient descent calculation.\n",
    "\n",
    "(5) *One Hot Encoding (Not)* --\n",
    "\n",
    "To ensure scalability and prevent shuffling of large amount of data over the network our goal was to maintain the compactness of the dataset until computation was about to be done. Thus, after hashing, we optimized the algorithm to run without the need for one hot encoding of the categorical features. This definitely saved on space since one hot encoding of our features would have created an additional 250 columns that mostly consist of )'s.\n",
    "\n",
    "(6) *Gradient-Descent/Batch vs Stochastic* --\n",
    "\n",
    "Gradient descent was used because the minimization of the log loss of the sigmoid function results to a convex optimization problem that has a global minimum. By using the gradient descent with log loss we ensure that we will eventually converge to a local minimum the more oterations we make over the entire dataset.\n",
    "\n",
    "Because we are going through the entire dataset at each epoch before we make a descent along the direction of maximum gradient, the algorithm for now is quite slow. However, to implement this on a larger scale we would need to use either a batch gradient desent methodology or a stochastic gradient descent methodology.\n",
    "\n",
    "The batch gradient descent methodology makes changes to the beta coeffients/weights after receiving a specified number of batches while the stochastic gradient descent takes a randomized sample from the dataset at every iteration rather than just taking the next row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
