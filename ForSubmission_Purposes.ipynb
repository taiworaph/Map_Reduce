{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "import pyspark\n\nimport re\nimport ast\nimport time\nimport numpy as np\n#import pandas as pd\n\n\nsc = pyspark.SparkContext()\n", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "40f869815c184354ac4dae6c28817c02"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1544586234495_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-203.us-west-2.compute.internal:20888/proxy/application_1544586234495_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-46-9.us-west-2.compute.internal:8042/node/containerlogs/container_1544586234495_0002_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}, {"output_type": "stream", "text": "Cannot run multiple SparkContexts at once; existing SparkContext(app=livy-session-1, master=yarn) created by __init__ at /tmp/1260729744655070957:589 \nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 115, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 316, in _ensure_initialized\n    callsite.function, callsite.file, callsite.linenum))\nValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=livy-session-1, master=yarn) created by __init__ at /tmp/1260729744655070957:589 \n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import SQLContext", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "62e95f9422824c879cffe4678e1c2e13"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "trainRDD = sc.textFile('s3://breastcancerfile/e-AKVLH55FZCUE4SN1JHTY7DWU4/train.txt')", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "33ade4fe4f76456bb4b29f7801ee7d71"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "TrainRDD, TrainRDD2 = trainRDD.randomSplit([0.01,0.99], seed = 2018)\nTrainRDD2.unpersist()", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d6ee551566e0439b9a650333db837611"}}, "metadata": {}}, {"output_type": "stream", "text": "PythonRDD[2] at RDD at PythonRDD.scala:52", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "TrainRDD.count()", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8af652fad4fb4ee6945dd77b624f2df8"}}, "metadata": {}}, {"output_type": "stream", "text": "----------------------------------------\nException happened during processing of request from ('127.0.0.1', 53424)\n----------------------------------------\n458906\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 263, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 238, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 251, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "NodeMeans = [(1.9136993727636809, 7.184627898705608),\n (105.84841979766556, 391.45781841729996),\n (21.13629851448115, 352.8574390110648),\n (5.735263227368864, 8.346464252535688),\n (18060.51214960742, 68556.28645274039),\n (90.10443053155232, 340.53335300283976),\n (15.62662976373116, 64.6908374702787),\n (12.510823839914709, 16.68706965443228),\n (101.51997332409383, 216.54476824937402),\n (0.33741472109766746, 0.59176564119444),\n (2.6146237734976343, 5.115681630473718),\n (0.2328159762771082, 2.7454850748632738),\n (6.436072751813085, 14.741644511057435)]\n\nNeeded = sc.broadcast(NodeMeans)", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "65d05f60c53e4d56be9a292c86941299"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def NumericValuesOnly(LineRDD):\n    \"\"\"Function takes the RDD and subsets the RDD\"\"\"\n    Node = Needed.value\n    Values = LineRDD.split('\\t')\n    ZZ = []\n    for ii in range(0,40):\n        \n        if ii < 14:\n            if Values[ii] == '':\n                ZZ.append(Node[ii-1][0])\n            else:\n                if (ii == 0):\n                    ZZ.append(float(Values[ii]))\n                else:\n                    ZZ.append((float(Values[ii])- float(Node[ii-1][0]))/float(Node[ii-1][1]))\n        else:\n            if Values[ii] == '':\n                ZZ.append(str(0))\n            else:\n                ZZ.append(str(Values[ii]))\n    \n    return (((Values[14], Values[15], Values[16]), ZZ))", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "81388948e20c424996afc63ec986f800"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "NonNumericRDD = TrainRDD.map(NumericValuesOnly).cache()\nNonNumericRDD.take(10)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "19ea7c2e929f4a3786a7d91cd101ad23"}}, "metadata": {}}, {"output_type": "stream", "text": "[((u'05db9164', u'80e26c9b', u'5c7d8ff6'), [1.0, 0.15119789675287548, -0.2729500211023082, 21.13629851448115, 5.735263227368864, -0.25372599727376444, -0.25285168037808703, -0.02514467005437195, -0.7497316244851707, 0.9211953181255527, 1.119675142958533, 0.6617644472509671, 1.007903502757413, 6.436072751813085, '05db9164', '80e26c9b', '5c7d8ff6', '902872c9', '384874ce', '7e0ccccf', '92ce5a7d', '0b153874', 'a73ee510', 'd1a1d478', 'e0c3cae0', '79b87c55', 'e8df3343', 'b28479f6', '4c1df281', '5627d7e0', '8efede7f', 'f54016b9', '21ddcdc9', 'b1252a9d', '4063500f', '0', '3a171ecb', '54baf4d1', 'e8b83407', '98f9ccac']), ((u'68fd1e64', u'78ccd99e', u'24ef584c'), [0.0, 1.9136993727636809, -0.27039546744939336, 21.13629851448115, -0.327715203061929, -0.234384226174274, -0.24991511046156653, -0.1488097872925819, -0.4500984292301882, -0.4457275698895711, 0.33741472109766746, -0.3156224116605392, 0.2328159762771082, -0.23308612205617563, '68fd1e64', '78ccd99e', '24ef584c', 'c8167e68', '4cf72387', '0', '1b76cf1e', '1f89b562', 'a73ee510', 'b43f9937', '0d8e34fa', 'e9e2c23f', 'e21429e2', '051219e6', '9917ad07', 'f95cc162', '1e88c74f', 'e7e991cb', '21ddcdc9', 'a458ea53', 'f0db6227', '0', '32c7478e', '1f163fc7', 'ea9a246c', '8c532d04']), ((u'05db9164', u'78ccd99e', u'339af0cf'), [1.0, 1.9136993727636809, -0.27039546744939336, -0.04006235082955944, -0.20790399082362143, -0.24052516556559037, -0.24991511046156653, -0.1797260666021344, -0.4500984292301882, -0.3348959843748313, 0.33741472109766746, -0.12014503987823798, 0.2328159762771082, -0.09741604817128192, '05db9164', '78ccd99e', '339af0cf', 'bcf28591', '25c83c98', '7e0ccccf', '81bb0302', '0b153874', 'a73ee510', '2ec6a85f', 'b7094596', '7769555d', '1f9d2c38', 'b28479f6', '1ca2ec64', 'e73be0e3', 'e5ba7672', 'e7e991cb', '790d09cc', 'b1252a9d', '294b378d', '0', '32c7478e', '735a8fe9', '33d94071', '8af183b4']), ((u'05db9164', u'1550810c', u'12970416'), [0.0, -0.26636026245819294, 1.7732474548825037, 21.13629851448115, -0.44752641530023646, -0.23171488672389184, -0.14126202355030748, 0.021229748909956783, -0.33024515112819525, -0.04858105512842025, -0.5701830211308282, 0.07533233190406327, 0.2328159762771082, -0.029581011228835073, '05db9164', '1550810c', '12970416', '971b9e1f', '43b19349', 'fe6b92e5', 'fda1a50f', '062b5529', 'a73ee510', '3b08e48b', 'd2b7c44b', 'e90a010f', '68637c0d', 'b28479f6', '465dad55', 'a8a75566', 'e5ba7672', '6e200add', '21ddcdc9', '5840adea', '55e899dd', '0', 'bcdee96c', '3b425be9', '010f6491', '2fede552']), ((u'05db9164', u'8947f767', u'56c2d9d8'), [0.0, 1.9136993727636809, -0.0507038532987144, -0.03722834511098185, 5.735263227368864, -0.24523662262828674, 90.10443053155232, -0.24155862522123936, -0.7497316244851707, -0.4688174835384752, 0.33741472109766746, -0.5110997834428405, 0.2328159762771082, 6.436072751813085, '05db9164', '8947f767', '56c2d9d8', '428cff52', '25c83c98', '7e0ccccf', '925f31a7', '1f89b562', 'a73ee510', '3b08e48b', 'a4d2f869', '7092f5c1', '6b7b3066', '07d13a8f', '2c14c412', '15da0096', '776ce399', 'bd17c3da', '21ddcdc9', 'b1252a9d', 'a9ff294b', '0', 'c7dc6720', '0b351a52', 'e8b83407', 'b1c17344']), ((u'68fd1e64', u'8db5bc37', u'2ee4f385'), [1.0, -0.26636026245819294, -0.27039546744939336, 21.13629851448115, 5.735263227368864, 0.055844446198697856, 90.10443053155232, 15.62662976373116, -0.7497316244851707, 101.51997332409383, -0.5701830211308282, 2.6146237734976343, 0.2328159762771082, 6.436072751813085, '68fd1e64', '8db5bc37', '2ee4f385', 'c83f7247', '25c83c98', '7e0ccccf', 'b5e1898d', '0b153874', 'a73ee510', '3b08e48b', '165c12cf', 'a763084b', 'cb2e33ed', '64c94865', '00e52733', '7605dea5', 'e5ba7672', '821c30b8', '0', '0', '7b082c3f', '0', 'be7c41b4', '6feea34b', '0', '0']), ((u'05db9164', u'38a947a1', u'b9bc789e'), [0.0, -0.26636026245819294, 0.7309895644932363, 21.13629851448115, -0.567337627538544, -0.24210051343794703, -0.2058665617137588, 0.42314137993413914, -0.6898049854341742, -0.4318736217002286, -0.5701830211308282, 0.07533233190406327, 0.2328159762771082, -0.36875619594106934, '05db9164', '38a947a1', 'b9bc789e', '6a14f9b9', '25c83c98', 'fbad5c96', 'eabf23ff', '5b392875', 'a73ee510', '3b08e48b', 'c7435d5a', '4940d4ed', '4d7749d6', '07d13a8f', '78530a26', 'f8b34416', 'e5ba7672', 'c9ac134a', '0', '0', 'f3ddd519', '0', '32c7478e', 'b34f3128', '0', '0']), ((u'05db9164', u'404660bb', u'341322e5'), [0.0, -0.26636026245819294, -0.27039546744939336, 0.013783757823414726, 1.4694529805126837, -0.22008940288807446, 0.5224027775833288, 0.42314137993413914, 0.029314683177783733, 12.18445519606098, -0.5701830211308282, 1.0527191908155695, -0.0847995781906418, 4.17619127920287, '05db9164', '404660bb', '341322e5', 'ea483df3', '25c83c98', 'fbad5c96', 'ec258437', '5b392875', 'a73ee510', 'fa7d0797', '68471789', 'd818f210', 'f948ca5d', 'b28479f6', 'abcca5c1', '06f33c3d', '8efede7f', '4b17f8a2', '21ddcdc9', '5840adea', 'fbca226b', '0', '32c7478e', '8418f43d', 'f0f449dd', 'aaf35019']), ((u'5bfa8ab5', u'f0cf0024', u'a1acf62a'), [0.0, -0.26636026245819294, -0.26784091379647845, -0.04573036226671461, 0.15152964589130113, 0.01717549055409071, 0.792567209903216, -0.19518420625691063, -0.5100250682811848, 0.5609926652026485, -0.5701830211308282, -0.3156224116605392, 0.2328159762771082, 0.37742921042584604, '5bfa8ab5', 'f0cf0024', 'a1acf62a', '651d542f', '25c83c98', 'fbad5c96', '5b07414c', '062b5529', 'a73ee510', '86cff510', '54649d62', '51d098b7', 'c679a49f', 'cfef1c29', '131dff63', '2e97807d', '07c540c4', 'cc693e93', '21ddcdc9', '5840adea', '9ce3d12f', 'c9d4222a', '32c7478e', '03ba7282', 'ea9a246c', '983e320a']), ((u'05db9164', u'4f25e98b', u'886f259e'), [0.0, 1.9136993727636809, -0.25251359187898925, 0.08463390078785442, 0.39115207036791616, -0.23240045477945448, 0.569387896247657, -0.24155862522123936, 0.6285810736877487, -0.34413194983439294, 0.33741472109766746, -0.5110997834428405, 0.2328159762771082, 0.1739240995985055, '05db9164', '4f25e98b', '886f259e', 'bf1e296a', '25c83c98', '0', 'b5043bcd', '1f89b562', 'a73ee510', '3013a9ec', '371dae82', '1583d70f', '18fc2b1e', '051219e6', 'b6dcf31f', '5fd31a15', 'd4bb7bd8', '7ef5affa', '2e30f394', 'a458ea53', '45981ea0', '0', '32c7478e', 'c892aaad', '001f3601', '9f025d1e'])]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "StartFromA = 14\nfor ii in range(14, 40):\n    Node = ii\n    def ValuesNonNumericFeatures(toyRDDLine):\n\n        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n        Values = toyRDDLine.split('\\t')\n        T14 = Values[Node]\n        return (T14, 1)\n\n\n    def NonNumericFeatures(toyRDDLine):\n\n        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n        Values = toyRDDLine.split('\\t')\n        T14 = Values[Node]\n        T0 = Values[0]\n        return (((T14, T0), 1))\n\n    def ConvertForMerge(toyRDDLine):\n\n        YY = toyRDDLine[0][0]\n        YYY = toyRDDLine[0][1]\n        YYZ = toyRDDLine[1]\n\n        return ((YY,(YYY,YYZ)))\n\n\n    def MovingOn(trainRDD):\n\n        Key = trainRDD[0]\n        Value = trainRDD[1]\n        \n        ValueKey = Value[0][0]\n        ValueValue = Value[0][1]\n        ValueDivide = Value[1]\n        \n        return ((Key, (ValueKey, float(ValueValue/ValueDivide))))\n\n\n    def HasherFunction(trainRDD):\n    \n        Key = trainRDD[0]\n        Value = trainRDD[1][1]\n        \n        if (Value >0.9):\n            FinalValue = 1\n        elif (Value <=0.9) & (Value >0.8):\n            FinalValue = 2\n        elif (Value <=0.8) & (Value >0.7):\n            FinalValue = 3\n        elif (Value <=0.7) & (Value>0.6):\n            FinalValue = 4\n        elif (Value <=0.6) & (Value >0.5):\n            FinalValue = 5\n        elif (Value <=0.5) & (Value >0.4):\n            FinalValue = 6\n        elif (Value <=0.4) & (Value >0.3):\n            FinalValue = 7\n        elif (Value <=0.3) & (Value >0.2):\n            FinalValue =8\n        elif (Value <=0.2) & (Value >0.1):\n            FinalValue = 9\n        elif (Value <= 0.1):\n            FinalValue = 10\n            \n        return ((Key, FinalValue))\n\n\n#def ConvertToSignificance(toyRDDLine):\n#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n    \n    YY = TrainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).cache()\n\n    ZZ = TrainRDD.map(NonNumericFeatures)\\\n                 .reduceByKey(lambda x,y: x+y)\\\n                 .map(ConvertForMerge)\\\n                 .filter(lambda x: x[1][0] == '1')\\\n                 .leftOuterJoin(YY)\\\n                 .map(MovingOn).map(HasherFunction).cache()\n    TT = ZZ.collect()\n\n    ZZ.unpersist()\n    YY.unpersist()\n\n    #NamingDict = \"HashDictionary\" + str(Node)\n    from collections import defaultdict\n\n    NamingDict = defaultdict(list)\n\n    for ii in TT:\n        NamingDict[ii[0]] = ii[1]\n        \n\n        \n    YYY = sc.broadcast(NamingDict)\n    def MappingChangesWithDictionary(trainRDD):\n\n\n        \"\"\" Mapping Changes with What the Dictionary Has \"\"\"\n        Dictionary = YYY.value\n\n        \"\"\" Taking in all the Key/Value Components \"\"\"\n        FinalKey = trainRDD[0]\n        \n\n\n        Value = trainRDD[1]\n        Value[Node] = Dictionary.get(Value[Node], 0)\n\n        return (FinalKey,Value)\n\n\n\n    NonNumericRDD = NonNumericRDD.map(MappingChangesWithDictionary).cache()\n    \n    \nNonNumericRDD.take(1)", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b4a158cf051b47439e2b8084e64b9248"}}, "metadata": {}}, {"output_type": "stream", "text": "[((u'05db9164', u'80e26c9b', u'5c7d8ff6'), [1.0, 0.15119789675287548, -0.2729500211023082, 21.13629851448115, 5.735263227368864, -0.25372599727376444, -0.25285168037808703, -0.02514467005437195, -0.7497316244851707, 0.9211953181255527, 1.119675142958533, 0.6617644472509671, 1.007903502757413, 6.436072751813085, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 10, 10, 10, 10])]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def sigmaFunction(z):\n    yhat = 1.0/(1.0 + np.exp(-1 * z))\n    return (yhat)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3b4a7fec475448d9817b28e8ce125f7a"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "initialWeights = (np.ones(300))", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8fe1960595aa493c99c8dd5795787bbc"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "lrate = 0.7\ninitialWeights = list(np.ones(300))\ndef predict(RDDRow):\n    \n    \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n    Key = RDDRow[0]\n    Value = RDDRow[1]\n    realY = Value[0]\n    RestY = Value[1:14]\n    RestYCat = Value[14:]\n    #RestY.insert(1,0)\n    initialWeights = initWeight.value\n    Y = initialWeights[0]\n    \n    for i in range(len(RestY)-1):\n        \n        Y+= initialWeights[i+1] *RestY[i]\n    \n    for index,value in enumerate(RestYCat):\n        Y += initialWeights[14+(value)+(index*11)]\n    \n        \n    ZZ = sigmaFunction(Y)\n    \n    Error = (realY - ZZ)\n    ErrorSquared = Error**2\n    \n    \n    for index,value in enumerate(initialWeights):\n\n        if index == 0:\n            initialWeights[index] = initialWeights[index] + (lrate * Error * ZZ * (1.0 - ZZ))\n        else:\n            \n            if index <= 14:\n                initialWeights[index] = initialWeights[index] + ((lrate* Error * ZZ * (1.0 - ZZ))*Value[index-1])\n    \n    for indices, values in enumerate(RestYCat):\n        \n        \n        IndexToUpdate = 14+values +(indices*11)\n        initialWeights[IndexToUpdate] = initialWeights[IndexToUpdate] + ((lrate* Error * ZZ * (1.0 - ZZ)))\n                \n                                                \n\n    \n    \n    return(initialWeights)", "execution_count": 32, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d0d5eb1c5cd3466d955053e664960b04"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "PredictionCalc = NonNumericRDD.map(predict).cache()\nY = PredictionCalc.collect()[-1]\nY", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f52f761997014c349a497eccaa147060"}}, "metadata": {}}, {"output_type": "stream", "text": "[0.9999999995924044, 1.0, 1.000000000062389, 1.000000000097784, 1.0000000000097127, 1.0000000001410299, 1.000000000073139, 1.000000000029065, 1.0000000000740206, 1.000000000192697, 1.000000000099937, 1.0000000001310472, 1.0000000001001772, 0.9999999999075673, 1.0000000000276408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999879455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996044588, 0.9999999997636152, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998287892, 0.9999999999191619, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996732425, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999035522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996888522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999547172, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996376872, 0.9999999999999936, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924108, 0.9999999997652532, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998271512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.999999999978634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996137704, 0.9999999997944371, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997979673, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999797423, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996126621, 0.9999999998694905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229139, 0.9999999998694968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229075, 0.9999999997718119, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998205925, 0.9999999996101715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999822329, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999771013, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996153031, 0.9999999998694968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229075, 0.9999999998655218, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997268826]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "lrate = 0.7\ninitialWeights = Y\nPredictionCalc = NonNumericRDD.map(predict).cache()\n\nY = PredictionCalc.collect()[-1]\nY", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b36101291c4b48bd8044651b235cd909"}}, "metadata": {}}, {"output_type": "stream", "text": "[0.9999999991848088, 1.0, 1.0000000001247777, 1.000000000195568, 1.0000000000194251, 1.0000000002820597, 1.000000000146278, 1.00000000005813, 1.0000000001480411, 1.000000000385394, 1.000000000199874, 1.000000000262094, 1.0000000002003544, 0.9999999998151345, 1.000000000055281, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999758911, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992089177, 0.9999999995272304, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996575784, 0.9999999998383238, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999999999346485, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999998071043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999993777045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999094344, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992753743, 0.9999999999999871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848216, 0.9999999995305064, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996543023, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.999999999957268, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992275408, 0.9999999995888742, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995959346, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999594846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992253241, 0.999999999738981, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458277, 0.9999999997389937, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458151, 0.9999999995436237, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999999999641185, 0.999999999220343, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999644658, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999542026, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992306061, 0.9999999997389937, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458151, 0.9999999997310436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994537652]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "lrate = 0.9\ninitialWeights = Y\nPredictionCalc = NonNumericRDD.map(predict).cache()\n\nY = PredictionCalc.collect()[-1]\nY", "execution_count": 23, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c4a848e03d8a4aa1ab7ec30a8d112d19"}}, "metadata": {}}, {"output_type": "stream", "text": "[0.9999999987772131, 1.0, 1.0000000001871665, 1.000000000293352, 1.0000000000291376, 1.0000000004230896, 1.0000000002194172, 1.000000000087195, 1.0000000002220617, 1.000000000578091, 1.0000000002998108, 1.0000000003931406, 1.0000000003005316, 0.9999999997227018, 1.000000000082921, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999999638366, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988133765, 0.9999999992908456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994863675, 0.9999999997574857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999990197275, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999997106565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999990665567, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999998641517, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999989130615, 0.9999999999999807, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772324, 0.9999999992957597, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994814535, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999999359019, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988413112, 0.9999999993833113, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999993939018, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.999999999939227, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988379862, 0.9999999996084715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687416, 0.9999999996084905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687226, 0.9999999993154356, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994617775, 0.9999999988305145, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999466986, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.999999999931304, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988459092, 0.9999999996084905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687226, 0.9999999995965654, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991806477]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def realLoss(toyRDD):\n    \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n    Key = toyRDD[0]\n    Value = toyRDD[1]\n    realY = Value[0]\n    RestY = Value[1:14]\n    RestYCat = Value[14:]\n    #RestY.insert(1,0)\n    \n    initialWeights = initWeight.value\n    Y = initialWeights[0]\n    \n    for i in range(len(RestY)-1):\n        \n        Y+= initialWeights[i+1] *RestY[i]\n    \n    for index,value in enumerate(RestYCat):\n        Y += initialWeights[14+(value)+(index*11)]\n    \n        \n    ZZ = sigmaFunction(Y)\n    \n    Error = (realY - ZZ)\n    return Error", "execution_count": 33, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3e4e78eef8a248aa916f2c0810709efb"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "LossShift = []\ninitWeights = list(np.ones(300))\ninitWeight = sc.broadcast(initWeights)\n\nfor ii in range(40):\n    lrate = 0.9\n    PredictionCalc = NonNumericRDD.map(predict).cache()\n    Y = PredictionCalc.collect()[-1]\n    print(\"This is Run: \", ii)\n    Y\n    initWeight = sc.broadcast(Y)\n    Looser = NonNumericRDD.map(realLoss).mean()\n    LossShift.append(Looser)", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e7d6f07b5f334765ba84d20c084e3d66"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "LossShift", "execution_count": 31, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cee2315669ee4e32aa8637dd76f851b0"}}, "metadata": {}}, {"output_type": "stream", "text": "[-0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def RunningEpochs(Epoch, lrate, initialWeights):\n    Epoch = Epoch\n    lrate = lrate\n\n########################################################################\n#################### Initial Start of the Function #####################\n\n    def predict(RDDRow):\n    \n        \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n        Key = RDDRow[0]\n        Value = RDDRow[1]\n        realY = Value[0]\n        RestY = Value[1:14]\n        RestYCat = Value[14:]\n        #RestY.insert(1,0)\n\n        Y = initialWeights[0]\n\n        for i in range(len(RestY)-1):\n\n            Y+= initialWeights[i+1] *RestY[i]\n\n        for index,value in enumerate(RestYCat):\n            Y += initialWeights[14+(value)+(index*11)]\n\n\n        ZZ = sigmaFunction(Y)\n\n        Error = (realY - ZZ)\n        ErrorSquared = Error**2\n\n\n        for index,value in enumerate(initialWeights):\n\n            if index == 0:\n                initialWeights[index] = initialWeights[index] + (lrate * Error * ZZ * (1.0 - ZZ))\n            else:\n\n                if index <= 14:\n                    initialWeights[index] = initialWeights[index] + ((lrate* Error * ZZ * (1.0 - ZZ))*Value[index-1])\n\n        for indices, values in enumerate(RestYCat):\n\n            IndexToUpdate = 14+values +(indices*11)\n            initialWeights[IndexToUpdate] = initialWeights[IndexToUpdate] + ((lrate* Error * ZZ * (1.0 - ZZ)))\n            \n        return(initialWeights)\n    \n################################################################################ \n##################### Final End of The Function ################################\n\n    for ii in range(Epoch):\n        \n        \n        PredictnewWeight= NonNumericRDD.map(predict).collect()[-1]\n        initialWeights = PredictionCalc\n    return initialWeights  \n\n", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "48d0c53cd2534913b4a2778f9d60f22e"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "YY = list(np.ones(300))\nRunningEpochs(20, 0.89, YY)", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8475f7886b3044dd8e97733b24fa85d5"}}, "metadata": {}}, {"output_type": "stream", "text": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\nTraceback (most recent call last):\n  File \"<stdin>\", line 56, in RunningEpochs\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2472, in _jrdd\n    self._jrdd_deserializer, profiler)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2405, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2391, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 575, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 918, in dumps\n    cp.dump(obj)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 249, in dump\n    raise pickle.PicklingError(msg)\nPicklingError: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "def MakingRDDDataFrames(rowRDD):\n    \n    \"\"\"Getting the RDD in the format of Sets and Lists\"\"\"\n    Key = rowRDD[0]\n    ####################\n    String1 = str(Key[0])\n    String2 = str(Key[1])\n    String3 = str(Key[2])\n    FinalKey = String1 + String2 + String3\n    \n    Values = rowRDD[1]\n    \n    ### Getting each individual Value WoW\n    Num1 = Values[0]\n    Num2 = Values[1]\n    Num3 = Values[3]\n    Num4 = Values[4]\n    Num5 = Values[5]\n    Num6 = Values[6]\n    Num7 = Values[7]\n    Num8 = Values[8]\n    Num9 = Values[9]\n    Num10 = Values[10]\n    Num11 = Values[11]\n    Num12 = Values[12]\n    Num13 = Values[13]\n    \n    return(FinalKey, Num1, Num2, Num3, Num4, Num5, Num6, Num7, Num8, Num9, Num10, Num11, Num12, Num13)", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "31ebbde95de3476f94a987942281cf5f"}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nInvalid status code '404' from https://172.31.44.68:18888/sessions/2 with error payload: \"Session '2' not found.\"\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Working with a random split 70:30 Toy Example -- The best splitting criterion"}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "NumericRDD = TrainRDD.map(NumericValuesOnly).map(MakingRDDDataFrames).cache()\nNumericRDD.take(10)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dc07049311114ac2ba3ea9a7ecb214d3"}}, "metadata": {}}, {"output_type": "stream", "text": "name 'MakingRDDDataFrames' is not defined\nTraceback (most recent call last):\nNameError: name 'MakingRDDDataFrames' is not defined\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "NumericRDD1 = TrainRDD.map(NumericValuesOnly).cache()\nNumericRDD1.take(10)", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6186e436059f439994bb423738e0731f"}}, "metadata": {}}, {"output_type": "stream", "text": "----------------------------------------\nException happened during processing of request from ('127.0.0.1', 47956)\n----------------------------------------\n[('68fd1e6480e26c9bfb936136', [0.0, -0.1271742093878368, -0.26784091379647845, -0.04573036226671461, -0.6871488397768515, -0.24328202434221455, -0.25285168037808703, -0.009686530399595706, -0.6298783463831777, 0.36703739055185386, 1.119675142958533, -0.12014503987823798, 0, -0.3009211589986225]), ('68fd1e64f0cf00246f67f7e5', [0.0, 0.01201184368251934, -0.27039546744939336, 0.06479586075781131, -0.567337627538544, -0.261952813940516, -0.24110540071200498, -0.21064234591168687, -0.6298783463831777, -0.4503455526193519, 1.119675142958533, -0.3156224116605392, 0, -0.16525108511372877]), ('8cf07265ae46a29dc81688bb', [0.0, 0.15119789675287548, -0.2729500211023082, 0, -0.6871488397768515, -0.2634114693778833, -0.2645979600441691, -0.19518420625691063, -0.7497316244851707, -0.4688174835384752, 1.119675142958533, -0.3156224116605392, 0, -0.4365912328835162]), ('05db91646c9c9cf32730ec9c', [0.0, 0, -0.2729500211023082, 0, 0, -0.07638266919864797, 0, -0.24155862522123936, -0.7497316244851707, -0.44110958715979026, 0, -0.5110997834428405, 0, 0]), ('439a44a4ad4527a2c02372d0', [0.0, 0, -0.26784091379647845, -0.05423237942244738, 0, -0.21723043823083457, 0, -0.24155862522123936, -0.6898049854341742, -0.4595815180789135, 0, -0.5110997834428405, 0, 0]), ('68fd1e642c16a946503b9dbc', [1.0, -0.1271742093878368, -0.26017725283773385, -0.05423237942244738, -0.6871488397768515, -0.2634406424866306, -0.2645979600441691, -0.22610048556646312, -0.7497316244851707, -0.4688174835384752, 1.119675142958533, -0.3156224116605392, 0, -0.4365912328835162]), ('05db9164d833535fd032c263', [0.0, 0, -0.157995106721139, -0.0485643679852922, 0.2713408581296086, 0.013849756156893262, 0.46660794916943893, 0.19126928511249547, 1.1079941860957208, 0.18231808136062091, 0, -0.3156224116605392, 0, 0.10608906265605864]), ('05db9164510b40a5d03e7c24', [0.0, 0, -0.18098608959737283, 0, -0.567337627538544, 0.22866594241797567, -0.20292999179723828, -0.22610048556646312, -0.6298783463831777, -0.4549635353491327, 0, -0.3156224116605392, 0, -0.36875619594106934]), ('05db91649b5fd12f', [0.0, -0.26636026245819294, -0.2550681455319041, -0.042896356548137025, 0.0317184336529936, -0.2572997030953143, 0.05548816085656697, -0.22610048556646312, -0.33024515112819525, 0.025306668548072925, -0.5701830211308282, -0.3156224116605392, 0, -0.029581011228835073]), ('241546e038a947a1fa673455', [1.0, -0.26636026245819294, -0.2729500211023082, 0, 0, -0.2420713403291997, -0.2645979600441691, 0.021229748909956783, -0.7497316244851707, -0.4503455526193519, -0.5701830211308282, 0.27080970368636453, 0, 0])]\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 263, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 238, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 251, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nschema = StructType([StructField(\"Keys\", StringType(), False),\n                    StructField(\"Num1\", DoubleType(), False),\n                    StructField(\"Num2\", DoubleType(), False),\n                    StructField(\"Num3\", DoubleType(), False),\n                    StructField(\"Num4\", DoubleType(), False),\n                    StructField(\"Num5\", DoubleType(), False),\n                    StructField(\"Num6\", DoubleType(), False),\n                    StructField(\"Num7\", DoubleType(), False),\n                    StructField(\"Num8\", DoubleType(), False),\n                    StructField(\"Num9\", DoubleType(), False),\n                    StructField(\"Num10\", DoubleType(), False),\n                    StructField(\"Num11\", DoubleType(), False),\n                    StructField(\"Num12\", DoubleType(), False),\n                    StructField(\"Num13\", DoubleType(), False),])", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "60d748bf43004ffaac4befbec9f9af6b"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Numeric = NumericRDD.map(lambda y: Row(Keys = y[0], Num1 = float(y[1]), Num2 = float(y[2]), Num3 = float(y[3]),\n                                   Num4 = float(y[4]), Num5 = float(y[5]), Num6 = float(y[6]), Num7 = float(y[7]),\n                                   Num8 = float(y[8]), Num9 = float(y[9]), Num10 = float(y[10]), Num11 = float(y[11]),\n                                   Num12 = float(y[12]), Num13 = float(y[13])))", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "21ebf38688af4cbda09396a6a21d2474"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "DataFrame = spark.createDataFrame(Numeric, schema)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ce33cbf2417648499a3dd3b72a0f395a"}}, "metadata": {}}, {"output_type": "stream", "text": "u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 693, in createDataFrame\n    jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\nAnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "DataFrame.head(20)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def NonNumericValuesOnly(LineRDD):\n    \"\"\"Function takes the RDD and subsets the RDD\"\"\"\n    Values = LineRDD.split('\\t')\n    ZZ = []\n    RealKey = str(Values[14]) + str(Values[15]) + str(Values[16])\n    for ii in range(14,40):\n        if Values[ii] == '':\n            ZZ.append(str(0))\n        else:\n            ZZ.append(str(Values[ii]))\n    \n    return (RealKey, ZZ)\n\n\ndef MakingRDDDataFrames(rowRDD):\n    \n    \"\"\"Getting the RDD in the format of Sets and Lists\"\"\"\n    Key = rowRDD[0]\n    ####################\n    \n    Values = rowRDD[1]\n    \n    ### Getting each individual Value WoW\n    Str14 = Values[0]\n    Str15 = Values[1]\n    Str16 = Values[3]\n    Str17 = Values[4]\n    Str18 = Values[5]\n    Str19 = Values[6]\n    Str20 = Values[7]\n    Str21 = Values[8]\n    Str22 = Values[9]\n    Str23 = Values[10]\n    Str24 = Values[11]\n    Str25 = Values[12]\n    Str26 = Values[13]\n    Str27 = Values[14]\n    Str28 = Values[15]\n    Str29 = Values[16]\n    Str30 = Values[17]\n    Str31 = Values[18]\n    Str32 = Values[19]\n    Str33 = Values[20]\n    Str34 = Values[21]\n    Str35 = Values[22]\n    Str36 = Values[23]\n    Str37 = Values[24]\n    Str38 = Values[25]\n    Str39 = Values[26]\n    \n    return(FinalKey, Str14, Str15, Str16, Str17, Str18, Str19, Str20, Str21, Str22, Str23, Str24, Str25, Str26, Str27, Str28, Str29,\n          Str30, Str31, Str32, Str33, Str34, Str35, Str36, Str37, Str38, Str39)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2a07c86ea4a14a1ba3d1fd7c1eb551c1"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "NonNumericRDD = TrainRDD.map(NonNumericValuesOnly).cache()\nNonNumericRDD.take(20)\n", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "93d9a79cf4364c8ab984e5df69c657f2"}}, "metadata": {}}, {"output_type": "stream", "text": "[('68fd1e6480e26c9bfb936136', ['68fd1e64', '80e26c9b', 'fb936136', '7b4723c4', '25c83c98', '7e0ccccf', 'de7995b8', '1f89b562', 'a73ee510', 'a8cd5504', 'b2cb9c98', '37c9c164', '2824a5f6', '1adce6ef', '8ba8b39a', '891b62e7', 'e5ba7672', 'f54016b9', '21ddcdc9', 'b1252a9d', '07b5194c', '0', '3a171ecb', 'c5c50484', 'e8b83407', '9727dd16']), ('68fd1e64f0cf00246f67f7e5', ['68fd1e64', 'f0cf0024', '6f67f7e5', '41274cd7', '25c83c98', 'fe6b92e5', '922afcc0', '0b153874', 'a73ee510', '2b53e5fb', '4f1b46f3', '623049e6', 'd7020589', 'b28479f6', 'e6c5b5cd', 'c92f3b61', '07c540c4', 'b04e4670', '21ddcdc9', '5840adea', '60f6221e', '0', '3a171ecb', '43f13e8b', 'e8b83407', '731c3655']), ('8cf07265ae46a29dc81688bb', ['8cf07265', 'ae46a29d', 'c81688bb', 'f922efad', '25c83c98', '13718bbd', 'ad9fa255', '0b153874', 'a73ee510', '5282c137', 'e5d8af57', '66a76a26', 'f06c53ac', '1adce6ef', '8ff4b403', '01adbab4', '1e88c74f', '26b3c7a7', '0', '0', '21c9516a', '0', '32c7478e', 'b34f3128', '0', '0']), ('05db91646c9c9cf32730ec9c', ['05db9164', '6c9c9cf3', '2730ec9c', '5400db8b', '43b19349', '6f6d9be8', '53b5f978', '0b153874', 'a73ee510', '3b08e48b', '91e8fc27', 'be45b877', '9ff13f22', '07d13a8f', '06969a20', '9bc7fff5', '776ce399', '92555263', '0', '0', '242bb710', '8ec974f4', 'be7c41b4', '72c78f11', '0', '0']), ('439a44a4ad4527a2c02372d0', ['439a44a4', 'ad4527a2', 'c02372d0', 'd34ebbaa', '43b19349', 'fe6b92e5', '4bc6ffea', '0b153874', 'a73ee510', '3b08e48b', 'a4609aab', '14d63538', '772a00d7', '07d13a8f', 'f9d1382e', 'b00d3dc9', '776ce399', 'cdfa8259', '0', '0', '20062612', '0', '93bad2c0', '1b256e61', '0', '0']), ('68fd1e642c16a946503b9dbc', ['68fd1e64', '2c16a946', '503b9dbc', 'e4dbea90', 'f3474129', '13718bbd', '38eb9cf4', '1f89b562', 'a73ee510', '547c0ffe', 'bc8c9f21', '60ab2f07', '46f42a63', '07d13a8f', '18231224', 'e6b6bdc7', 'e5ba7672', '74ef3502', '0', '0', '5316a17f', '0', '32c7478e', '9117a34a', '0', '0']), ('05db9164d833535fd032c263', ['05db9164', 'd833535f', 'd032c263', 'c18be181', '25c83c98', '7e0ccccf', 'd5b6acf2', '0b153874', 'a73ee510', '2acdcf4e', '086ac2d2', 'dfbb09fb', '41a6ae00', 'b28479f6', 'e2502ec9', '84898b2a', 'e5ba7672', '42a2edb9', '0', '0', '0014c32a', '0', '32c7478e', '3b183c5c', '0', '0']), ('05db9164510b40a5d03e7c24', ['05db9164', '510b40a5', 'd03e7c24', 'eb1fd928', '25c83c98', '0', '52283d1c', '0b153874', 'a73ee510', '015ac893', 'e51ddf94', '951fe4a9', '3516f6e6', '07d13a8f', '2ae4121c', '8ec71479', 'd4bb7bd8', '70d0f5f9', '0', '0', '0e63fca0', '0', '32c7478e', '0e8fe315', '0', '0']), ('05db91649b5fd12f', ['05db9164', '9b5fd12f', '0', '0', '4cf72387', '0', '111121f4', '0b153874', 'a73ee510', '3b08e48b', 'ac9c2e8f', '0', '6e2d6a15', '07d13a8f', '796a1a2e', '0', 'd4bb7bd8', '8aaa5b67', '0', '0', '0', '0', '32c7478e', '0', '0', '0']), ('241546e038a947a1fa673455', ['241546e0', '38a947a1', 'fa673455', '6a14f9b9', '25c83c98', 'fe6b92e5', '1c86e0eb', '1f89b562', 'a73ee510', 'e7ba2569', '755e4a50', '208d9687', '5978055e', '07d13a8f', '5182f694', 'f8b34416', 'e5ba7672', 'e5f8f18f', '0', '0', 'f3ddd519', '0', '32c7478e', 'b34f3128', '0', '0']), ('be589b51287130e0cd7a7a22', ['be589b51', '287130e0', 'cd7a7a22', 'fb7334df', '25c83c98', '0', '6cdb3998', '361384ce', 'a73ee510', '3ff10fb2', '5874c9c9', '976cbd4c', '740c210d', '1adce6ef', '310d155b', '07eb8110', '07c540c4', '891589e7', '18259a83', 'a458ea53', 'a0ab60ca', '0', '32c7478e', 'a052b1ed', '9b3e8820', '8967c0d2']), ('5a9ed9b080e26c9b97144401', ['5a9ed9b0', '80e26c9b', '97144401', '5dbf0cc5', '0942e0a7', '13718bbd', '9ce6136d', '0b153874', 'a73ee510', '2106e595', 'b5bb9d63', '04f55317', 'ab04d8fe', '1adce6ef', '0ad47a49', '2bd32e5c', '3486227d', '12195b22', '21ddcdc9', 'b1252a9d', 'fa131867', '0', 'dbb486d7', '8ecc176a', 'e8b83407', 'c43c3f58']), ('05db9164bc6e3dc167799c69', ['05db9164', 'bc6e3dc1', '67799c69', 'd00d0f35', '4cf72387', '7e0ccccf', 'ca4fd8f8', '64523cfa', 'a73ee510', '3b08e48b', 'a0060bca', 'b9f28c33', '22d23aac', '5aebfb83', 'd702713a', '0f655650', '776ce399', '3a2028fd', '0', '0', 'b426bc93', '0', '3a171ecb', '2e0a0035', '0', '0']), ('68fd1e6438d50e09da603082', ['68fd1e64', '38d50e09', 'da603082', '431a5096', '43b19349', '7e0ccccf', '3f35b640', '0b153874', 'a73ee510', '3b08e48b', '3d5fb018', '6aaab577', '94172618', '07d13a8f', 'ee569ce2', '2f03ef40', 'd4bb7bd8', '582152eb', '21ddcdc9', 'b1252a9d', '3b203ca1', '0', '32c7478e', 'b21dc903', '001f3601', 'aa5f0a15']), ('8cf072657cd19acc77f2f2e5', ['8cf07265', '7cd19acc', '77f2f2e5', 'd16679b9', '4cf72387', 'fbad5c96', '8fb24933', '0b153874', 'a73ee510', '0095a535', '3617b5f5', '9f32b866', '428332cf', 'b28479f6', '83ebd498', '31ca40b6', 'e5ba7672', 'd0e5eb07', '0', '0', 'dfcfc3fa', 'ad3062eb', '32c7478e', 'aee52b6f', '0', '0']), ('1464facd38a947a1223b0e16', ['1464facd', '38a947a1', '223b0e16', 'ca55061c', '25c83c98', '7e0ccccf', '6933dec1', '5b392875', 'a73ee510', '3b08e48b', '860c302b', '156f99ef', '30735474', '1adce6ef', '0e78291e', '5fbf4a84', 'e5ba7672', '1999bae9', '0', '0', 'deb9605d', '0', '32c7478e', 'e448275f', '0', '0']), ('05db916409e68b8613b87f72', ['05db9164', '09e68b86', '13b87f72', '13a91973', '25c83c98', '7e0ccccf', 'cc5ed2f1', '0b153874', 'a73ee510', '3b08e48b', '081c279a', 'd25f00b6', '9f16a973', '07d13a8f', '36721ddc', '1746d357', 'd4bb7bd8', '5aed7436', 'a153cea2', 'a458ea53', 'dd37e0d1', '0', '32c7478e', 'c70a58f2', 'e8b83407', 'af7ece63']), ('05db916438a947a1e88a1d4c', ['05db9164', '38a947a1', 'e88a1d4c', '8eb9aec7', '25c83c98', 'fbad5c96', '3fd38f3b', '5b392875', 'a73ee510', '5162b19c', '7c430b79', '4ac05ba7', '7f0d7407', 'b28479f6', 'd1128331', 'ce881087', '07c540c4', '5d93f8ab', '0', '0', '57d0811b', '0', '3a171ecb', '1793a828', '0', '0']), ('05db916408d6d899cf59444f', ['05db9164', '08d6d899', 'cf59444f', '60d5f5a7', '25c83c98', '7e0ccccf', '38850d41', '0b153874', 'a73ee510', '6e7947ce', '49aeb6a9', '1d00cbc4', '8f7e5dc7', '07d13a8f', '41f10449', 'b93ac0ad', '1e88c74f', '698d1c68', '0', '0', 'bf8efd4c', '0', 'c7dc6720', 'f96a556f', '0', '0']), ('5a9ed9b03df44d94d032c263', ['5a9ed9b0', '3df44d94', 'd032c263', 'c18be181', '25c83c98', '7e0ccccf', 'a0845add', '0b153874', 'a73ee510', '967857d1', 'e469acef', 'dfbb09fb', '849a0a56', '07d13a8f', '72d05a1c', '84898b2a', 'd4bb7bd8', 'e7648a8f', '0', '0', '0014c32a', 'c9d4222a', '3a171ecb', '3b183c5c', '0', '0'])]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##### 16\n\n\"\"\" Nodes and their meanings \"\"\"\nNode = 16\ndef ValuesNonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    return (T14, 1)\n\n\ndef NonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    T0 = Values[0]\n    return (((T14, T0), 1))\n\ndef ConvertForMerge(toyRDDLine):\n    \n    YY = toyRDDLine[0][0]\n    YYY = toyRDDLine[0][1]\n    YYZ = toyRDDLine[1]\n    \n    return ((YY,(YYY,YYZ)))\n\ndef GetImportance(toyRDDLine):\n    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n    PayLoad = toyRDDLine\n    Key = toyRDDLine[0]\n    PayLoads = PayLoad[1]\n    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n    Value = float(PayLoads[1])   ##This is the numerical estimate value\n    \n    if type(PayLoads2) == list:\n        Lister= []\n        for ii in PayLoads2:\n            if (ii[0] == \"1\"):\n                if (ii[1]/float(Value) >0.9):\n                    Lister.append(1)\n                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n                    Lister.append(2)\n                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n                    Lister.append(3)\n                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n                    Lister.append(4)\n                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n                    Lister.append(5)\n                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n                    Lister.append(6)\n                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n                    Lister.append(7)\n                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n                    Lister.append(8)\n                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n                    Lister.append(9)\n                elif (ii[1] <= 0.1):\n                    Lister.append(10)\n            else:\n                continue\n                \n                \n    else:\n        Lister =[]\n        if (PayLoads2[0] == \"1\"):\n            if (PayLoads2[1]/float(Value) >0.9):\n                Lister.append(1)\n            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n                Lister.append(2)\n            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n                Lister.append(3)\n            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n                Lister.append(4)\n            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n                Lister.append(5)\n            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n                Lister.append(6)\n            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n                Lister.append(7)\n            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n                Lister.append(8)\n            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n                Lister.append(9)\n            elif (PayLoads2[1] <= 0.1):\n                Lister.append(10)\n        \n    return (Key, Lister)\n\n\n    \n    \n\n\n#def ConvertToSignificance(toyRDDLine):\n    \n\n#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n    \nYY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n\nZZ = trainRDD.map(NonNumericFeatures)\\\n             .reduceByKey(lambda x,y: x+y)\\\n             .map(ConvertForMerge)\\\n             .union(YY)\\\n             .reduceByKey(lambda x,y: [x] + [y])\\\n             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\nTT = ZZ.collect()\n\nZZ.unpersist()\nYY.unpersist()\n\n#NamingDict = \"HashDictionary\" + str(Node)\nfrom collections import defaultdict\n\nNamingDict = defaultdict(list)\n\nfor ii in TT:\n    NamingDict[ii[0]] = ii[1]", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4c5b49488ac442359513524b6e694494"}}, "metadata": {}}, {"output_type": "stream", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 4 times, most recent failure: Lost task 1.3 in stage 29.0 (TID 3616, ip-172-31-13-1.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 4 times, most recent failure: Lost task 1.3 in stage 29.0 (TID 3616, ip-172-31-13-1.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Node = 2\nYYY = sc.broadcast(NamingDict)\ndef MappingChangesWithDictionary(trainRDD):\n    \n    \n    Dictionary = YYY.value\n    \n    \"\"\" Taking in all the Key/Value Components \"\"\"\n    FinalKey = trainRDD[0]\n    \n    \n    \n    Value = trainRDD[1]\n    Value[Node] = Dictionary[Value[Node]]\n    \n    return (FinalKey,Value[Node])\n\n\nHashing_1 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\nHashing_1.sample(False, 0.0000001, None).collect()", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f6a627e74d8749ce9a56371ec8ba2b16"}}, "metadata": {}}, {"output_type": "stream", "text": "name 'NamingDict' is not defined\nTraceback (most recent call last):\nNameError: name 'NamingDict' is not defined\n\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 16"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##### 16\n\n\"\"\" Nodes and their meanings \"\"\"\nNode = 17\ndef ValuesNonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    return (T14, 1)\n\n\ndef NonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    T0 = Values[0]\n    return (((T14, T0), 1))\n\ndef ConvertForMerge(toyRDDLine):\n    \n    YY = toyRDDLine[0][0]\n    YYY = toyRDDLine[0][1]\n    YYZ = toyRDDLine[1]\n    \n    return ((YY,(YYY,YYZ)))\n\ndef GetImportance(toyRDDLine):\n    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n    PayLoad = toyRDDLine\n    Key = toyRDDLine[0]\n    PayLoads = PayLoad[1]\n    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n    Value = float(PayLoads[1])   ##This is the numerical estimate value\n    \n    if type(PayLoads2) == list:\n        Lister= []\n        for ii in PayLoads2:\n            if (ii[0] == \"1\"):\n                if (ii[1]/float(Value) >0.9):\n                    Lister.append(1)\n                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n                    Lister.append(2)\n                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n                    Lister.append(3)\n                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n                    Lister.append(4)\n                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n                    Lister.append(5)\n                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n                    Lister.append(6)\n                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n                    Lister.append(7)\n                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n                    Lister.append(8)\n                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n                    Lister.append(9)\n                elif (ii[1] <= 0.1):\n                    Lister.append(10)\n            else:\n                continue\n                \n                \n    else:\n        Lister =[]\n        if (PayLoads2[0] == \"1\"):\n            if (PayLoads2[1]/float(Value) >0.9):\n                Lister.append(1)\n            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n                Lister.append(2)\n            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n                Lister.append(3)\n            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n                Lister.append(4)\n            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n                Lister.append(5)\n            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n                Lister.append(6)\n            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n                Lister.append(7)\n            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n                Lister.append(8)\n            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n                Lister.append(9)\n            elif (PayLoads2[1] <= 0.1):\n                Lister.append(10)\n        \n    return (Key, Lister)\n\n\n    \n    \n\n\n#def ConvertToSignificance(toyRDDLine):\n    \n\n#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n    \nYY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n\nZZ = trainRDD.map(NonNumericFeatures)\\\n             .reduceByKey(lambda x,y: x+y)\\\n             .map(ConvertForMerge)\\\n             .union(YY)\\\n             .reduceByKey(lambda x,y: [x] + [y])\\\n             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\nTT = ZZ.collect()\n\nZZ.unpersist()\nYY.unpersist()\n\n#NamingDict = \"HashDictionary\" + str(Node)\nfrom collections import defaultdict\n\nNamingDict1 = defaultdict(list)\n\nfor ii in TT:\n    NamingDict1[ii[0]] = ii[1]", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "52ecad9b46a547148f65714400b7d0fe"}}, "metadata": {}}, {"output_type": "stream", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 4804, ip-172-31-2-59.us-west-2.compute.internal, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 4804, ip-172-31-2-59.us-west-2.compute.internal, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n    process()\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 35, in GetImportance\nTypeError: float() argument must be a string or a number\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "YYY.unpersist()", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5176265ece9444ec87f8f3e5edc80d3b"}}, "metadata": {}}, {"output_type": "stream", "text": "name 'YYY' is not defined\nTraceback (most recent call last):\nNameError: name 'YYY' is not defined\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Node = 3\nYYY = sc.broadcast(NamingDict1)\ndef MappingChangesWithDictionary(trainRDD):\n    \n    \n    Dictionary = YYY.value\n    \n    \"\"\" Taking in all the Key/Value Components \"\"\"\n    FinalKey = trainRDD[0]\n    \n    \n    \n    Value = trainRDD[1]\n    Value[Node] = Dictionary[Value[Node]]\n    \n    return (FinalKey,Value[Node])\n\n\nHashing_2 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\nHashing_2.sample(False, 0.0000001, None).collect()", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ee5da780ac0841429ebac0442dae2028"}}, "metadata": {}}, {"output_type": "stream", "text": "name 'NamingDict1' is not defined\nTraceback (most recent call last):\nNameError: name 'NamingDict1' is not defined\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Hashing3 = Hashing_1.leftOuterJoin(Hashing_2).cache()\nHashing3.take(20)", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "39283c62db124dc89adb71106f647c53"}}, "metadata": {}}, {"output_type": "stream", "text": "name 'Hashing_1' is not defined\nTraceback (most recent call last):\nNameError: name 'Hashing_1' is not defined\n\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 19"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##### 19\n\n\"\"\" Nodes and their meanings \"\"\"\nNode = 19\ndef ValuesNonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    return (T14, 1)\n\n\ndef NonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    T0 = Values[0]\n    return (((T14, T0), 1))\n\ndef ConvertForMerge(toyRDDLine):\n    \n    YY = toyRDDLine[0][0]\n    YYY = toyRDDLine[0][1]\n    YYZ = toyRDDLine[1]\n    \n    return ((YY,(YYY,YYZ)))\n\ndef GetImportance(toyRDDLine):\n    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n    PayLoad = toyRDDLine\n    Key = toyRDDLine[0]\n    PayLoads = PayLoad[1]\n    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n    Value = float(PayLoads[1])   ##This is the numerical estimate value\n    \n    if type(PayLoads2) == list:\n        Lister= []\n        for ii in PayLoads2:\n            if (ii[0] == \"1\"):\n                if (ii[1]/float(Value) >0.9):\n                    Lister.append(1)\n                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n                    Lister.append(2)\n                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n                    Lister.append(3)\n                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n                    Lister.append(4)\n                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n                    Lister.append(5)\n                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n                    Lister.append(6)\n                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n                    Lister.append(7)\n                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n                    Lister.append(8)\n                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n                    Lister.append(9)\n                elif (ii[1] <= 0.1):\n                    Lister.append(10)\n            else:\n                continue\n                \n                \n    else:\n        Lister =[]\n        if (PayLoads2[0] == \"1\"):\n            if (PayLoads2[1]/float(Value) >0.9):\n                Lister.append(1)\n            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n                Lister.append(2)\n            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n                Lister.append(3)\n            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n                Lister.append(4)\n            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n                Lister.append(5)\n            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n                Lister.append(6)\n            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n                Lister.append(7)\n            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n                Lister.append(8)\n            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n                Lister.append(9)\n            elif (PayLoads2[1] <= 0.1):\n                Lister.append(10)\n        \n    return (Key, Lister)\n\n\n    \n    \n\n\n#def ConvertToSignificance(toyRDDLine):\n    \n\n#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n    \nYY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n\nZZ = trainRDD.map(NonNumericFeatures)\\\n             .reduceByKey(lambda x,y: x+y)\\\n             .map(ConvertForMerge)\\\n             .union(YY)\\\n             .reduceByKey(lambda x,y: [x] + [y])\\\n             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\nTT = ZZ.collect()\n\nZZ.unpersist()\nYY.unpersist()\n\n#NamingDict = \"HashDictionary\" + str(Node)\nfrom collections import defaultdict\n\nNamingDict2 = defaultdict(list)\n\nfor ii in TT:\n    NamingDict2[ii[0]] = ii[1]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "YYY.unpersist()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Node = 5\nYYY = sc.broadcast(NamingDict2)\ndef MappingChangesWithDictionary(trainRDD):\n    \n    \n    Dictionary = YYY.value\n    \n    \"\"\" Taking in all the Key/Value Components \"\"\"\n    FinalKey = trainRDD[0]\n    \n    \n    \n    Value = trainRDD[1]\n    Value[Node] = Dictionary[Value[Node]]\n    \n    return (FinalKey,Value[Node])\n\n\nHashing_4 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\nHashing_4.sample(False, 0.0000001, None).collect()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Hashing_1.unpersist()\nHashing_2.unpersist()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Hashing5 = Hashing_3.leftOuterJoin(Hashing_4).cache()\nHashing5.take(20)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 20"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##### 20\n\n\"\"\" Nodes and their meanings \"\"\"\nNode = 20\ndef ValuesNonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    return (T14, 1)\n\n\ndef NonNumericFeatures(toyRDDLine):\n\n    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n    Values = toyRDDLine.split('\\t')\n    T14 = Values[Node]\n    T0 = Values[0]\n    return (((T14, T0), 1))\n\ndef ConvertForMerge(toyRDDLine):\n    \n    YY = toyRDDLine[0][0]\n    YYY = toyRDDLine[0][1]\n    YYZ = toyRDDLine[1]\n    \n    return ((YY,(YYY,YYZ)))\n\ndef GetImportance(toyRDDLine):\n    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n    PayLoad = toyRDDLine\n    Key = toyRDDLine[0]\n    PayLoads = PayLoad[1]\n    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n    Value = float(PayLoads[1])   ##This is the numerical estimate value\n    \n    if type(PayLoads2) == list:\n        Lister= []\n        for ii in PayLoads2:\n            if (ii[0] == \"1\"):\n                if (ii[1]/float(Value) >0.9):\n                    Lister.append(1)\n                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n                    Lister.append(2)\n                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n                    Lister.append(3)\n                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n                    Lister.append(4)\n                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n                    Lister.append(5)\n                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n                    Lister.append(6)\n                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n                    Lister.append(7)\n                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n                    Lister.append(8)\n                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n                    Lister.append(9)\n                elif (ii[1] <= 0.1):\n                    Lister.append(10)\n            else:\n                continue\n                \n                \n    else:\n        Lister =[]\n        if (PayLoads2[0] == \"1\"):\n            if (PayLoads2[1]/float(Value) >0.9):\n                Lister.append(1)\n            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n                Lister.append(2)\n            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n                Lister.append(3)\n            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n                Lister.append(4)\n            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n                Lister.append(5)\n            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n                Lister.append(6)\n            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n                Lister.append(7)\n            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n                Lister.append(8)\n            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n                Lister.append(9)\n            elif (PayLoads2[1] <= 0.1):\n                Lister.append(10)\n        \n    return (Key, Lister)\n\n\n    \n    \n\n\n#def ConvertToSignificance(toyRDDLine):\n    \n\n#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n    \nYY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n\nZZ = trainRDD.map(NonNumericFeatures)\\\n             .reduceByKey(lambda x,y: x+y)\\\n             .map(ConvertForMerge)\\\n             .union(YY)\\\n             .reduceByKey(lambda x,y: [x] + [y])\\\n             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\nTT = ZZ.collect()\n\nZZ.unpersist()\nYY.unpersist()\n\n#NamingDict = \"HashDictionary\" + str(Node)\nfrom collections import defaultdict\n\nNamingDict2 = defaultdict(list)\n\nfor ii in TT:\n    NamingDict2[ii[0]] = ii[1]", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "YYY.unpersist()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Node = 6\nYYY = sc.broadcast(NamingDict2)\ndef MappingChangesWithDictionary(trainRDD):\n    \n    \n    Dictionary = YYY.value\n    \n    \"\"\" Taking in all the Key/Value Components \"\"\"\n    FinalKey = trainRDD[0]\n    \n    \n    \n    Value = trainRDD[1]\n    Value[Node] = Dictionary[Value[Node]]\n    \n    return (FinalKey,Value[Node])\n\n\nHashing_6 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\nHashing_6.sample(False, 0.0000001, None).collect()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Hashing7 = Hashing_5.leftOuterJoin(Hashing_6).cache()\nHashing7.take(20)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "Hashing_5.unpersist()\nHashing_6.unpersist()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}