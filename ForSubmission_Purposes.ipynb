{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click Through Rate (CTR) Prediction for Advertisement Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial goal of our analysis is to build a model to predict ClickThroughRate (CTR), i.e. given a set of features of both an advertisement(advertisment length, type, duration, etc) and a set of features of a user (webpage visited, web-page history), what is the probability that he or she will click on that given advertisement? The larger goal is to build a model that will perform at scale using concepts taught in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Questions Do We Seek to Answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the questions that we seek to answer are:\n",
    "\n",
    "(1) Which variables are more important in determining the click-through rate prediction?\n",
    "\n",
    "(2) We will be exploring the use of Logistic Regression for Click Through Rate prediction, but we will also discuss improvements and expected accuracy increase with using other Machine Learning techniques.\n",
    "\n",
    "(3) More importantly we will be discussing how the eventual predictive modelling approach is expected to scale to the entire 10GB dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do people perform this kind of analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click-through rate (CTR) prediction is critical to many web applications including web search, recommender systems, sponsored search, and display advertising. Search advertising, known as sponsored search, refers to advertisers identifying relevant keywords based on their product or service for advertising. When the user retrieves the keyword purchased by the advertiser, the corresponding advertisement is triggered and displayed. In the cost-per-click model, the advertiser pays the web publisher only when a user clicks their advertisements and visits the advertiser's site. The CTR prediction is defined to estimate the ratio of clicks to impressions of advertisements that will be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What level of performance should the model be to achieve practical use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a literature search, the model would have to have an `areaUnderROC` value of close to 75% and a `logloss` of 0.025 for it to be considered a high-performing model (Wang et al., 2018). Entropy or mutual information is another metric that is useful for CTR prediction algorithms. Juan et al., 2017 used a `normalized logloss` to measure performance of their model as well as a `Utility` metric which allows to model offline the potential change in profit due to a prediction model change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw5_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ade4fe4f76456bb4b29f7801ee7d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainRDD = sc.textFile('s3://breastcancerfile/e-AKVLH55FZCUE4SN1JHTY7DWU4/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile('data/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.0: Checking the Distribution of the Labels in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used for finding the count of \"1\"s and \"0\"s in the entire dataset.\n",
    "The count shows that we have 34,095,179 \"0\"s (no-clicks) and 11,745,438 \"1\"s (clicks). The dataset is unbalanced, but not by a lot, so it is not necessary that we do subsampling of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 34095179), ('1', 11745438)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CountNumberLabel(toyRDDLine):\n",
    "    \n",
    "    \"\"\" Takes the Count of Number of 1 and 0's in the Label for the entire dataset \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T1 = Values[0]\n",
    "    return (T1, 1)\n",
    "\n",
    "trainRDD.map(CountNumberLabel).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Creating a Training and Testing Dataset from the Original 10GB DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset was created from the original dataset using the `randomSplit()` function. `randomSplit()` enables us to generate a random subset of any dataset. We decided to go with an 1% split of the dataset that represents a count of 458,906 rows called `TrainRDD` - enough for one to train a Logistic Regression function. The testing dataset was created from the 99% remainder of the split (called `TrainRDD2`). The test dataset was used for validating the weights from the Logistic Regression Model used as a model for click prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainRDD, TrainRDD2 = trainRDD.randomSplit([0.01,0.99], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to doing exploratory data analysis, we needed to know what the distribution of the number of clicks (1) and no-click (0) are in the entire dataset. This enabled us to understand if we have an unbalanced dataset and if subsampling of the dataset would be necessary to ensure that we have an even distribution between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestRDD2, TestRDD = TrainRDD2.randomSplit([0.001,0.999], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of the number of rows of data in the Train and Test dataset is shown below. 458,455 rows are present in the TrainRDD while 45,355 are present in the TestRDD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the training dataset:  458455\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows in the training dataset: \", TrainRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the test dataset:  45355\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows in the test dataset: \", TestRDD2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: DataSet Contents and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 14 numeric columns or features and 25 categorical columns or features. The first numerical column in the dataset represents the binary \"Yes\"/\"No\" click event represented as either a \"1\" or a \"0\" respectively. \n",
    "\n",
    "To better understand the dataset, we divided the dataset exploratory section into a numerical exploratory section and a categorical exploratory section. \n",
    "\n",
    "In the numerical exploratory section, our main goals were to:\n",
    "\n",
    "(1) Find the minimum and maximum values of each numerical feature in the column(s) from 1-14.\n",
    "\n",
    "(2) Find the mean and standard deviation of each of the numerical features contained in the entire dataset.\n",
    "\n",
    "The distribution within each dataset is discussed in the Exploratory Data Analysis section.\n",
    "\n",
    "The questions we intended to investigate in the numerical data introduction section were:\n",
    "\n",
    "(1) What were the types of Labels and what was the the distribution of the Labels (1 and 0)? \n",
    "- The distribution of 1 and 0 i.e click vs no-click was shown to be roughly 1/3(section 1.0), indicating an unbalanced dataset but not severely unbalanced.\n",
    "\n",
    "(2) What were the minimum and maximum values of all the numerical features within each numerical column? \n",
    "- This can be done by going through the entire dataset. This is a computationally expensive process but is required. The values need to be saved into an array since the resultant matrix can easily be saved to disk/cached so that we do not have to compute that every time.\n",
    "\n",
    "(3) What were the means and standard deviations of the numerical features within each numerical column? \n",
    "- This is also a computationally expensive process, but the resultant matrix can also be saved to disk/cached so that computation is only done once.\n",
    "\n",
    "### Some Core Course Concepts Utilized in this Section:\n",
    "*Caching and Broadcasting the Matrix containing Mean and Standard Deviations* -- Finding the mean and standard deviation for the trainRDD (10GB dataset) involves going through the entire dataset. However, mean and standard deviation are supposed to be utilized for mean-normalizing only the TrainRDD and the TestRDD2. Thus an important concept here is to *first* do the mean and standard deviation calculation and then *cache* these values. Because the matrix of the mean and standard deviation is small, one can broadcast these values without incurring much performance penalty for use in mean-normalizing the TrainRDD and the TestRDD2.\n",
    "\n",
    "*Mean Normalization for SGD* -- Mean normalization of the numerical variables is very important for Gradient Descent because non-normalized features would take a longer time to converge. To ensure that we converge faster we use mean normalization of the features using the array of mean and standard deviation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractNumerals(toyRDDline):\n",
    "    \n",
    "    YY = toyRDDline.split('\\t')\n",
    "    \n",
    "    if YY[ii] == '':   ## There are empty values/NAN - They are replaced with 0's for this initial Mean, Std calculation\n",
    "        ReturnValue = 0\n",
    "    else:\n",
    "        ReturnValue = float(YY[ii])\n",
    "    return ReturnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featureMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-211780537d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeatureMin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureMeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureStdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mNodeMeanMax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureMax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureMin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'featureMeans' is not defined"
     ]
    }
   ],
   "source": [
    "Node = []\n",
    "NodeMeanMax = []\n",
    "    \n",
    "for ii in range(1,14):\n",
    "\n",
    "    featureRDD = trainRDD.map(ExtractNumerals)\\\n",
    "                         .cache()\n",
    "    feature = featureRDD.mean()\n",
    "    featureStdev = np.sqrt(featureRDD.variance())\n",
    "    \n",
    "    featureMax = featureRDD.max()\n",
    "    featureMin = featureRDD.min()\n",
    "    \n",
    "    Node.append((featureMeans, featureStdev))\n",
    "    NodeMeanMax.append((featureMax, featureMin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Needed = sc.broadcast(Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5775.0, 0),\n",
       " (257675.0, -3.0),\n",
       " (65535.0, 0),\n",
       " (969.0, 0.0),\n",
       " (23159456.0, 0.0),\n",
       " (431037.0, 0),\n",
       " (56311.0, 0.0),\n",
       " (6047.0, 0.0),\n",
       " (29019.0, 0.0),\n",
       " (11.0, 0),\n",
       " (231.0, 0.0),\n",
       " (4008.0, 0),\n",
       " (7393.0, 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NodeMeanMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NodeMeanMax = [(5775.0, 0),\n",
    " (257675.0, -3.0),\n",
    " (65535.0, 0),\n",
    " (969.0, 0.0),\n",
    " (23159456.0, 0.0),\n",
    " (431037.0, 0),\n",
    " (56311.0, 0.0),\n",
    " (6047.0, 0.0),\n",
    " (29019.0, 0.0),\n",
    " (11.0, 0),\n",
    " (231.0, 0.0),\n",
    " (4008.0, 0),\n",
    " (7393.0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d05f60c53e4d56be9a292c86941299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NodeMeans = [(1.9136993727636809, 7.184627898705608),\n",
    " (105.84841979766556, 391.45781841729996),\n",
    " (21.13629851448115, 352.8574390110648),\n",
    " (5.735263227368864, 8.346464252535688),\n",
    " (18060.51214960742, 68556.28645274039),\n",
    " (90.10443053155232, 340.53335300283976),\n",
    " (15.62662976373116, 64.6908374702787),\n",
    " (12.510823839914709, 16.68706965443228),\n",
    " (101.51997332409383, 216.54476824937402),\n",
    " (0.33741472109766746, 0.59176564119444),\n",
    " (2.6146237734976343, 5.115681630473718),\n",
    " (0.2328159762771082, 2.7454850748632738),\n",
    " (6.436072751813085, 14.741644511057435)]\n",
    "\n",
    "Needed = sc.broadcast(NodeMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Mean-Normalization of the Numerical Variable -- TrainRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, mean-normalization of the numerical variable is key to enabling fast gradient descent optimization.\n",
    "We can mean normalize the dataset by splitting the data into Numerical and Categorical variables and then working only on the numerical variable in this section. \n",
    "However, that would result to creating additional data and that is O(space) inefficient. A better idea to cater to scalability would be to work with the dataset as is and make modifications to the dataset. This would lead to an O(1) space complexity and thus enable faster computation of the mean normalization. \n",
    "Recall, we are broadcasting the Mean and Standard deviation array to all the partitions for computing the mean-normalization. This computation process has  O(n) time complexity and O(1) space complexity, while is ideal for scaling and also ideal for use in a map-reduce paradigmn or environment since it now represents an embarassingly parallel computation.\n",
    "\n",
    "During the computation of the mean normalization, we are also replacing all NaN or empty values with the calculated mean for that numerical column and replacing all the NaN or empty values with 0 for the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericValuesOnly(LineRDD):\n",
    "    \"\"\"Function takes the RDD Mean normalizes the Numerical Column in the RDD and replaces 'NAN' and '' with mean for \n",
    "       Numerical values and '0' for Categorical values\"\"\"\n",
    "    \n",
    "    Node = Needed.value\n",
    "    Values = LineRDD.split('\\t')\n",
    "    ZZ = []\n",
    "    for ii in range(0,40):\n",
    "        \n",
    "        if ii < 14:\n",
    "            if Values[ii] == '':\n",
    "                ZZ.append(Node[ii-1][0])\n",
    "            else:\n",
    "                if (ii == 0):\n",
    "                    ZZ.append(float(Values[ii]))\n",
    "                else:\n",
    "                    ZZ.append((float(Values[ii])- float(Node[ii-1][0]))/float(Node[ii-1][1]))\n",
    "        else:\n",
    "            if Values[ii] == '':\n",
    "                ZZ.append(str(0))\n",
    "            else:\n",
    "                ZZ.append(str(Values[ii]))\n",
    "    \n",
    "    return (((str(Values[14])+ str(Values[15]) + str(Values[16])), ZZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68fd1e64287130e071e126ad',\n",
       "  [0.0,\n",
       "   -0.26636026245819294,\n",
       "   -0.2729500211023082,\n",
       "   0.00811574638625955,\n",
       "   -0.6871488397768515,\n",
       "   -0.052139815829603434,\n",
       "   0.9217762862301186,\n",
       "   -0.1488097872925819,\n",
       "   -0.6298783463831777,\n",
       "   -0.24253632977921483,\n",
       "   -0.5701830211308282,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   '68fd1e64',\n",
       "   '287130e0',\n",
       "   '71e126ad',\n",
       "   '7846ae91',\n",
       "   '4cf72387',\n",
       "   '6f6d9be8',\n",
       "   'f6ce794a',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '70962768',\n",
       "   'ab066900',\n",
       "   'e95d3160',\n",
       "   '5d4198ed',\n",
       "   'f7c1b33f',\n",
       "   '42793602',\n",
       "   'fc877894',\n",
       "   'e5ba7672',\n",
       "   '891589e7',\n",
       "   '55dd3565',\n",
       "   'a458ea53',\n",
       "   'c8d017f7',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '7d290d33',\n",
       "   'c243e98b',\n",
       "   '71848e87']),\n",
       " ('68fd1e64083aa75b',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.2652863601435636,\n",
       "   -0.03722834511098185,\n",
       "   0.9902081315594536,\n",
       "   -0.09341976470709805,\n",
       "   -0.12951574388422543,\n",
       "   -0.08697722867347693,\n",
       "   0.3888745174837627,\n",
       "   -0.2563902779685573,\n",
       "   0.33741472109766746,\n",
       "   -0.3156224116605392,\n",
       "   0.2328159762771082,\n",
       "   0.852274469022974,\n",
       "   '68fd1e64',\n",
       "   '083aa75b',\n",
       "   '0',\n",
       "   '0',\n",
       "   '25c83c98',\n",
       "   '0',\n",
       "   'd385ea68',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '3b08e48b',\n",
       "   '7940fc2a',\n",
       "   '0',\n",
       "   '00e20e7b',\n",
       "   '1adce6ef',\n",
       "   '84203dfc',\n",
       "   '0',\n",
       "   'e5ba7672',\n",
       "   '06747363',\n",
       "   '21ddcdc9',\n",
       "   'b1252a9d',\n",
       "   '0',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '0',\n",
       "   'e8b83407',\n",
       "   'ed36f3c2']),\n",
       " ('be589b5180e26c9b74e1a23a',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.17843153594445796,\n",
       "   0.025119780697725077,\n",
       "   -0.44752641530023646,\n",
       "   18060.51214960742,\n",
       "   90.10443053155232,\n",
       "   -0.24155862522123936,\n",
       "   -0.6298783463831777,\n",
       "   -0.4595815180789135,\n",
       "   0.33741472109766746,\n",
       "   -0.5110997834428405,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   'be589b51',\n",
       "   '80e26c9b',\n",
       "   '74e1a23a',\n",
       "   '9a6888fb',\n",
       "   '25c83c98',\n",
       "   '3bf701e7',\n",
       "   '0dab78da',\n",
       "   '0b153874',\n",
       "   '7cc72ec2',\n",
       "   '3b08e48b',\n",
       "   '7bc78da9',\n",
       "   'fb8fab62',\n",
       "   '6b5d07b4',\n",
       "   'b28479f6',\n",
       "   '4c1df281',\n",
       "   'c6b1e1b2',\n",
       "   '2005abd1',\n",
       "   'f54016b9',\n",
       "   '21ddcdc9',\n",
       "   'b1252a9d',\n",
       "   '99c09e97',\n",
       "   '0',\n",
       "   'be7c41b4',\n",
       "   '335a6a1e',\n",
       "   'e8b83407',\n",
       "   'd15c0cc8']),\n",
       " ('05db91640468d67296166464',\n",
       "  [0.0,\n",
       "   0.2903839498232316,\n",
       "   1.2546730633407848,\n",
       "   -0.03722834511098185,\n",
       "   0.6307744948445312,\n",
       "   -0.25391562248062216,\n",
       "   -0.03848207647208949,\n",
       "   -0.1488097872925819,\n",
       "   0.5087277955857558,\n",
       "   0.011452720358730456,\n",
       "   1.119675142958533,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   3.9048511314330825,\n",
       "   '05db9164',\n",
       "   '0468d672',\n",
       "   '96166464',\n",
       "   '867d05be',\n",
       "   '25c83c98',\n",
       "   '7e0ccccf',\n",
       "   '81bb0302',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '012bac1e',\n",
       "   'b7094596',\n",
       "   'dc2f19a6',\n",
       "   '1f9d2c38',\n",
       "   '07d13a8f',\n",
       "   'a888f201',\n",
       "   '668f77c8',\n",
       "   'e5ba7672',\n",
       "   '9880032b',\n",
       "   '21ddcdc9',\n",
       "   '5840adea',\n",
       "   '1c6ba3ba',\n",
       "   '0',\n",
       "   '55dd3565',\n",
       "   'a9a2ac1a',\n",
       "   'ea9a246c',\n",
       "   '409c7293']),\n",
       " ('05db9164287130e0287f8cdf',\n",
       "  [1.0,\n",
       "   1.9136993727636809,\n",
       "   -0.07880394348077799,\n",
       "   21.13629851448115,\n",
       "   -0.44752641530023646,\n",
       "   -0.15463953341340367,\n",
       "   -0.21761284137984085,\n",
       "   -0.16426792694735814,\n",
       "   0.20909460033077323,\n",
       "   -0.2610082606983381,\n",
       "   0.33741472109766746,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   '05db9164',\n",
       "   '287130e0',\n",
       "   '287f8cdf',\n",
       "   '8e068c13',\n",
       "   '30903e74',\n",
       "   'fe6b92e5',\n",
       "   'fed3cb1d',\n",
       "   '0b153874',\n",
       "   'a73ee510',\n",
       "   '1cf80d48',\n",
       "   'b7094596',\n",
       "   'dd28c867',\n",
       "   '1f9d2c38',\n",
       "   'b28479f6',\n",
       "   '9efd8b77',\n",
       "   'f8584e89',\n",
       "   'e5ba7672',\n",
       "   '891589e7',\n",
       "   'a35e3db3',\n",
       "   'b1252a9d',\n",
       "   '204382b1',\n",
       "   '0',\n",
       "   '32c7478e',\n",
       "   '3fdb382b',\n",
       "   '9721386e',\n",
       "   '49d68486'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonNumericRDD = TrainRDD.map(NumericValuesOnly).cache()\n",
    "NonNumericRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the categorical exploratory section our main goals were to:\n",
    "\n",
    "(1) Find the minimum and maximum occurence of each categorical feature in the full dataset as well as the distict set of categorical features present in the dataset.\n",
    "\n",
    "(2) We were also interested in finding the minimum and maximum count of the categorical feature in each categorical column.\n",
    "\n",
    "Note: These two methods are computationally expensive but they give an underlying overview of the numerical distribution within the categorical dataset.\n",
    "\n",
    "More information on the underlying distrbution of the categorical features is in the EDA section.\n",
    "\n",
    "(3) More interesting and useful here is that we will need to implement an hashing algorithm in this section.\n",
    "\n",
    "- The need for hashing is borne out of the curse of dimensionality that we would have had if we were to one-hot encode all the categorical features in the dataset. We would probably have had tens of thousands of columns resulting in a very sparse matrix. That would not only be computationally expensive but also a challenge for shuffling through a computing network.\n",
    "\n",
    "To ensure we have a resulting dense matrix, we used a home-grown hashing function to reduce the dimensionality of each categorical variable from >100 to just 10. This eventually reduced the cardinality of the resultant architecture to 300 rather than in the order of thousands.\n",
    "\n",
    "### Some Core Course Concepts Utilized in this Section:\n",
    "*Hashing* -- Although we did not touch on this concept in depth in class, the hashing function as used here is the function that enables us to reduce the cardinality of the categeorical variables from several thousands to only 10 or as many as we deem fit. \n",
    "\n",
    "Hashing here was implemented using an home grown algorithm.\n",
    "\n",
    "Home-Grown Hashing Function Algorithm:\n",
    "\n",
    "The hashing function uses the co-occurence count of each feature within a column with the click outcome \"1\" divided by the total count of coccurence of the feature. This results in what we call a \"co-occurence count of importance\". Because this is normalized by the overall count, the values are always between 0 and 1. With 1 showing that the feature is very important (meaning everytime this feature occured we also had the occurence of a count) and 0 meaning the feature is not important at all (meaning everytime this feature occured we did not see the occurence of a count).\n",
    "\n",
    "The hashing function enables us to reduce cardinality and also go from a sparse distribution to a denser representation of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('68fd1e64287130e071e126ad',\n",
       "  [0.0,\n",
       "   -0.26636026245819294,\n",
       "   -0.2729500211023082,\n",
       "   0.00811574638625955,\n",
       "   -0.6871488397768515,\n",
       "   -0.052139815829603434,\n",
       "   0.9217762862301186,\n",
       "   -0.1488097872925819,\n",
       "   -0.6298783463831777,\n",
       "   -0.24253632977921483,\n",
       "   -0.5701830211308282,\n",
       "   -0.12014503987823798,\n",
       "   0.2328159762771082,\n",
       "   -0.3009211589986225,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   7,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   7,\n",
       "   8,\n",
       "   7,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   7,\n",
       "   0]),\n",
       " ('68fd1e64083aa75b',\n",
       "  [0.0,\n",
       "   1.9136993727636809,\n",
       "   -0.2652863601435636,\n",
       "   -0.03722834511098185,\n",
       "   0.9902081315594536,\n",
       "   -0.09341976470709805,\n",
       "   -0.12951574388422543,\n",
       "   -0.08697722867347693,\n",
       "   0.3888745174837627,\n",
       "   -0.2563902779685573,\n",
       "   0.33741472109766746,\n",
       "   -0.3156224116605392,\n",
       "   0.2328159762771082,\n",
       "   0.852274469022974,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   8,\n",
       "   9,\n",
       "   0,\n",
       "   7,\n",
       "   9,\n",
       "   8,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   8,\n",
       "   0,\n",
       "   8,\n",
       "   0])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" This for loop loops through the rows in the RDD containing categorical features and converts the categorical features\n",
    "    based on feature importance into a more condensed representation between 1 and 10 \"\"\"\n",
    "\n",
    "\n",
    "StartFromA = 14\n",
    "for ii in range(14, 40):\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "\n",
    "\n",
    "    def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        T0 = Values[0]\n",
    "        return (((T14, T0), 1))\n",
    "\n",
    "    def ConvertForMerge(toyRDDLine):\n",
    "\n",
    "        YY = toyRDDLine[0][0]\n",
    "        YYY = toyRDDLine[0][1]\n",
    "        YYZ = toyRDDLine[1]\n",
    "\n",
    "        return ((YY,(YYY,YYZ)))\n",
    "\n",
    "\n",
    "    def MovingOn(trainRDD):\n",
    "\n",
    "        Key = trainRDD[0]\n",
    "        Value = trainRDD[1]\n",
    "        \n",
    "        ValueKey = Value[0][0]\n",
    "        ValueValue = Value[0][1]\n",
    "        ValueDivide = Value[1]\n",
    "        \n",
    "        return ((Key, (ValueKey, float(ValueValue/ValueDivide))))\n",
    "\n",
    "\n",
    "    def HasherFunction(trainRDD):\n",
    "    \n",
    "        \"\"\" The Hashing Function converts all categorical variables based on feature importance to values between \n",
    "            1 and 10\"\"\"\n",
    "        Key = trainRDD[0]\n",
    "        Value = trainRDD[1][1]\n",
    "        \n",
    "        if (Value >0.9):\n",
    "            FinalValue = 1\n",
    "        elif (Value <=0.9) & (Value >0.8):\n",
    "            FinalValue = 2\n",
    "        elif (Value <=0.8) & (Value >0.7):\n",
    "            FinalValue = 3\n",
    "        elif (Value <=0.7) & (Value>0.6):\n",
    "            FinalValue = 4\n",
    "        elif (Value <=0.6) & (Value >0.5):\n",
    "            FinalValue = 5\n",
    "        elif (Value <=0.5) & (Value >0.4):\n",
    "            FinalValue = 6\n",
    "        elif (Value <=0.4) & (Value >0.3):\n",
    "            FinalValue = 7\n",
    "        elif (Value <=0.3) & (Value >0.2):\n",
    "            FinalValue =8\n",
    "        elif (Value <=0.2) & (Value >0.1):\n",
    "            FinalValue = 9\n",
    "        elif (Value <= 0.1):\n",
    "            FinalValue = 10\n",
    "            \n",
    "        return ((Key, FinalValue))\n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "    YY = TrainRDD.map(ValuesNonNumericFeatures)\\\n",
    "                .reduceByKey(lambda x,y: x+y)\\\n",
    "                .cache()\n",
    "\n",
    "    ZZ = TrainRDD.map(NonNumericFeatures)\\\n",
    "                 .reduceByKey(lambda x,y: x+y)\\\n",
    "                 .map(ConvertForMerge)\\\n",
    "                 .filter(lambda x: x[1][0] == '1')\\\n",
    "                 .leftOuterJoin(YY)\\\n",
    "                 .map(MovingOn).map(HasherFunction).cache()\n",
    "    TT = ZZ.collect()\n",
    "\n",
    "    ZZ.unpersist()\n",
    "    YY.unpersist()\n",
    "\n",
    "    #NamingDict = \"HashDictionary\" + str(Node)\n",
    "    from collections import defaultdict\n",
    "\n",
    "    NamingDict = defaultdict(list)\n",
    "\n",
    "    for ii in TT:\n",
    "        NamingDict[ii[0]] = ii[1]\n",
    "        \n",
    "\n",
    "    \"\"\" The dictionary containing feature importance is broadcasted to all partitions for use in conversion \n",
    "        of the categorical variables to features with lower cardinality - denser representation\"\"\"  \n",
    "    \n",
    "    YYY = sc.broadcast(NamingDict)\n",
    "    \n",
    "    \n",
    "    def MappingChangesWithDictionary(trainRDD):\n",
    "\n",
    "\n",
    "        \"\"\" Mapping Changes with What the Dictionary has for feature importance \"\"\"\n",
    "        Dictionary = YYY.value\n",
    "\n",
    "        \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "        FinalKey = trainRDD[0]\n",
    "        Value = trainRDD[1]\n",
    "        Value[Node] = Dictionary.get(Value[Node], 0)\n",
    "\n",
    "        return (FinalKey,Value)\n",
    "\n",
    "\n",
    "\n",
    "    NonNumericRDD = NonNumericRDD.map(MappingChangesWithDictionary)\\\n",
    "                    .cache()\n",
    "    \n",
    "    \n",
    "NonNumericRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4.1: Minimum and Maximum Co-Occurence of Each Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This function caalculates the minimum and maximum counts of distinct features in each categorical column \"\"\"\n",
    "for ii in range(14,40):\n",
    "    \"\"\" Nodes and their meanings \"\"\"\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "    \n",
    "    Feature = trainRDD.map(ValuesNonNumericFeatures)\\\n",
    "                    .reduceByKey(lambda x,y: x+y)\\\n",
    "                    .values()\\\n",
    "                    .collect()\n",
    "    print(\"This is the minumum value for :\", Node, \"Minimum: =\", min(Feature),\n",
    "          \"This is the maximum value for :\", Node, \"Maximum: =\", max(Feature))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4.2: The Features with the Minimum and Maximum Occurence for Each of the 40 Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This for loop calculates count of the features within each column pickts the top 5-features\"\"\"\n",
    "for ii in range(14,40):\n",
    "    \"\"\" Nodes and their meanings \"\"\"\n",
    "    Node = ii\n",
    "    def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "        \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "        Values = toyRDDLine.split('\\t')\n",
    "        T14 = Values[Node]\n",
    "        return (T14, 1)\n",
    "    \n",
    "    print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(5,lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5:  Core Course Concepts Used for Algorithm and Data-Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) *Hashing* -- Although we did not touch on this concept indepth in class, the hashing function as used here is the function that enables us to reduce the cardinality of the categorical variables from several thousands to only 10 or as many as we deem fit. \n",
    "\n",
    "Hashing here was implemented using an home grown algorithm.\n",
    "\n",
    "Home-Grown Hashing Function Algorithm:\n",
    "\n",
    "The hashing function uses the co-occurence count of each feature within a column with the click outcome \"1\" divided by the total count of coccurence of the feature. This results in what we call a \"co-occurence count of importance\". Because this is normalized by the overall count, the values are always between 0 and 1. With 1 showing that the feature is very important (meaning everytime this feature occured we also had the occurence of a count) and 0 meaning the feature is not important at all (meaning everytime this feature occured we did not see the occurence of a count). The hashing function enables us to reduce cardinality and also go from a sparse distribution to a denser representation of the categorical variables.\n",
    "\n",
    "By choosing an hashing from 1-10 we went from a very high dimensional (cardinal) problem to a lower dimensionality. One can argue that even 10 categories for every categorical colun is still a lot, will it be more practical to have the categories be 5 instead of 10. If computation resource is a bottleneck and given that we know there is some similarity amongst some of the features within a column, we can definitely divide the categorical varaibles into 5 classes. Also note that hashing, as discussed previously, enables us to make the data less sparse.\n",
    "\n",
    "(2) *Caching and Broadcasting the Matrix containing Mean and Standard Deviations* -- Finding the mean and standard deviation for the trainRDD (10GB dataset) involves going through the entire dataset. However, mean and standard deviation are supposed to be utilized for mean-normalizing only the TrainRDD and the TestRDD2. Thus an important concept here is to *first* do the mean and standard deviation calculation and then *cache* these values. Because the matrix of the mean and standard deviation is small, one can broadcast these values without incurring much performance penalty for use in mean-normalizing the TrainRDD and the TestRDD2.\n",
    "\n",
    "In addition, generating the dictionary containing, as keys the categroical feature, and as value the co-occuring feature importance is a  computationally expensive, the resulting dictionary ( a default dict from python) is not a large data for each column. This data is thus broadcasted for use in the transofrmation of each column to a hashed value between 1 and 10.\n",
    "\n",
    "Caching was also used multiple times in the initial analysis, this is because some RDDs calculated at an initial time t1 still had multiple uses downstream at times t+1 and t+2. If the RDD were not cached, pyspark would need to reevaluate the RDD everytime a call was made to that RDD. Caching is especially useful when RDD computation is expensive (NonNumericRDDs - These are computationally expensive calculations). Without caching, every call to these RDDs would mean reevaluating them again.\n",
    "\n",
    "While Caching is good, it also takes away computational resource as some part of memory is allocated to the RDD. When all memory is used for Caching the unused RDDs are spilled-to and stored on disk. Although having RDDs in memry ensures faster computation/evaluation than having RDDs on disk, it is still faster re-evaluating RDDs from memory then recomputing/re-evaluating the RDD. Because of the spill-to-disk problem it is definitely worth it storing in memory only RDDs that are computationally expensive to calculate. Once we are done with the RDD one should ensure to unpersist() the RDD to free memory space for other RDDs.\n",
    "\n",
    "To ensure scalability one needed to ensure that RDDs were unpersisted() from memory once they were no longer in use.If the RDDs were not unpersisted() then it is totally concievable that one would run out of memory space for computation and eventually RDDs would spill to disk.\n",
    "\n",
    "(3) *Normalization* -- Normalization of features (numerical features) is extremely important especially for gradient descent. Numerical features can have widely varying ranges. The values can from -inf to +inf. This range of values can result in Stochastic Gradient Descent taking a very long time to converge. To prevent this from occuring, Normalization of the Numerical variables is done.\n",
    "\n",
    "The method utilized for normalizing here was deployed on the 10GB dataset and it appears to be scalable. The process involves taking a pass through the dataset to calculate the mean and standard deviation of each numerical column and storing each of these values in a list as a set (mean, standard deviation).\n",
    "Because this set is small one can then broadcast this set to the RDD partitions and do another loop through the dataset this time only doing the mean normalization computation ($x_i - mean)/sd$.\n",
    "\n",
    "(4) *Mean Normalization for SGD* -- Mean normalization of the numerical variables is very important for Gradient Descent because non-normalized features would take a longer time to converge. To ensure that we converge faster we use mean normalization of the features using the array of mean and standard deviation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "For the CTR prediction problem, the dependent variable is binary - either the customer clicks on an advertisement or they don't. It is therefore a classification problem, as the dependent variable (click/no-click) is in categorical form. Linear Regression is typically not used for classification problems since it does not perform well when there are outliers. This because the Linear Regression best fit line may not give accurate decision boundaries. \n",
    "\n",
    "Instead, for classification problems we use Logistic Regression. Logistic Regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike Linear Regression which outputs continuous number values, Logistic Regression transforms its output using the logistic sigmoid function which maps any real value into another value between 0 and 1 to return a probability value, which can then be mapped to two or more discrete classes. \n",
    "\n",
    "### The Sigmoid Function\n",
    "The sigmoid function can be written as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s(z) = \\frac{1}{1-e^{-z}}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(1)\n",
    "\n",
    "where:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s(z)$ is the output, i.e. the probability estimate, between 0 and 1 \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $z$ is the input to the function, i.e. the algorithm’s prediction: $\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(where $x_i = i$th feature or predictor and $\\beta_i$ is the $i$th coefficient of the model)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $e$ is the base of natural log\n",
    "\n",
    "The following is a plot of the sigmoid function:\n",
    "\n",
    "<img src=\"sigmoid.png\" width=200>\n",
    "\n",
    "### The Loss Function\n",
    "All predictive algorithms use a loss function to be minimized in order to arrive at a prediction. For Linear Regression, we typically use Mean Squared Error (MSE) as the loss or cost function. However, we cannot use MSE as the loss function for Logistic Regression. This is because the prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.\n",
    "\n",
    "Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.\n",
    "\n",
    "Combining the two cost functions, the loss function (which is to be minimized in Logistic Regression) can be written compactly as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1 - y^{(i)})(1 - log(h_{\\theta}(x^{(i)}))]$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(2)\n",
    "\n",
    "where $h_{\\theta}(x)$ is the model expression.\n",
    "\n",
    "Vectorizing this equation we have:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $h = g(X\\theta)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $J(\\theta) = \\frac{1}{m}\\cdot(-y^{T}log(h)-(1 - y)^{T}log(1-h)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(3)\n",
    "\n",
    "where $X$ is the input feature vector, $\\theta$ is the parameter vector and $y$ is the observation vector.\n",
    "\n",
    "### Gradient Descent\n",
    "To minimize the cost we use Gradient Descent. We know from calculus that to find the minimum of a function, we take the derivative and solve for the derivative equal to zero. \n",
    "\n",
    "Taking the derivative of the cost function, we have:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $C' = x(s(z)-y)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...(4)\n",
    "\n",
    "where\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$C′$ is the derivative of cost with respect to weights\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y$ is the actual class label (0 or 1)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s(z)$ is the model’s prediction\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x$ is the feature vector.\n",
    "\n",
    "Steps to be implented include:\n",
    "\n",
    "  1. Calculate gradient average\n",
    "  2. Multiply by learning rate\n",
    "  3. Subtract from weights \n",
    "\n",
    "### The Implementation\n",
    "In the code below, first we define the Sigmoid function. It takes a vector input and returns the Sigmoid value. \n",
    "\n",
    "The `predict()` method takes a single row of the RDD as input. We know that our dataset comprises 14 numeric features and  25 categorical features (see above). Also, our categorical features have been indexed and hashed earlier (see above) hence they have been transformed into numbers.  \n",
    "\n",
    "We first separately calculate the linear combination of the model weights and the numeric variables and the categorical variables. \n",
    "\n",
    "We then combine the transformed numerical and categorical features to create the vector to be input to the sigmoid function. This implements equation (1) above.  \n",
    "\n",
    "Next, we iterate through the features, updating the weights by adding the product of the learning rate and the squared error, thus implementing equation (3) above. \n",
    "\n",
    "At the end of the iteration we have the predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmaFunction(z):\n",
    "    yhat = 1.0/(1.0 + np.exp(-1 * z))\n",
    "    return (yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe1960595aa493c99c8dd5795787bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initialWeights = (np.ones(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d5eb1c5cd3466d955053e664960b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrate = 0.7\n",
    "initialWeights = list(np.ones(300))\n",
    "def predict(RDDRow):\n",
    "    \n",
    "    \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n",
    "    Key = RDDRow[0]\n",
    "    Value = RDDRow[1]\n",
    "    realY = Value[0]\n",
    "    RestY = Value[1:14]\n",
    "    RestYCat = Value[14:]\n",
    "    #RestY.insert(1,0)\n",
    "    initialWeights = initWeight.value\n",
    "    Y = initialWeights[0]\n",
    "    \n",
    "    for i in range(len(RestY)-1):\n",
    "        \n",
    "        Y+= initialWeights[i+1] *RestY[i]\n",
    "    \n",
    "    for index,value in enumerate(RestYCat):\n",
    "        Y += initialWeights[14+(value)+(index*11)]\n",
    "    \n",
    "        \n",
    "    ZZ = sigmaFunction(Y)\n",
    "    \n",
    "    Error = (realY - ZZ)\n",
    "    ErrorSquared = Error**2\n",
    "    \n",
    "    \n",
    "    for index,value in enumerate(initialWeights):\n",
    "\n",
    "        if index == 0:\n",
    "            initialWeights[index] = initialWeights[index] + (lrate * Error * ZZ * (1.0 - ZZ))\n",
    "        else:\n",
    "            \n",
    "            if index <= 14:\n",
    "                initialWeights[index] = initialWeights[index] + ((lrate* Error * ZZ * (1.0 - ZZ))*Value[index-1])\n",
    "    \n",
    "    for indices, values in enumerate(RestYCat):\n",
    "        \n",
    "        \n",
    "        IndexToUpdate = 14+values +(indices*11)\n",
    "        initialWeights[IndexToUpdate] = initialWeights[IndexToUpdate] + ((lrate* Error * ZZ * (1.0 - ZZ)))\n",
    "                \n",
    "                                            \n",
    "    return(initialWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52f761997014c349a497eccaa147060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999995924044, 1.0, 1.000000000062389, 1.000000000097784, 1.0000000000097127, 1.0000000001410299, 1.000000000073139, 1.000000000029065, 1.0000000000740206, 1.000000000192697, 1.000000000099937, 1.0000000001310472, 1.0000000001001772, 0.9999999999075673, 1.0000000000276408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999879455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996044588, 0.9999999997636152, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998287892, 0.9999999999191619, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996732425, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999035522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996888522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999547172, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996376872, 0.9999999999999936, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924108, 0.9999999997652532, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998271512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.999999999978634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996137704, 0.9999999997944371, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997979673, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999797423, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996126621, 0.9999999998694905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229139, 0.9999999998694968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229075, 0.9999999997718119, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999998205925, 0.9999999996101715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999822329, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995924044, 0.9999999999771013, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996153031, 0.9999999998694968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997229075, 0.9999999998655218, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999997268826]"
     ]
    }
   ],
   "source": [
    "PredictionCalc = NonNumericRDD.map(predict).cache()\n",
    "Y = PredictionCalc.collect()[-1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36101291c4b48bd8044651b235cd909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999991848088, 1.0, 1.0000000001247777, 1.000000000195568, 1.0000000000194251, 1.0000000002820597, 1.000000000146278, 1.00000000005813, 1.0000000001480411, 1.000000000385394, 1.000000000199874, 1.000000000262094, 1.0000000002003544, 0.9999999998151345, 1.000000000055281, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999758911, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992089177, 0.9999999995272304, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996575784, 0.9999999998383238, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999999999346485, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999998071043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999993777045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999094344, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992753743, 0.9999999999999871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848216, 0.9999999995305064, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999996543023, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.999999999957268, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992275408, 0.9999999995888742, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999995959346, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999594846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992253241, 0.999999999738981, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458277, 0.9999999997389937, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458151, 0.9999999995436237, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999999999641185, 0.999999999220343, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999644658, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991848088, 0.9999999999542026, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999992306061, 0.9999999997389937, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994458151, 0.9999999997310436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994537652]"
     ]
    }
   ],
   "source": [
    "lrate = 0.7\n",
    "initialWeights = Y\n",
    "PredictionCalc = NonNumericRDD.map(predict).cache()\n",
    "\n",
    "Y = PredictionCalc.collect()[-1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a848e03d8a4aa1ab7ec30a8d112d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999987772131, 1.0, 1.0000000001871665, 1.000000000293352, 1.0000000000291376, 1.0000000004230896, 1.0000000002194172, 1.000000000087195, 1.0000000002220617, 1.000000000578091, 1.0000000002998108, 1.0000000003931406, 1.0000000003005316, 0.9999999997227018, 1.000000000082921, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999999638366, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988133765, 0.9999999992908456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994863675, 0.9999999997574857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999990197275, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999997106565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999990665567, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999998641517, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999989130615, 0.9999999999999807, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772324, 0.9999999992957597, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994814535, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.9999999999359019, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988413112, 0.9999999993833113, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999993939018, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.999999999939227, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988379862, 0.9999999996084715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687416, 0.9999999996084905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687226, 0.9999999993154356, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999994617775, 0.9999999988305145, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999999466986, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999987772131, 0.999999999931304, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999988459092, 0.9999999996084905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991687226, 0.9999999995965654, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999991806477]"
     ]
    }
   ],
   "source": [
    "lrate = 0.9\n",
    "initialWeights = Y\n",
    "PredictionCalc = NonNumericRDD.map(predict).cache()\n",
    "\n",
    "Y = PredictionCalc.collect()[-1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4e78eef8a248aa916f2c0810709efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def realLoss(toyRDD):\n",
    "    \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n",
    "    Key = toyRDD[0]\n",
    "    Value = toyRDD[1]\n",
    "    realY = Value[0]\n",
    "    RestY = Value[1:14]\n",
    "    RestYCat = Value[14:]\n",
    "    #RestY.insert(1,0)\n",
    "    \n",
    "    initialWeights = initWeight.value\n",
    "    Y = initialWeights[0]\n",
    "    \n",
    "    for i in range(len(RestY)-1):\n",
    "        \n",
    "        Y+= initialWeights[i+1] *RestY[i]\n",
    "    \n",
    "    for index,value in enumerate(RestYCat):\n",
    "        Y += initialWeights[14+(value)+(index*11)]\n",
    "    \n",
    "        \n",
    "    ZZ = sigmaFunction(Y)\n",
    "    \n",
    "    Error = (realY - ZZ)\n",
    "    return Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d6f07b5f334765ba84d20c084e3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LossShift = []\n",
    "initWeights = list(np.ones(300))\n",
    "initWeight = sc.broadcast(initWeights)\n",
    "\n",
    "for ii in range(40):\n",
    "    lrate = 0.9\n",
    "    PredictionCalc = NonNumericRDD.map(predict).cache()\n",
    "    Y = PredictionCalc.collect()[-1]\n",
    "    print(\"This is Run: \", ii)\n",
    "    Y\n",
    "    initWeight = sc.broadcast(Y)\n",
    "    Looser = NonNumericRDD.map(realLoss).mean()\n",
    "    LossShift.append(Looser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee2315669ee4e32aa8637dd76f851b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929, -0.7438821893789929]"
     ]
    }
   ],
   "source": [
    "LossShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0c53cd2534913b4a2778f9d60f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def RunningEpochs(Epoch, lrate, initialWeights):\n",
    "    Epoch = Epoch\n",
    "    lrate = lrate\n",
    "\n",
    "########################################################################\n",
    "#################### Initial Start of the Function #####################\n",
    "\n",
    "    def predict(RDDRow):\n",
    "    \n",
    "        \"\"\" The first Coefficient is always the intercept or the Bias term\"\"\"\n",
    "        Key = RDDRow[0]\n",
    "        Value = RDDRow[1]\n",
    "        realY = Value[0]\n",
    "        RestY = Value[1:14]\n",
    "        RestYCat = Value[14:]\n",
    "        #RestY.insert(1,0)\n",
    "\n",
    "        Y = initialWeights[0]\n",
    "\n",
    "        for i in range(len(RestY)-1):\n",
    "\n",
    "            Y+= initialWeights[i+1] *RestY[i]\n",
    "\n",
    "        for index,value in enumerate(RestYCat):\n",
    "            Y += initialWeights[14+(value)+(index*11)]\n",
    "\n",
    "\n",
    "        ZZ = sigmaFunction(Y)\n",
    "\n",
    "        Error = (realY - ZZ)\n",
    "        ErrorSquared = Error**2\n",
    "\n",
    "\n",
    "        for index,value in enumerate(initialWeights):\n",
    "\n",
    "            if index == 0:\n",
    "                initialWeights[index] = initialWeights[index] + (lrate * Error * ZZ * (1.0 - ZZ))\n",
    "            else:\n",
    "\n",
    "                if index <= 14:\n",
    "                    initialWeights[index] = initialWeights[index] + ((lrate* Error * ZZ * (1.0 - ZZ))*Value[index-1])\n",
    "\n",
    "        for indices, values in enumerate(RestYCat):\n",
    "\n",
    "            IndexToUpdate = 14+values +(indices*11)\n",
    "            initialWeights[IndexToUpdate] = initialWeights[IndexToUpdate] + ((lrate* Error * ZZ * (1.0 - ZZ)))\n",
    "            \n",
    "        return(initialWeights)\n",
    "    \n",
    "################################################################################ \n",
    "##################### Final End of The Function ################################\n",
    "\n",
    "    for ii in range(Epoch):\n",
    "        \n",
    "        \n",
    "        PredictnewWeight= NonNumericRDD.map(predict).collect()[-1]\n",
    "        initialWeights = PredictionCalc\n",
    "    return initialWeights  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8475f7886b3044dd8e97733b24fa85d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 56, in RunningEpochs\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2472, in _jrdd\n",
      "    self._jrdd_deserializer, profiler)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2405, in _wrap_function\n",
      "    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2391, in _prepare_for_python_RDD\n",
      "    pickled_command = ser.dumps(command)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 575, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 918, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 249, in dump\n",
      "    raise pickle.PicklingError(msg)\n",
      "PicklingError: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YY = list(np.ones(300))\n",
    "RunningEpochs(20, 0.89, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ebbde95de3476f94a987942281cf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from https://172.31.44.68:18888/sessions/2 with error payload: \"Session '2' not found.\"\n"
     ]
    }
   ],
   "source": [
    "def MakingRDDDataFrames(rowRDD):\n",
    "    \n",
    "    \"\"\"Getting the RDD in the format of Sets and Lists\"\"\"\n",
    "    Key = rowRDD[0]\n",
    "    ####################\n",
    "    String1 = str(Key[0])\n",
    "    String2 = str(Key[1])\n",
    "    String3 = str(Key[2])\n",
    "    FinalKey = String1 + String2 + String3\n",
    "    \n",
    "    Values = rowRDD[1]\n",
    "    \n",
    "    ### Getting each individual Value WoW\n",
    "    Num1 = Values[0]\n",
    "    Num2 = Values[1]\n",
    "    Num3 = Values[3]\n",
    "    Num4 = Values[4]\n",
    "    Num5 = Values[5]\n",
    "    Num6 = Values[6]\n",
    "    Num7 = Values[7]\n",
    "    Num8 = Values[8]\n",
    "    Num9 = Values[9]\n",
    "    Num10 = Values[10]\n",
    "    Num11 = Values[11]\n",
    "    Num12 = Values[12]\n",
    "    Num13 = Values[13]\n",
    "    \n",
    "    return(FinalKey, Num1, Num2, Num3, Num4, Num5, Num6, Num7, Num8, Num9, Num10, Num11, Num12, Num13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a Random Split 70:30 Toy Example -- The Best Splitting Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc07049311114ac2ba3ea9a7ecb214d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'MakingRDDDataFrames' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'MakingRDDDataFrames' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NumericRDD = TrainRDD.map(NumericValuesOnly).map(MakingRDDDataFrames).cache()\n",
    "NumericRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6186e436059f439994bb423738e0731f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 47956)\n",
      "----------------------------------------\n",
      "[('68fd1e6480e26c9bfb936136', [0.0, -0.1271742093878368, -0.26784091379647845, -0.04573036226671461, -0.6871488397768515, -0.24328202434221455, -0.25285168037808703, -0.009686530399595706, -0.6298783463831777, 0.36703739055185386, 1.119675142958533, -0.12014503987823798, 0, -0.3009211589986225]), ('68fd1e64f0cf00246f67f7e5', [0.0, 0.01201184368251934, -0.27039546744939336, 0.06479586075781131, -0.567337627538544, -0.261952813940516, -0.24110540071200498, -0.21064234591168687, -0.6298783463831777, -0.4503455526193519, 1.119675142958533, -0.3156224116605392, 0, -0.16525108511372877]), ('8cf07265ae46a29dc81688bb', [0.0, 0.15119789675287548, -0.2729500211023082, 0, -0.6871488397768515, -0.2634114693778833, -0.2645979600441691, -0.19518420625691063, -0.7497316244851707, -0.4688174835384752, 1.119675142958533, -0.3156224116605392, 0, -0.4365912328835162]), ('05db91646c9c9cf32730ec9c', [0.0, 0, -0.2729500211023082, 0, 0, -0.07638266919864797, 0, -0.24155862522123936, -0.7497316244851707, -0.44110958715979026, 0, -0.5110997834428405, 0, 0]), ('439a44a4ad4527a2c02372d0', [0.0, 0, -0.26784091379647845, -0.05423237942244738, 0, -0.21723043823083457, 0, -0.24155862522123936, -0.6898049854341742, -0.4595815180789135, 0, -0.5110997834428405, 0, 0]), ('68fd1e642c16a946503b9dbc', [1.0, -0.1271742093878368, -0.26017725283773385, -0.05423237942244738, -0.6871488397768515, -0.2634406424866306, -0.2645979600441691, -0.22610048556646312, -0.7497316244851707, -0.4688174835384752, 1.119675142958533, -0.3156224116605392, 0, -0.4365912328835162]), ('05db9164d833535fd032c263', [0.0, 0, -0.157995106721139, -0.0485643679852922, 0.2713408581296086, 0.013849756156893262, 0.46660794916943893, 0.19126928511249547, 1.1079941860957208, 0.18231808136062091, 0, -0.3156224116605392, 0, 0.10608906265605864]), ('05db9164510b40a5d03e7c24', [0.0, 0, -0.18098608959737283, 0, -0.567337627538544, 0.22866594241797567, -0.20292999179723828, -0.22610048556646312, -0.6298783463831777, -0.4549635353491327, 0, -0.3156224116605392, 0, -0.36875619594106934]), ('05db91649b5fd12f', [0.0, -0.26636026245819294, -0.2550681455319041, -0.042896356548137025, 0.0317184336529936, -0.2572997030953143, 0.05548816085656697, -0.22610048556646312, -0.33024515112819525, 0.025306668548072925, -0.5701830211308282, -0.3156224116605392, 0, -0.029581011228835073]), ('241546e038a947a1fa673455', [1.0, -0.26636026245819294, -0.2729500211023082, 0, 0, -0.2420713403291997, -0.2645979600441691, 0.021229748909956783, -0.7497316244851707, -0.4503455526193519, -0.5701830211308282, 0.27080970368636453, 0, 0])]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 263, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 238, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 251, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "NumericRDD1 = TrainRDD.map(NumericValuesOnly).cache()\n",
    "NumericRDD1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d748bf43004ffaac4befbec9f9af6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"Keys\", StringType(), False),\n",
    "                    StructField(\"Num1\", DoubleType(), False),\n",
    "                    StructField(\"Num2\", DoubleType(), False),\n",
    "                    StructField(\"Num3\", DoubleType(), False),\n",
    "                    StructField(\"Num4\", DoubleType(), False),\n",
    "                    StructField(\"Num5\", DoubleType(), False),\n",
    "                    StructField(\"Num6\", DoubleType(), False),\n",
    "                    StructField(\"Num7\", DoubleType(), False),\n",
    "                    StructField(\"Num8\", DoubleType(), False),\n",
    "                    StructField(\"Num9\", DoubleType(), False),\n",
    "                    StructField(\"Num10\", DoubleType(), False),\n",
    "                    StructField(\"Num11\", DoubleType(), False),\n",
    "                    StructField(\"Num12\", DoubleType(), False),\n",
    "                    StructField(\"Num13\", DoubleType(), False),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ebf38688af4cbda09396a6a21d2474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Numeric = NumericRDD.map(lambda y: Row(Keys = y[0], Num1 = float(y[1]), Num2 = float(y[2]), Num3 = float(y[3]),\n",
    "                                   Num4 = float(y[4]), Num5 = float(y[5]), Num6 = float(y[6]), Num7 = float(y[7]),\n",
    "                                   Num8 = float(y[8]), Num9 = float(y[9]), Num10 = float(y[10]), Num11 = float(y[11]),\n",
    "                                   Num12 = float(y[12]), Num13 = float(y[13])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33cbf2417648499a3dd3b72a0f395a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 693, in createDataFrame\n",
      "    jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrame = spark.createDataFrame(Numeric, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a07c86ea4a14a1ba3d1fd7c1eb551c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def NonNumericValuesOnly(LineRDD):\n",
    "    \"\"\"Function takes the RDD and subsets the RDD\"\"\"\n",
    "    Values = LineRDD.split('\\t')\n",
    "    ZZ = []\n",
    "    RealKey = str(Values[14]) + str(Values[15]) + str(Values[16])\n",
    "    for ii in range(14,40):\n",
    "        if Values[ii] == '':\n",
    "            ZZ.append(str(0))\n",
    "        else:\n",
    "            ZZ.append(str(Values[ii]))\n",
    "    \n",
    "    return (RealKey, ZZ)\n",
    "\n",
    "\n",
    "def MakingRDDDataFrames(rowRDD):\n",
    "    \n",
    "    \"\"\"Getting the RDD in the format of Sets and Lists\"\"\"\n",
    "    Key = rowRDD[0]\n",
    "    ####################\n",
    "    \n",
    "    Values = rowRDD[1]\n",
    "    \n",
    "    ### Getting each individual Value WoW\n",
    "    Str14 = Values[0]\n",
    "    Str15 = Values[1]\n",
    "    Str16 = Values[3]\n",
    "    Str17 = Values[4]\n",
    "    Str18 = Values[5]\n",
    "    Str19 = Values[6]\n",
    "    Str20 = Values[7]\n",
    "    Str21 = Values[8]\n",
    "    Str22 = Values[9]\n",
    "    Str23 = Values[10]\n",
    "    Str24 = Values[11]\n",
    "    Str25 = Values[12]\n",
    "    Str26 = Values[13]\n",
    "    Str27 = Values[14]\n",
    "    Str28 = Values[15]\n",
    "    Str29 = Values[16]\n",
    "    Str30 = Values[17]\n",
    "    Str31 = Values[18]\n",
    "    Str32 = Values[19]\n",
    "    Str33 = Values[20]\n",
    "    Str34 = Values[21]\n",
    "    Str35 = Values[22]\n",
    "    Str36 = Values[23]\n",
    "    Str37 = Values[24]\n",
    "    Str38 = Values[25]\n",
    "    Str39 = Values[26]\n",
    "    \n",
    "    return(FinalKey, Str14, Str15, Str16, Str17, Str18, Str19, Str20, Str21, Str22, Str23, Str24, Str25, Str26, Str27, Str28, Str29,\n",
    "          Str30, Str31, Str32, Str33, Str34, Str35, Str36, Str37, Str38, Str39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9a79cf4364c8ab984e5df69c657f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('68fd1e6480e26c9bfb936136', ['68fd1e64', '80e26c9b', 'fb936136', '7b4723c4', '25c83c98', '7e0ccccf', 'de7995b8', '1f89b562', 'a73ee510', 'a8cd5504', 'b2cb9c98', '37c9c164', '2824a5f6', '1adce6ef', '8ba8b39a', '891b62e7', 'e5ba7672', 'f54016b9', '21ddcdc9', 'b1252a9d', '07b5194c', '0', '3a171ecb', 'c5c50484', 'e8b83407', '9727dd16']), ('68fd1e64f0cf00246f67f7e5', ['68fd1e64', 'f0cf0024', '6f67f7e5', '41274cd7', '25c83c98', 'fe6b92e5', '922afcc0', '0b153874', 'a73ee510', '2b53e5fb', '4f1b46f3', '623049e6', 'd7020589', 'b28479f6', 'e6c5b5cd', 'c92f3b61', '07c540c4', 'b04e4670', '21ddcdc9', '5840adea', '60f6221e', '0', '3a171ecb', '43f13e8b', 'e8b83407', '731c3655']), ('8cf07265ae46a29dc81688bb', ['8cf07265', 'ae46a29d', 'c81688bb', 'f922efad', '25c83c98', '13718bbd', 'ad9fa255', '0b153874', 'a73ee510', '5282c137', 'e5d8af57', '66a76a26', 'f06c53ac', '1adce6ef', '8ff4b403', '01adbab4', '1e88c74f', '26b3c7a7', '0', '0', '21c9516a', '0', '32c7478e', 'b34f3128', '0', '0']), ('05db91646c9c9cf32730ec9c', ['05db9164', '6c9c9cf3', '2730ec9c', '5400db8b', '43b19349', '6f6d9be8', '53b5f978', '0b153874', 'a73ee510', '3b08e48b', '91e8fc27', 'be45b877', '9ff13f22', '07d13a8f', '06969a20', '9bc7fff5', '776ce399', '92555263', '0', '0', '242bb710', '8ec974f4', 'be7c41b4', '72c78f11', '0', '0']), ('439a44a4ad4527a2c02372d0', ['439a44a4', 'ad4527a2', 'c02372d0', 'd34ebbaa', '43b19349', 'fe6b92e5', '4bc6ffea', '0b153874', 'a73ee510', '3b08e48b', 'a4609aab', '14d63538', '772a00d7', '07d13a8f', 'f9d1382e', 'b00d3dc9', '776ce399', 'cdfa8259', '0', '0', '20062612', '0', '93bad2c0', '1b256e61', '0', '0']), ('68fd1e642c16a946503b9dbc', ['68fd1e64', '2c16a946', '503b9dbc', 'e4dbea90', 'f3474129', '13718bbd', '38eb9cf4', '1f89b562', 'a73ee510', '547c0ffe', 'bc8c9f21', '60ab2f07', '46f42a63', '07d13a8f', '18231224', 'e6b6bdc7', 'e5ba7672', '74ef3502', '0', '0', '5316a17f', '0', '32c7478e', '9117a34a', '0', '0']), ('05db9164d833535fd032c263', ['05db9164', 'd833535f', 'd032c263', 'c18be181', '25c83c98', '7e0ccccf', 'd5b6acf2', '0b153874', 'a73ee510', '2acdcf4e', '086ac2d2', 'dfbb09fb', '41a6ae00', 'b28479f6', 'e2502ec9', '84898b2a', 'e5ba7672', '42a2edb9', '0', '0', '0014c32a', '0', '32c7478e', '3b183c5c', '0', '0']), ('05db9164510b40a5d03e7c24', ['05db9164', '510b40a5', 'd03e7c24', 'eb1fd928', '25c83c98', '0', '52283d1c', '0b153874', 'a73ee510', '015ac893', 'e51ddf94', '951fe4a9', '3516f6e6', '07d13a8f', '2ae4121c', '8ec71479', 'd4bb7bd8', '70d0f5f9', '0', '0', '0e63fca0', '0', '32c7478e', '0e8fe315', '0', '0']), ('05db91649b5fd12f', ['05db9164', '9b5fd12f', '0', '0', '4cf72387', '0', '111121f4', '0b153874', 'a73ee510', '3b08e48b', 'ac9c2e8f', '0', '6e2d6a15', '07d13a8f', '796a1a2e', '0', 'd4bb7bd8', '8aaa5b67', '0', '0', '0', '0', '32c7478e', '0', '0', '0']), ('241546e038a947a1fa673455', ['241546e0', '38a947a1', 'fa673455', '6a14f9b9', '25c83c98', 'fe6b92e5', '1c86e0eb', '1f89b562', 'a73ee510', 'e7ba2569', '755e4a50', '208d9687', '5978055e', '07d13a8f', '5182f694', 'f8b34416', 'e5ba7672', 'e5f8f18f', '0', '0', 'f3ddd519', '0', '32c7478e', 'b34f3128', '0', '0']), ('be589b51287130e0cd7a7a22', ['be589b51', '287130e0', 'cd7a7a22', 'fb7334df', '25c83c98', '0', '6cdb3998', '361384ce', 'a73ee510', '3ff10fb2', '5874c9c9', '976cbd4c', '740c210d', '1adce6ef', '310d155b', '07eb8110', '07c540c4', '891589e7', '18259a83', 'a458ea53', 'a0ab60ca', '0', '32c7478e', 'a052b1ed', '9b3e8820', '8967c0d2']), ('5a9ed9b080e26c9b97144401', ['5a9ed9b0', '80e26c9b', '97144401', '5dbf0cc5', '0942e0a7', '13718bbd', '9ce6136d', '0b153874', 'a73ee510', '2106e595', 'b5bb9d63', '04f55317', 'ab04d8fe', '1adce6ef', '0ad47a49', '2bd32e5c', '3486227d', '12195b22', '21ddcdc9', 'b1252a9d', 'fa131867', '0', 'dbb486d7', '8ecc176a', 'e8b83407', 'c43c3f58']), ('05db9164bc6e3dc167799c69', ['05db9164', 'bc6e3dc1', '67799c69', 'd00d0f35', '4cf72387', '7e0ccccf', 'ca4fd8f8', '64523cfa', 'a73ee510', '3b08e48b', 'a0060bca', 'b9f28c33', '22d23aac', '5aebfb83', 'd702713a', '0f655650', '776ce399', '3a2028fd', '0', '0', 'b426bc93', '0', '3a171ecb', '2e0a0035', '0', '0']), ('68fd1e6438d50e09da603082', ['68fd1e64', '38d50e09', 'da603082', '431a5096', '43b19349', '7e0ccccf', '3f35b640', '0b153874', 'a73ee510', '3b08e48b', '3d5fb018', '6aaab577', '94172618', '07d13a8f', 'ee569ce2', '2f03ef40', 'd4bb7bd8', '582152eb', '21ddcdc9', 'b1252a9d', '3b203ca1', '0', '32c7478e', 'b21dc903', '001f3601', 'aa5f0a15']), ('8cf072657cd19acc77f2f2e5', ['8cf07265', '7cd19acc', '77f2f2e5', 'd16679b9', '4cf72387', 'fbad5c96', '8fb24933', '0b153874', 'a73ee510', '0095a535', '3617b5f5', '9f32b866', '428332cf', 'b28479f6', '83ebd498', '31ca40b6', 'e5ba7672', 'd0e5eb07', '0', '0', 'dfcfc3fa', 'ad3062eb', '32c7478e', 'aee52b6f', '0', '0']), ('1464facd38a947a1223b0e16', ['1464facd', '38a947a1', '223b0e16', 'ca55061c', '25c83c98', '7e0ccccf', '6933dec1', '5b392875', 'a73ee510', '3b08e48b', '860c302b', '156f99ef', '30735474', '1adce6ef', '0e78291e', '5fbf4a84', 'e5ba7672', '1999bae9', '0', '0', 'deb9605d', '0', '32c7478e', 'e448275f', '0', '0']), ('05db916409e68b8613b87f72', ['05db9164', '09e68b86', '13b87f72', '13a91973', '25c83c98', '7e0ccccf', 'cc5ed2f1', '0b153874', 'a73ee510', '3b08e48b', '081c279a', 'd25f00b6', '9f16a973', '07d13a8f', '36721ddc', '1746d357', 'd4bb7bd8', '5aed7436', 'a153cea2', 'a458ea53', 'dd37e0d1', '0', '32c7478e', 'c70a58f2', 'e8b83407', 'af7ece63']), ('05db916438a947a1e88a1d4c', ['05db9164', '38a947a1', 'e88a1d4c', '8eb9aec7', '25c83c98', 'fbad5c96', '3fd38f3b', '5b392875', 'a73ee510', '5162b19c', '7c430b79', '4ac05ba7', '7f0d7407', 'b28479f6', 'd1128331', 'ce881087', '07c540c4', '5d93f8ab', '0', '0', '57d0811b', '0', '3a171ecb', '1793a828', '0', '0']), ('05db916408d6d899cf59444f', ['05db9164', '08d6d899', 'cf59444f', '60d5f5a7', '25c83c98', '7e0ccccf', '38850d41', '0b153874', 'a73ee510', '6e7947ce', '49aeb6a9', '1d00cbc4', '8f7e5dc7', '07d13a8f', '41f10449', 'b93ac0ad', '1e88c74f', '698d1c68', '0', '0', 'bf8efd4c', '0', 'c7dc6720', 'f96a556f', '0', '0']), ('5a9ed9b03df44d94d032c263', ['5a9ed9b0', '3df44d94', 'd032c263', 'c18be181', '25c83c98', '7e0ccccf', 'a0845add', '0b153874', 'a73ee510', '967857d1', 'e469acef', 'dfbb09fb', '849a0a56', '07d13a8f', '72d05a1c', '84898b2a', 'd4bb7bd8', 'e7648a8f', '0', '0', '0014c32a', 'c9d4222a', '3a171ecb', '3b183c5c', '0', '0'])]"
     ]
    }
   ],
   "source": [
    "NonNumericRDD = TrainRDD.map(NonNumericValuesOnly).cache()\n",
    "NonNumericRDD.take(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5b49488ac442359513524b6e694494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 4 times, most recent failure: Lost task 1.3 in stage 29.0 (TID 3616, ip-172-31-13-1.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 4 times, most recent failure: Lost task 1.3 in stage 29.0 (TID 3616, ip-172-31-13-1.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000002/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### 16\n",
    "\n",
    "\"\"\" Nodes and their meanings \"\"\"\n",
    "Node = 16\n",
    "def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    return (T14, 1)\n",
    "\n",
    "\n",
    "def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    T0 = Values[0]\n",
    "    return (((T14, T0), 1))\n",
    "\n",
    "def ConvertForMerge(toyRDDLine):\n",
    "    \n",
    "    YY = toyRDDLine[0][0]\n",
    "    YYY = toyRDDLine[0][1]\n",
    "    YYZ = toyRDDLine[1]\n",
    "    \n",
    "    return ((YY,(YYY,YYZ)))\n",
    "\n",
    "def GetImportance(toyRDDLine):\n",
    "    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n",
    "    PayLoad = toyRDDLine\n",
    "    Key = toyRDDLine[0]\n",
    "    PayLoads = PayLoad[1]\n",
    "    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n",
    "    Value = float(PayLoads[1])   ##This is the numerical estimate value\n",
    "    \n",
    "    if type(PayLoads2) == list:\n",
    "        Lister= []\n",
    "        for ii in PayLoads2:\n",
    "            if (ii[0] == \"1\"):\n",
    "                if (ii[1]/float(Value) >0.9):\n",
    "                    Lister.append(1)\n",
    "                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n",
    "                    Lister.append(2)\n",
    "                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n",
    "                    Lister.append(3)\n",
    "                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n",
    "                    Lister.append(4)\n",
    "                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n",
    "                    Lister.append(5)\n",
    "                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n",
    "                    Lister.append(6)\n",
    "                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n",
    "                    Lister.append(7)\n",
    "                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n",
    "                    Lister.append(8)\n",
    "                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n",
    "                    Lister.append(9)\n",
    "                elif (ii[1] <= 0.1):\n",
    "                    Lister.append(10)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        Lister =[]\n",
    "        if (PayLoads2[0] == \"1\"):\n",
    "            if (PayLoads2[1]/float(Value) >0.9):\n",
    "                Lister.append(1)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n",
    "                Lister.append(2)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n",
    "                Lister.append(3)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n",
    "                Lister.append(4)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n",
    "                Lister.append(5)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n",
    "                Lister.append(6)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n",
    "                Lister.append(7)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n",
    "                Lister.append(8)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n",
    "                Lister.append(9)\n",
    "            elif (PayLoads2[1] <= 0.1):\n",
    "                Lister.append(10)\n",
    "        \n",
    "    return (Key, Lister)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "    \n",
    "\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "YY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n",
    "\n",
    "ZZ = trainRDD.map(NonNumericFeatures)\\\n",
    "             .reduceByKey(lambda x,y: x+y)\\\n",
    "             .map(ConvertForMerge)\\\n",
    "             .union(YY)\\\n",
    "             .reduceByKey(lambda x,y: [x] + [y])\\\n",
    "             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\n",
    "TT = ZZ.collect()\n",
    "\n",
    "ZZ.unpersist()\n",
    "YY.unpersist()\n",
    "\n",
    "#NamingDict = \"HashDictionary\" + str(Node)\n",
    "from collections import defaultdict\n",
    "\n",
    "NamingDict = defaultdict(list)\n",
    "\n",
    "for ii in TT:\n",
    "    NamingDict[ii[0]] = ii[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a627e74d8749ce9a56371ec8ba2b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'NamingDict' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'NamingDict' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Node = 2\n",
    "YYY = sc.broadcast(NamingDict)\n",
    "def MappingChangesWithDictionary(trainRDD):\n",
    "    \n",
    "    \n",
    "    Dictionary = YYY.value\n",
    "    \n",
    "    \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "    FinalKey = trainRDD[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Value = trainRDD[1]\n",
    "    Value[Node] = Dictionary[Value[Node]]\n",
    "    \n",
    "    return (FinalKey,Value[Node])\n",
    "\n",
    "\n",
    "Hashing_1 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\n",
    "Hashing_1.sample(False, 0.0000001, None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ecad9b46a547148f65714400b7d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 4804, ip-172-31-2-59.us-west-2.compute.internal, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 4804, ip-172-31-2-59.us-west-2.compute.internal, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 253, in main\n",
      "    process()\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/worker.py\", line 248, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/serializers.py\", line 379, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1544211040330_0001/container_1544211040330_0001_01_000020/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 35, in GetImportance\n",
      "TypeError: float() argument must be a string or a number\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1111)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### 16\n",
    "\n",
    "\"\"\" Nodes and their meanings \"\"\"\n",
    "Node = 17\n",
    "def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    return (T14, 1)\n",
    "\n",
    "\n",
    "def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    T0 = Values[0]\n",
    "    return (((T14, T0), 1))\n",
    "\n",
    "def ConvertForMerge(toyRDDLine):\n",
    "    \n",
    "    YY = toyRDDLine[0][0]\n",
    "    YYY = toyRDDLine[0][1]\n",
    "    YYZ = toyRDDLine[1]\n",
    "    \n",
    "    return ((YY,(YYY,YYZ)))\n",
    "\n",
    "def GetImportance(toyRDDLine):\n",
    "    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n",
    "    PayLoad = toyRDDLine\n",
    "    Key = toyRDDLine[0]\n",
    "    PayLoads = PayLoad[1]\n",
    "    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n",
    "    Value = float(PayLoads[1])   ##This is the numerical estimate value\n",
    "    \n",
    "    if type(PayLoads2) == list:\n",
    "        Lister= []\n",
    "        for ii in PayLoads2:\n",
    "            if (ii[0] == \"1\"):\n",
    "                if (ii[1]/float(Value) >0.9):\n",
    "                    Lister.append(1)\n",
    "                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n",
    "                    Lister.append(2)\n",
    "                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n",
    "                    Lister.append(3)\n",
    "                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n",
    "                    Lister.append(4)\n",
    "                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n",
    "                    Lister.append(5)\n",
    "                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n",
    "                    Lister.append(6)\n",
    "                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n",
    "                    Lister.append(7)\n",
    "                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n",
    "                    Lister.append(8)\n",
    "                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n",
    "                    Lister.append(9)\n",
    "                elif (ii[1] <= 0.1):\n",
    "                    Lister.append(10)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        Lister =[]\n",
    "        if (PayLoads2[0] == \"1\"):\n",
    "            if (PayLoads2[1]/float(Value) >0.9):\n",
    "                Lister.append(1)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n",
    "                Lister.append(2)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n",
    "                Lister.append(3)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n",
    "                Lister.append(4)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n",
    "                Lister.append(5)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n",
    "                Lister.append(6)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n",
    "                Lister.append(7)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n",
    "                Lister.append(8)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n",
    "                Lister.append(9)\n",
    "            elif (PayLoads2[1] <= 0.1):\n",
    "                Lister.append(10)\n",
    "        \n",
    "    return (Key, Lister)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "    \n",
    "\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "YY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n",
    "\n",
    "ZZ = trainRDD.map(NonNumericFeatures)\\\n",
    "             .reduceByKey(lambda x,y: x+y)\\\n",
    "             .map(ConvertForMerge)\\\n",
    "             .union(YY)\\\n",
    "             .reduceByKey(lambda x,y: [x] + [y])\\\n",
    "             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\n",
    "TT = ZZ.collect()\n",
    "\n",
    "ZZ.unpersist()\n",
    "YY.unpersist()\n",
    "\n",
    "#NamingDict = \"HashDictionary\" + str(Node)\n",
    "from collections import defaultdict\n",
    "\n",
    "NamingDict1 = defaultdict(list)\n",
    "\n",
    "for ii in TT:\n",
    "    NamingDict1[ii[0]] = ii[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5176265ece9444ec87f8f3e5edc80d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'YYY' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'YYY' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YYY.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5da780ac0841429ebac0442dae2028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'NamingDict1' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'NamingDict1' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Node = 3\n",
    "YYY = sc.broadcast(NamingDict1)\n",
    "def MappingChangesWithDictionary(trainRDD):\n",
    "    \n",
    "    \n",
    "    Dictionary = YYY.value\n",
    "    \n",
    "    \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "    FinalKey = trainRDD[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Value = trainRDD[1]\n",
    "    Value[Node] = Dictionary[Value[Node]]\n",
    "    \n",
    "    return (FinalKey,Value[Node])\n",
    "\n",
    "\n",
    "Hashing_2 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\n",
    "Hashing_2.sample(False, 0.0000001, None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39283c62db124dc89adb71106f647c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'Hashing_1' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'Hashing_1' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hashing3 = Hashing_1.leftOuterJoin(Hashing_2).cache()\n",
    "Hashing3.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 19\n",
    "\n",
    "\"\"\" Nodes and their meanings \"\"\"\n",
    "Node = 19\n",
    "def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    return (T14, 1)\n",
    "\n",
    "\n",
    "def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    T0 = Values[0]\n",
    "    return (((T14, T0), 1))\n",
    "\n",
    "def ConvertForMerge(toyRDDLine):\n",
    "    \n",
    "    YY = toyRDDLine[0][0]\n",
    "    YYY = toyRDDLine[0][1]\n",
    "    YYZ = toyRDDLine[1]\n",
    "    \n",
    "    return ((YY,(YYY,YYZ)))\n",
    "\n",
    "def GetImportance(toyRDDLine):\n",
    "    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n",
    "    PayLoad = toyRDDLine\n",
    "    Key = toyRDDLine[0]\n",
    "    PayLoads = PayLoad[1]\n",
    "    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n",
    "    Value = float(PayLoads[1])   ##This is the numerical estimate value\n",
    "    \n",
    "    if type(PayLoads2) == list:\n",
    "        Lister= []\n",
    "        for ii in PayLoads2:\n",
    "            if (ii[0] == \"1\"):\n",
    "                if (ii[1]/float(Value) >0.9):\n",
    "                    Lister.append(1)\n",
    "                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n",
    "                    Lister.append(2)\n",
    "                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n",
    "                    Lister.append(3)\n",
    "                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n",
    "                    Lister.append(4)\n",
    "                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n",
    "                    Lister.append(5)\n",
    "                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n",
    "                    Lister.append(6)\n",
    "                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n",
    "                    Lister.append(7)\n",
    "                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n",
    "                    Lister.append(8)\n",
    "                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n",
    "                    Lister.append(9)\n",
    "                elif (ii[1] <= 0.1):\n",
    "                    Lister.append(10)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        Lister =[]\n",
    "        if (PayLoads2[0] == \"1\"):\n",
    "            if (PayLoads2[1]/float(Value) >0.9):\n",
    "                Lister.append(1)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n",
    "                Lister.append(2)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n",
    "                Lister.append(3)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n",
    "                Lister.append(4)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n",
    "                Lister.append(5)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n",
    "                Lister.append(6)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n",
    "                Lister.append(7)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n",
    "                Lister.append(8)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n",
    "                Lister.append(9)\n",
    "            elif (PayLoads2[1] <= 0.1):\n",
    "                Lister.append(10)\n",
    "        \n",
    "    return (Key, Lister)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "    \n",
    "\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "YY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n",
    "\n",
    "ZZ = trainRDD.map(NonNumericFeatures)\\\n",
    "             .reduceByKey(lambda x,y: x+y)\\\n",
    "             .map(ConvertForMerge)\\\n",
    "             .union(YY)\\\n",
    "             .reduceByKey(lambda x,y: [x] + [y])\\\n",
    "             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\n",
    "TT = ZZ.collect()\n",
    "\n",
    "ZZ.unpersist()\n",
    "YY.unpersist()\n",
    "\n",
    "#NamingDict = \"HashDictionary\" + str(Node)\n",
    "from collections import defaultdict\n",
    "\n",
    "NamingDict2 = defaultdict(list)\n",
    "\n",
    "for ii in TT:\n",
    "    NamingDict2[ii[0]] = ii[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYY.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node = 5\n",
    "YYY = sc.broadcast(NamingDict2)\n",
    "def MappingChangesWithDictionary(trainRDD):\n",
    "    \n",
    "    \n",
    "    Dictionary = YYY.value\n",
    "    \n",
    "    \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "    FinalKey = trainRDD[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Value = trainRDD[1]\n",
    "    Value[Node] = Dictionary[Value[Node]]\n",
    "    \n",
    "    return (FinalKey,Value[Node])\n",
    "\n",
    "\n",
    "Hashing_4 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\n",
    "Hashing_4.sample(False, 0.0000001, None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashing_1.unpersist()\n",
    "Hashing_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashing5 = Hashing_3.leftOuterJoin(Hashing_4).cache()\n",
    "Hashing5.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 20\n",
    "\n",
    "\"\"\" Nodes and their meanings \"\"\"\n",
    "Node = 20\n",
    "def ValuesNonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    return (T14, 1)\n",
    "\n",
    "\n",
    "def NonNumericFeatures(toyRDDLine):\n",
    "\n",
    "    \"\"\" Take the node value from a broadcast variable that is sent to the function \"\"\"\n",
    "    Values = toyRDDLine.split('\\t')\n",
    "    T14 = Values[Node]\n",
    "    T0 = Values[0]\n",
    "    return (((T14, T0), 1))\n",
    "\n",
    "def ConvertForMerge(toyRDDLine):\n",
    "    \n",
    "    YY = toyRDDLine[0][0]\n",
    "    YYY = toyRDDLine[0][1]\n",
    "    YYZ = toyRDDLine[1]\n",
    "    \n",
    "    return ((YY,(YYY,YYZ)))\n",
    "\n",
    "def GetImportance(toyRDDLine):\n",
    "    \"\"\"Get the ToyRDD and then divide by the count of occurence\"\"\"\n",
    "    PayLoad = toyRDDLine\n",
    "    Key = toyRDDLine[0]\n",
    "    PayLoads = PayLoad[1]\n",
    "    PayLoads2 = PayLoads[0]  ##This is the real payload to divide through\n",
    "    Value = float(PayLoads[1])   ##This is the numerical estimate value\n",
    "    \n",
    "    if type(PayLoads2) == list:\n",
    "        Lister= []\n",
    "        for ii in PayLoads2:\n",
    "            if (ii[0] == \"1\"):\n",
    "                if (ii[1]/float(Value) >0.9):\n",
    "                    Lister.append(1)\n",
    "                elif (ii[1]/float(Value) <=0.9) & (ii[1]/float(Value) >0.8):\n",
    "                    Lister.append(2)\n",
    "                elif (ii[1]/float(Value) <=0.8) & (ii[1]/float(Value) >0.7):\n",
    "                    Lister.append(3)\n",
    "                elif (ii[1]/float(Value) <=0.7) & (ii[1]/float(Value) >0.6):\n",
    "                    Lister.append(4)\n",
    "                elif (ii[1]/float(Value) <=0.6) & (ii[1]/float(Value) >0.5):\n",
    "                    Lister.append(5)\n",
    "                elif (ii[1]/float(Value) <=0.5) & (ii[1]/float(Value) >0.4):\n",
    "                    Lister.append(6)\n",
    "                elif (ii[1]/float(Value) <=0.4) & (ii[1]/float(Value) >0.3):\n",
    "                    Lister.append(7)\n",
    "                elif (ii[1]/float(Value) <=0.3) & (ii[1]/float(Value) >0.2):\n",
    "                    Lister.append(8)\n",
    "                elif (ii[1]/float(Value) <=0.2) & (ii[1]/float(Value) >0.1):\n",
    "                    Lister.append(9)\n",
    "                elif (ii[1] <= 0.1):\n",
    "                    Lister.append(10)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        Lister =[]\n",
    "        if (PayLoads2[0] == \"1\"):\n",
    "            if (PayLoads2[1]/float(Value) >0.9):\n",
    "                Lister.append(1)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.9) & (PayLoads2[1]/float(Value) >0.8):\n",
    "                Lister.append(2)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.8) & (PayLoads2[1]/float(Value) >0.7):\n",
    "                Lister.append(3)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.7) & (PayLoads2[1]/float(Value) >0.6):\n",
    "                Lister.append(4)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.6) & (PayLoads2[1]/float(Value) >0.5):\n",
    "                Lister.append(5)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.5) & (PayLoads2[1]/float(Value) >0.4):\n",
    "                Lister.append(6)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.4) & (PayLoads2[1]/float(Value) >0.3):\n",
    "                Lister.append(7)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.3) & (PayLoads2[1]/float(Value) >0.2):\n",
    "                Lister.append(8)\n",
    "            elif (PayLoads2[1]/float(Value) <=0.2) & (PayLoads2[1]/float(Value) >0.1):\n",
    "                Lister.append(9)\n",
    "            elif (PayLoads2[1] <= 0.1):\n",
    "                Lister.append(10)\n",
    "        \n",
    "    return (Key, Lister)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#def ConvertToSignificance(toyRDDLine):\n",
    "    \n",
    "\n",
    "#print(trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).takeOrdered(100,lambda x: -x[1]))\n",
    "    \n",
    "YY = trainRDD.map(ValuesNonNumericFeatures).reduceByKey(lambda x,y: x+y).sortBy(lambda x: -x[1]).cache()\n",
    "\n",
    "ZZ = trainRDD.map(NonNumericFeatures)\\\n",
    "             .reduceByKey(lambda x,y: x+y)\\\n",
    "             .map(ConvertForMerge)\\\n",
    "             .union(YY)\\\n",
    "             .reduceByKey(lambda x,y: [x] + [y])\\\n",
    "             .map(GetImportance).filter(lambda x: len(x[1]) != 0 ).cache()\n",
    "TT = ZZ.collect()\n",
    "\n",
    "ZZ.unpersist()\n",
    "YY.unpersist()\n",
    "\n",
    "#NamingDict = \"HashDictionary\" + str(Node)\n",
    "from collections import defaultdict\n",
    "\n",
    "NamingDict2 = defaultdict(list)\n",
    "\n",
    "for ii in TT:\n",
    "    NamingDict2[ii[0]] = ii[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYY.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node = 6\n",
    "YYY = sc.broadcast(NamingDict2)\n",
    "def MappingChangesWithDictionary(trainRDD):\n",
    "    \n",
    "    \n",
    "    Dictionary = YYY.value\n",
    "    \n",
    "    \"\"\" Taking in all the Key/Value Components \"\"\"\n",
    "    FinalKey = trainRDD[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Value = trainRDD[1]\n",
    "    Value[Node] = Dictionary[Value[Node]]\n",
    "    \n",
    "    return (FinalKey,Value[Node])\n",
    "\n",
    "\n",
    "Hashing_6 = NonNumericRDD.map(MappingChangesWithDictionary).cache()\n",
    "Hashing_6.sample(False, 0.0000001, None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashing7 = Hashing_5.leftOuterJoin(Hashing_6).cache()\n",
    "Hashing7.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashing_5.unpersist()\n",
    "Hashing_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
